- 여러 카테고리 중 어디에 속할지 예측하는 것이 목표
- 대부분 각 클래스에 속할 확률 점수를 반환한다.
- 다중 분류이더라도 조건부확률을 이용해 여러 개의 이진 분류로 돌려서 생각해볼 수도 있다.

## 1. 나이브 베이즈
- 주어진 결과에 대해 예측 변수 값을 관찰할 확률$P(X_i|Y_i)$ 을 사용해 예측 변수가 주어졌을 때 결과를 관찰할 확률을 추정한다.$P(Y_i|X_i)$
- `나이브하지 않은 베이지언 분류`
	- 현실성이 없음 : 예측변수의 수가 커지게 되면 대부분의 데이터에서 일치하는 경우가 거의 없음.
- `나이브` : 전체 데이터를 활용함
	1. 이진 응답에 대해 각 예측변수에 대한 조건부확률 $P(X_j | Y = i)$를 구한다.  훈련 데이터에서 $Y = i$인 샘플 중 $X_j$의 비율로 구할 수 있다.
	2. 모든 확률 값을 곱한 뒤 $Y = i$에 속한 샘플의 비율을 곱한다
	3. 모든 클래스에 대해 1 ~ 2 단계를 반복한다
	4. 2단계에서 구한 모든 클래스에 대해 구한 확률 값을 모두 더한 다음 클래스 i의 확률로 나누면 결과 i의 확률을 구할 수 있다
	5. 이 예측변수에 대해 가장 높은 확률을 갖는 클래스를 해당 레코드에 할당한다
	- 혹은 이렇게 표현된다. 좌항의 $P$값은 확률이 Y에 독립이면서 `[0, 1]`사이에 오게 하기 위한 스케일링 계수이다.
	$P(X_1, X_2, ..., X_p) = P(Y=0)(P(X_1|Y=0)P(X_2|Y=0) ...P(X_p|Y=0))$ 
					$+ P(Y=1)(P(X_1|Y=1)P(X_2|Y=1) ...P(X_p|Y=1))$
	- `나이브`의 의미 : 결과가 주어졌을 때, X의 정확한 조건부확률은 각 조건부확률의 곱으로 충분히 잘 추정할 수 있다는 단순한 가정을 기반으로 하기 때문이다. 
		- 즉 **각 독립변수는 서로 독립임을 가정**한 것이다.
- `나이브 베이지언 분류기`는 **편향된 추정 결과를 예측**하는 것으로 알려져있다. 비편향된 추정치를 굳이 구할 필요가 없다면 나이브 베이즈를 써도 괜찮다.
### 수치형 예측 변수
- 예측변수들이 **범주형일 때 베이지언 분류기가 적합**하다. 
- 수치형 데이터라면 둘 중 하나를 따른다.
	1. `binning : 범주에 넣음`하여 범주형으로 변환한 뒤 알고리즘을 적용한다.
	2. 조건부확률 $P(X_j | Y = i)$를 추정하기 위해 정규분포 같은 확률 모형을 사용한다. 

## 2. 판별분석(Discriminant Analysis)
- 초창기의 통계분류 방법, 가장 일반적인 건 `선형판별분석LDA`이다.
- 현재는 `트리 모델`이나 `로지스틱 회귀` 등 더 정교한 기법이 출현하여 LDA를 많이 쓰진 않으나, `주성분분석` 같은 아직 많이 쓰이는 방법들과 연결된다. 또한 예측 변수들의 중요성을 측정하거나 효과적으로 특징을 선택하는 방법으로도 사용된다.
- (참고) LDA는 `잠재 디리클레 할당(Latent Dirichlet Allocation)`과 그 이니셜이 같은데, 해당 방법은 텍스트, 자연어처리에 사용되는 방법이다.

### 공분산행렬
- `공분산` : 두 변수 사이의 관계를 의미하는 지표이다.
$$
s_{x,z} = \frac{\Sigma^n_{i=1}(x_i - \bar{x})(z_i - \bar{z})}{n-1}
$$
- $n$은 레코드의 개수, $\bar{x}, \bar{z}$는 각 변수의 평균을 나타낸다.

- 공분산행렬
$$
\hat{\Sigma} = 
\begin{bmatrix}
s^2_x & s_{x,z} \\
s_{x,z} & s^2_z \\
\end{bmatrix}
$$
- 대각선엔 각 변수의 분산($s_x^2$)이 들어가며, 변수들 사이의 **공분산은 비대각(오른쪽 위방향)원소에 위치한 행렬**이다.

### 피셔의 선형 판별
- 판별분석은 예측변수가 정규분포를 따르는 연속적인 함수라는 가정이 있으나 실제로는 정규분포가 아니거나 이진 예측 변수에 대해서도 잘 동작한다.
- $SS_{사이}$ : 사이 제곱합 - 두 그룹 평균 사이의 거리 제곱합
- $SS_{내부}$ : 공분산행렬에 의해 가중치가 적용된, 각 그룹내의 평균이 주변으로 퍼져 있는 정도
- $\frac{SS_{사이}}{SS_{내부}}$ 값을 최대화하는 것이 두 그룹을 명확히 나누는 방법이다.
- 제곱합 비율을 최대화하는 $W_xx + W_zz$ 을 찾는다.

## 3. 로지스틱 회귀
- **빠른 계산 속도, 새로운 데이터에 대한 빠른 점수 산정 덕분에 다양한 분야에서 사용**됨.

### 1. 로지스틱 반응 함수`logsitic response function`와 로짓`logit`
- $p =\beta_0 +\beta_1x_1 + ... + \beta_kx_k$
	- p는 라벨 1이 될 확률이다
	- 이 모델을 피팅하더라도 p가 0과 1 사이를 벗어날 수 있다.
- `로지스틱반응 or 역로짓함수` : $p = \frac{1}{1 + e^-(\beta_0 +\beta_1x_1 + ... + \beta_kx_k)}$
	- p를 항상 0과 1 사이에 놓을 수 있다.
	- 분모의 지수 부분을 구하기 위해 `오즈(odds)비 : 사건 발생 확률 / 발생 X 확률`를 이용한다. 
		- $odds = \frac{p}{1-p}$ 이므로, $p = \frac{odds}{1+odds}$로 확률을 구할 수 있다.
- 최종적으로 이를 적용하면, $$log(odds(Y=1)) = \beta_0 +\beta_1x_1 + ... + \beta_kx_k$$라는 예측 변수에 대한 선형 함수(`로그오즈함수 = 로짓함수`를 얻을 수 있다.
- 이 다음엔 컷오프 기준을 이용해 그 값보다 큰 확률값은 1로, 아닌 값은 0으로 분류하는 방식을 적용하면 된다.

### 2. 로지스틱회귀와 GLM
- 우리가 실제 관찰한 데이터는 로그 오즈 값이 아닌, 이진 출력값이다. 
- 이를 피팅하기 위해 특별한 확률 기법을 적용한다.
- `일반화선형모형(Generalized Linear Model)`
	- **확률분포** 또는 분포군(로지스틱 : 이항분포)
	- 응답을 예측변수에 매핑하는 **연결 함수**(로지스틱 : 로짓)
	- 요 2가지로 구성되어 있음.
- 로지스틱 회귀는 GLM의 가장 널리 알려진 일반적인 형태이다.
- 다른 걸 사용하는 경우
	- 확률 분포
		- 푸아송 분포 : 카운트 데이터(일정 시간 동안 웹 페이지를 방문한 횟수)를 모델링
		- 음이항분포, 감마 분포 : 경과시간(고장시간) 모델링
	- 연결 함수
		- 데이터 과학자는 때로 로짓 대신 로그 연결 함수를 사용하게 된다.  그러나 매우 다른 결과가 발생할 가능성은 대부분의 응용 분야에서 거의 없다.
	- 로지스틱회귀와 달리 이 모델들을 GLM에 적용하는 건 미묘한 차이가 발생하므로, 익숙치 않다면 피하는 게 좋다.

### 3. 로지스틱회귀의 예측값
$$\hat{p} = \frac{1}{1 + e^{-\hat{Y}}}$$
- 기본적으로 $\hat{p}$ 값을 어느 쪽으로 분류하냐는 `기준값`을 어느 쪽으로 설정하느냐에 달려 있다.
- 일반적으로 `0.5`를 쓰지만 드문 클래스를 확인하고 싶다면 낮추는 것도 무방
#### 4. 계수와 오즈비 해석하기
- 로지스틱 회귀의 장점 : **새 데이터에 대한 결과를 빨리 얻을 수 있음 + 모델을 해석하기가 다른 분류방법들에 비해 훨씬 쉽다**
- `오즈비 이해하기`
$$오즈비 = \frac{오즈(Y=1|X=1)}{오즈(Y=1|X=0)}$$
- 해석하기 ) 만약 오즈비가 2라면, `X=1, Y=1`일때의 오즈가 `X=0., Y=1`일 때의 오즈보다 2배 높다는 것을 의미한다.
	- 확률 대신 오즈비를 쓰는 이유는, 로지스틱 회귀분석의 **계수 $\beta_j$는 $X_j$에 대한 오즈비의 로그 값이기 때문**에 그렇다.

### 5. 다중선형회귀 vs 로지스틱회귀
- 공통점
	1. 예측, 응답변수를 선형관계로 가정함
	2. 가장 좋은 모델을 찾는 과정도 유사함. 예측변수에 스플라인 변환을 사용하는 건 로지스틱에서도 사용 가능함.
- 차이점
	1. 모델 피팅
		- 선형회귀는 최소제곱법을 사용함. 
		- 로지스틱회귀는 `최대우도추정(Maximum Likelihood Estimation)`을 사용해야 한다.
			- `최대우도추정` : 이 데이터를 생성했을 가능성이 가장 큰 모델을 찾는 프로세스
			- **로지스틱회귀의 응답변수는 0, 1이 아니라 응답이 1인 로그 오즈비의 추정치**이다.
			- 따라서 로그 오즈비가 관찰된 결과를 가장 잘 설명하는 모델을 MLE가 찾게 된다.
				- `피셔의 점수화` : 현재 파라미터에 기반해 점수를 얻음
				- `준 뉴턴 최적화 메커니즘` : 적합성 향상 방향으로 파라미터 업데이터 반복
				- 위 2가지 알고리즘이 적용된다. 
				- 모델 평가 : 파라미터 집합$\hat{\theta}$ 에 따른 확률 모형에 대해 일련의 데이터를 관찰할 가장 좋은 파라미터 집합을 찾는 것이다. 
					- 그 기준은 $편차 = -2log(P_{\hat{\theta}}(X_1, X_2, ..., X_n))$이다. 
		- **데이터 과학자는 이게 어떤 과정에서 좋은 모델을 찾는 방법이라는 것만 이해**하면 된다. 소프트웨어가 다 해줌

*참고 : 이 장에서 다루는 모든 분류 방법은 원-핫 인코딩 방법을 사용한다 : 보통은 자동으로 처리되긴 하지만*

### 6. 모델 평가
- 선형회귀처럼 표준 통계 도구(`표준오차, z-score, p-value`)를 사용해 모델을 평가한다.
	- `p값` : **통계적 유의성 측정 지표가 아니라, 변수의 중요성을 나타내는 상대적 지표**로 봐야 한다.
- 선형회귀처럼 `단계적 회귀, 상호작용 항 도입, 스플라인 항 포함` 등 모두 쓸 수 있다.
- 선형회귀와 다른 부분 : **잔차 분석**

## 4. 분류 모델 평가하기
- 예측 모델링에서 수많은 모델 각각에 **홀드아웃 표본을 적용하고 성능을 평가**하는 건 아주 일반적이다. 가장 정확한 예측을 얻기 위해 기본적으로 필요한 일이다.


### 1. 혼동 행렬(Confusion Matrix)
- 분류 결과를 나타내는 가장 대표적인 행렬. 
|         | $\hat{y} = 1$ | $\hat{y} = 0$ |        |
|:-------:| ------------- |:-------------:| ------ |
| $y = 1$ | TP            |      FN       | Recall |
|  $y=0$  | FP            |      TN       |    Specificity    |
|         | Precision     |               | Accuracy
- 각 **row는 실제 클래스, 각 column은 예측 클래스**를 나타낸다.
- **예측 클래스에 따라 Positive, Negative**가 정해지고
- **예측클래스 = 실제클래스면 True, 아니라면 False**이다.
$$ Accuracy(정확도) = \frac{TP + TN}{TP + TN + FP + FN} $$
$$ Precision(정밀도) = \frac{TP}{TP + FP}$$
$$Recall(재현율) = \frac{TP}{TP + FN}$$
$$ FPO(거짓양성비율) = \frac{FP}{TP + FN} $$
$$ Specificity(특이도) = \frac{TN}{TN + FP}$$


### 2. 희귀 클래스 문제
- 드물게 일어나는 일을 `1`, 일반적인 일을 `0`으로 지정한다.
- 보통 중요한 사건을 `1`로 지정한다.
- 한편 어떤 모델을 선택하는지는 상황에 따라 다르다.
	- 예를 들어, `0.1%`의 방문객만이 물건을 구매한다고 하자. 모든 방문객을 물건을 구매하지 않을 것이라고 하는 모델의 정확도는 `99.9%`이다. 그러나 이런 모델은 있으나마나다.
	- 이 경우는 오히려 `정확도`가 떨어지더라도, 실재 구매자를 잘 골라내는 모델이 있다면 그 모델이 선호될 것이다.

### 3. ROC 곡선
- `Recall`과 `Specificity` 사이에는 트레이드오프 관계가 있다. 
- 이를 잘 표현하는 지표가 `수신자 조작 특성(Receiver Operating Characteristic) 곡선`이다.
- X축은 `Specificity`, Y축은 `Recall`이 표시된다.
- 계산 과정은 다음과 같다.
	1. 1로 예측할 확률에 따라 가장 1이 되기 쉬운 것부터 어려운 순으로 레코드를 정렬한다.
	2. 정렬된 순서대로 특이도와 재현율을 계산한다.

### 4. AUC
- ROC 곡선의 곡선 아래 면적`Area Underneath the Curve = AUC`이라는 지표가 있다.
- **AUC가 클수록 좋은 분류기**이며, **AUC = 1은 1을 정확히 분류**하는 분류기라는 의미다. **최악은 0.5**

### 5. 리프트
- 희귀케이스 문제에서 확률 컷오프를 0.5 미만으로 낮추는 상황이 있다.
- 1을 과대평가하지 않는 최적의 컷오프란 무엇일까?
	- 예를 들어, 상위 10%의 레코드를 1로 분류하는 알고리즘과 무작위로 선택했을 때를 비교해본다.
		- 무작위로 선택했을 때 `0.1%`의 정확도, 알고리즘이 `0.3%`의 정확도를 얻었다면, 이 알고리즘은 `3의 리프트lift = 이득gain`을 갖는다고 할 수 있다.
- 리프트 차트를 계산하려면` y축에 재현율(Recall), x축에 총 레코드 수`를 나타내는 `누적이득차트`를 작성해야 한다.
- 리프트 곡선은 확률 컷오프 값에 따른 결과의 변화를 한눈에 볼 수 있게 해준다.

## 5. 불균형 데이터 다루기

### 1. 과소표본추출
- 데이터 개수가 충분하다면 **다수의 데이터에 해당하는 클래스의 수를 줄여**(`과소표본추출(다운샘플링)`) 0과 1의  갯수에 균형을 맞출 수 있다.
	- 기본 아이디어 : 다수의 클래스 데이터 중엔 중복이 많을 것이다
	- 작지만 균형잡힌 데이터는 모델 성능, 데이터 준비, 모델 검증 과정을 더 수월하게 한다.
	- `충분한 데이터`는 분야에 따라 다르지만, 소수 클래스의 데이터가 수만 개 있다면 충분하다. 

### 2. 과잉표본추출, 상향/하향 가중치
- `과소표본추출의 약점`은 일부 데이터를 버리므로 모든 정보를 활용하지 못한다는 점이다.
	- 정말 유용한 정보가 버려질 가능성도 있다.
- 이 경우, **복원추출방식(부트스트랩)으로 희귀 클래스 데이터를** `과잉표본추출(업샘플링)`해야한다.
- 데이터에 가중치를 적용하는 방식으로 비슷한 효과를 얻을 수 있다.
	- 많은 분류 알고리즘에서 `weight`라는 인수를 지원한다.
- **가중치 적용은 업샘플링, 다운샘플링을 대체할 수 있다.**

*** 참고 : 손실 함수를 수정하는 방법이 제시되기도 하지만, 분류 알고리즘의 손실함수를 직접 수정하는 건 복잡하고 어려운 반면, 가중치를 사용하는 방법은 가중치가 높은 데이터를 선호하고, 가중치가 낮은 데이터의 오류를 줄이는 방식으로 손실함수를 변경하는 쉬운 방법이다.

### 3. 데이터 생성
- 업샘플링의 변형으로` 기존 데이터를 살짝 바꿔 새로운 레코드를 만드는` `데이터 생성` 방법이 있다.
- `Synthetic Minority Oversampling Technique : SMOTE` 알고리즘은 업샘플링된 레코드와 비슷한 레코드를 찾아 원래 레코드와 이웃 레코드 간 랜덤 가중평균으로 새로운 합성 레코드를 만든다. 각 예측변수에 대해 개별적으로 가중치를 생성한다. 

### 4. 비용 기반 분류
- 예를 들어 신규 대출에서 연체로 인한 비용을 $C$, 대출 상환으로 얻을 기대수익을 $R$이라고 하면, $기대수익 = P(Y=0)\times R + P(Y=1)\times C$ 이 된다. 
- 대출 결과를 단순히 연체 or 상환으로 결정하는 대신, 대출을 통해 얻을 수 있는 기대수익이 있는지 없는지를 결정하는 게 더 말이 된다. 즉, 기대 이익이 적은 대출보다 연체 확률이 높더라도 기대 이익이 더 큰 대출을 선호할 수도 있다는 뜻이다.

### 5. 예측 결과 분석
- `AUC` 등 단일성능 지표로는 모델의 적합성을 여러 측면에서 보기 어려울 수 있다.
- 따라서 여러 모델을 한 그래프에 동시에 적용해 시각화할 수 있다.
	- 단, 차원이 높아지고 GAM이나 트리 모델을 쓴다면 시각화가 쉽진 않다.
	- 어떤 경우에도 예측값에 대한 탐색 분석은 할 가치가 있긴 하다.