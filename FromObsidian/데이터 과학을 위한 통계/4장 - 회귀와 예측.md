통계학의 일반 목표는 **변수 X와 Y의 관계성 & X로 Y를 예측할 수 있는가?** 이다.  
통계 - 데이터 과학의 연결고리로는 **결과 변수 예측 & 이상 검출** 2가지가 있다.

## 1. 단순 선형 회귀
- 데이터의 산점도에서 `잔차를 최소화`하는 $y = ax + b$식이 회귀식이다.
- 회귀식은 명시적으로 오차 $e_i$ 항을 포함한다.
- 적합값=예측값으로 $\hat{Y}_i = \hat{b}_0 + \hat{b}_1 X_i$  로 쓸 수 있다.
	- `hat` 표기법이라고 하며, `추정치`이기 때문에 `불확실성`을 내포함을 표시한 것이다.
- `잔차` : $\hat{e}_i =Y_i - \hat{Y}_i$ 처럼 표시하며, 실제값 - 예측값을 의미한다.

### 최소제곱법
- 회귀선은 잔차제곱합(Residual Sum of Squares)을 최소화하는 선이다.
$$RSS = \Sigma_{i-1}^{n}(Y_i - \hat{Y_i})^2
=  \Sigma_{i-1}^{n}(Y_i - \hat{b_0} - \hat{b_1}X_i)^2$$ 
- 이런 방법을 `최소제곱회귀, 보통최소제곱`이라고 하며 계수 계산을 위한 공식은 이렇다.
$$ \hat{b_1} = \frac{\Sigma_{i=1}^n(Y_i - \bar{Y})(X_i - \bar{X})}{\Sigma_{i=1}^n(X_i - \bar{X})^2}$$
$$\hat{b_0} = \bar{Y} - \hat{b_1}\bar{X}$$
- **최소제곱법이 많이 쓰이는 이유는 계산의 편의성 때문이다.**
	- 그러나 특잇값에 매우 민감한데, 이는 작거나 중간 샘플 사이즈에서 문제가 될 수 있다.
- 회귀방정식 자체가 **인과관계를 정확히 증명하진 않는다**.  여기에는 **배경지식이나 해당 필드에 대한 명확한 이해가 필요**하다. 
	- 광고 클릭 수와 전환률 간의 관계에서, 광고 클릭 -> 판매는 말이 되지만 판매 -> 광고 클릭은 말이 안될 것이다.

## 2. 다중 선형 회귀
- (대충 변수 여러개 들어간 모양) - 그래프 자체가 선형은 아님.

### 모형 평가
- **RMSE** : 제곱근평균제곱오차(RootMeanSqaureError)
$$RMSE = \sqrt{\frac{\Sigma^{n}_{i = 1}(y_i - \hat{y_i})^2}{n}}$$
- **RSE** = 잔차표준오차(ResidualStandardError)
$$RSE = \sqrt{\frac{\Sigma^n_{i=1}(y_i-\hat{y_i})^2}{n-p-1}}$$
	- 분모가 데이터 수가 아니라 자유도라는 차이점이 있음.
	- 실무에서 선형회귀분석 시 RMSE와 RSE의 차이는 아주 작으며, 빅데이터 분야에선 더욱 그렇다.
	
- **결정계수** = R 제곱 통계량
$$R^2 = 1 - \frac{\Sigma^n_{i=1}(y_i-\hat{y_i})^2}{\Sigma^n_{i=1}(y_i-\bar{y_i})^2}$$
	- 분모는 Y의 분산에 비례한다. 

- 언어 R에서는 계수의 표준 오차와 t통계량을 함께 출력해 보여준다.
$$t_b = \frac{\hat{b}}{SE(\hat{b})}$$
	- p값과 F 통계량이 동원되기도 한다.
- 데이터 과학에서는 통계적 유의성 문제에 너무 깊이 관여하지 않는다. 다만 **모델에 예측변수 X를 포함할지 말지를 판단하기 위해 t-통계량**을 유용하게 쓰는데, **값이 높을수록 모델에 포함해야 함을 의미한다(이는 p-value가 0에 가까워진다는 말과 동의어)**

### 교차타당성검사
- 왜 하는가 : 예측을 위해선 표본 밖에서 유효성 검사를 할 필요가 있음
- `홀드아웃샘플` : 주어진 데이터 중 일부를 따로 떼어놨다가, 모델을 테스트할 때 사용하는 것
- `교차타당성검사CrossValidation` : 홀드아웃 샘플을 여러 개 한 것
	- 홀드아웃 샘플마다 테스트 결과가 다를 수 있기 때문에 교차검증이 꼭 필요하다.
	- 대표적인 게 `K-fold CV` : 뭔지 아니까 그냥 넘어감
	- `fold` : 훈련 / 홀드아웃 샘플로 데이터를 나누는 행위

#### 모형 선택과 단계적 회귀
- `오컴의 면도날` : 모든 것이 동일한 조건에서는 복잡한 모델보다 단순한 모델을 우선 사용해야 한다. 
	- **변수를 추가하면 항상 RMSE는 감소하고 $R^2$은 증가한다.

- 1970년 `AIC`라는 측정 기준이 개발되었는데, 변수가 추가되면 불이익을 주는 수식이다.
$AIC = 2P + nlog(\frac{RSS}{n})$
	- $P$는 변수의 개수, $n$은 레코드의 개수이다.
- $AIC$를 최소화하는 모델을 찾는 게 목표이다.
	- 이 중 한가지 방법이 부분 집합 회귀가 있는데, 계산 비용이 크고 대용량에 부적합하다.
	- 다른 방법으론 `단계적 회귀(Stepwise Regression)`가 있다.
		- `Python`에서는 [링크1](https://planspace.org/20150423-forward_selection_with_statsmodels/), [링크2](https://signature95.tistory.com/21) 을 참고하자.
	- 더 단순한 방법으로는 전진선택, 후진선택이 있다.
		- 전진선택 : 예측변수 없이 시작, $R^2$에 가장 큰 기여도를 갖는 예측 변수를 하나씩 추가하고 기여도가 더 이상 통계적으로 유의미하지 않을 때 증가를 멈춘다.
		- 후진선택 : 전체 모델로 시작, 모든 예측변수가 통계적으로 유의미한 모델이 될 떄까지 유의하지 않은 변수을 제거한다.
	- `벌점회귀Penealized Regression`도 있다. 모델 적합 방정식에 많은 변수에 대해 모델에 불이익을 주는 제약 조건을 추가한다. `Ridge`, `Lasso`가 여기에 속함.
- 위 방법은 `표본 내`방법으로, `오버피팅` 가능성이 있다. 복잡하고 좁을수록 `교차검증`의 필요성이 증가한다.

#### 가중 회귀
- 방정식 피팅 시 레코드 별로 가중치를 주기 위해 사용함

## 3. 회귀를 이용한 예측

### 외삽의 위험
- 회귀모형을 데이터 범위를 초과하면서까지 외삽하는 데 사용해선 안된다.
	- 충분한 데이터 값이 있는 X에 대해 유용하며, 그렇더라도 다른 문제가 있을 수 있다.

#### 신뢰 구간과 예측 구간
- 회귀 분석 결과에 나오는 t 통계량과 p값은 변동성을 다루는 일반적인 방법이고, 변수 선택을 위해 쓰이기도 한다.
- 회귀 파라미터(계수)에 대한 신뢰구간 생성 부트스트랩 알고리즘은 이렇다.
	1. 각 행(X, y 모두 포함)을 하나의 티켓으로 생각, n개의 티켓을 박스에 넣는다.
	2. 무작위로 티켓을 뽑아 값 기록 후 다시 넣는다.
	3. 2.를 n번 반복한다. 부트스트랩 재표본을 하나 만든다.
	4. 이 표본으로 회귀 모형을 구하고, 추정된 계수들을 기록한다.
	5. 2~4를 1000번 반복한다.
	6. 각각에 대해 적합한 백분위수를 구한다.( ex) 90% 신뢰구간을 위해 5 ~ 95번째 백분위수를 구한다.)
- 데이터 과학자들의 관심은 회귀계수의 수치 자체와, 예측값 $\hat{Y_i}$의 구간에 있다. 예측값의 불확실성은
	1. 적합한 예측변수가 무엇인가, 계수가 얼마인가에 따른 불확실성
	2. 개별 데이터 값의 오류

- `신뢰구간` : 개별 예측값의 불확실성 정량화 (여러 값에서 계산된 통계량에 대한 불확실성)
- `예측구간` : 회귀계수 주변의 불확실성 정량화 (하나의 값에 대한 불확실성)

## 4. 요인변수`FactorVariable`
- 제한된 개수의 이산값 : 회귀분석을 위해선 요인변수의 수치화가 필요하다.
- 일반적으로 사용되는 건 `원-핫 인코딩`이다. 각 카테고리에 대해 0과 1로 정리하는 것.
- n개의 카테고리를 갖는다면 요인 변수는 n-1개의 열을 갖는 행렬로 표시된다. (이전에도 나왔듯 n-1개가 정해지면 나머지 1개는 자동으로 정해지기 떄문)
- 순서가 있을 수 있다 : 이런 경우는 **수치형 변수로 다뤄서 순서에 대한 정보를 유지**해준다.

## 5. 회귀방정식 해석
------
<용어>  
- `변수 간 상관Correlated Variables` : 예측 변수간 높은 상관성을 가진다면, 개별 계수를 해석하는 건 어렵다.
- `다중공선성Multicollinearity` : 예측 변수들이 완벽하거나 완벽에 가까운 상관성을 가질 때, 회귀는 불안정하며 계산이 불가능하다.
- `교란변수Confounding Variable` : 중요한 예측 변수이지만 회귀방정식에 누락되어 결과를 잘못되게 이끄는 변수
- `주효과Main Effect` : 다른 변수들과 독립된 하나의 예측변수와 결과변수 사이의 관계
- `상호작용Interaction` : 둘 이상의 예측변수, 응답변수 사이의 상호의존적 관계
------------
#### 1. 예측 변수 간 상관
- 예를 들면 집의 가치와 집 정보에 관한 문제를 보면, 다변수 회귀식에서 침실의 개수는 음의 계수를 나타낸다.
	- 이것은 예측변수(X)끼리 연관되어 있기 때문으로, 집이 클수록 침실이 많을 것이다. 그런데 침실 수보다는 주택의 크기가 집의 가격에 더 큰 영향을 줄 것이다. 같은 크기에 침실이 많다는 건 작은 크기의 침실이 여러 개 있다는 의미이기 때문임.
	- 이 문제의 경우 침실 수, 평수, 욕실 수에 대한 변수들은 모두 상관관계가 있다.
	- 모델의 변수를 추가하거나 제외할 수 있고, 이 떄의 계수 또한 다시 측정될 수 있을 것이다.

### 2. 다중공선성
- 완전 다중공선성 : **한 예측 변수가 다른 변수들의 선형 결합으로 표현**되는 경우
	- 한 변수가 여러번 포함되었거나
	- 요인변수로부터 P-1개가 아닌 P개의 가변수가 만들어졌거나
	- 두 변수가 거의 완벽하게 상관관계가 있는 경우 발생한다.
- **회귀분석에서 다중공선성 문제는 반드시 해결되어야 한다.**
- 다행히 다중공선성 문제는 자동으로 처리될 수 있다. 근데 불완전 다중공선성은 결과가 불안정할 수는 있다.
	- 회귀 유형이 아니라면 큰 문제가 되지 않는다.(트리, 클러스터링, k-NN 등)
	- 위 문제들에선 P개의 가변수를 유지하는 것이 좋다. 예측변수의 비중복성을 유지해야 하는 건 동일하다.

### 3. 교란변수
- 회귀방정식에서 **주요한 변수가 포함되지 못해 생기는 누락**의 문제
	- `예측 변수 간 상관`과 구분 : 비슷한 예측 관계를 갖는 다른 변수가 있어서 생기는 문제

### 4. 상호작용과 주효과
- 통계학에서` 주효과`와 `주효과 사이의 상호작용`이 구분된다.
- 모델에서 주효과만 사용한다면, 예측변수 - 응답변수 간의 관계가 다른 예측변수에 대해 독립적이라는 가정이 깔려있는데, 이는 종종 사실이 아니다.
- 근데 다수의 변수가 있는 문제의 경우 상호작용을 어떻게 고려해야 하는가는 쉬운 문제가 아니다.
	- 사전 지식이 도움이 될 수 있고
	- 단계적 선택을 사용해 다양한 모델을 거를 수 있으며
	- 벌점 부여 회귀를 사용해 자동으로 상호작용들을 걸러내거나
	- 일반적으로는 **`랜덤 포레스트, 그래디언트 부스팅 트리` 등을 가장 일반적으로 쓴다.** 자동으로 최적의 상호작용 항들을 걸러낸다.

## 6. 가정 검정 : 회귀 진단
- 설명을 위한 모델링에선 매 단계마다 모델이 데이터에 얼마나 적합한지를 평가한다.
	- 대부분 **잔차 분석**을 기본으로 한다.

### 1. 특잇값
- 대부분의 측정치에서 멀리 벗어난 값.
- **회귀의 경우 실제 y값이 예측된 값에서 멀리 떨어진 경우**를 의미한다.
- `표준화잔차` : $\frac{잔차}{표준화오차}$
- **특잇값을 정상값과 구분하는 통계 이론은 없다.**
	- 다만 경험칙이 존재한다.
	- 상자 플롯에서 상자 경계선 바깥에 위치한 점을 의미한다.
	- 회귀에선 표준화잔차를 사용한다.
- 특잇값의 경우 부주의한 데이터 입력이나 단위 실수, 혹은 비정상적인 데이터 발생 등에 의해 발생할 수 있고, 특히 마지막의 경우는 모델에 포함시키면 안된다.
- 빅데이터에서 예측을 위한 회귀분석에서 특잇값이 큰 문제가 되진 않는다. 그러나 특잇값 겂출의 경우는 이 값들이 매우 중요해진다.(사기, 사건 발생 등)

### 2. 영향값
- `주영향관측값` : 회귀모형에서 제외됐을 때 모델에 중요한 변화를 가져오는 값
	- 그 값을 큰 `레버리지`를 가졌다고 표현하는 것 같다.
- `레버리지 측정 척도`
	- `햇값hat-value` : $\frac{2(P+1)}{n}$ 이상의 값들을 레버리지가 높은 데이터값이라고 한다.
	- `쿡의 거리Cook's distance` : 레버리지와 잔차의 크기를 합치며, 경험칙에 의하면 $\frac{4}{n-P-1}$ 보다 크면 영향력이 높다고 봄
- 샘플의 수가 적다면 이런 높은 레버리지를 갖는 데이터를 찾는 작업은 중요할 수 있다.

### 3. 이분산성, 비정규성, 오차 간 상관
- 대부분의 문제에서 데이터 과학자는 잔차 분포에 너무 많은 신경을 쓸 필요는 없다.
	- 공식적인 통계적 추론(가설 검정 & p 값)의 유효성과 관련이 있기 때문에, `예측 정확도`가 중요한 데이터 과학자에겐 별로 중요하진 않다.
- 데이터 과학자가 신경써야 하는 부분은, 잔차에 대한 가정을 기반으로 예상 값에 대한 신뢰 구간을 계산하는 방법이다.(`신뢰구간과 예측구간 참고`)
- <이분산성>  
	- 다양한 범위의 예측값에 따라 잔차의 분산이 일정하지 않은 것
	- 예측값이 어떤 경우엔 맞고, 어떤 경우엔 틀리다는 것을 나타내며, 얻은 모델이 불완전함을 의미한다.
	- `seaborn`에서 `regplot`을 쓸 때 보이는 영역을 의미하는 듯? (`ci`가 켜져있을 때)
		- 즉 그 영역이 넓을수록 그 X값에 대해선 잘 설명하지 못한다는 걸 의미함
	- 참고 ) 회귀모형 평가 시, 두 변수 사이의 관계를 시각적으로 강조하기 위해 산점도 평활기(`Scatterplot Smoother`)를 사용하는 게 좋다.

### 4. 편잔차 그림과 비선형성
- `편잔차그림` : 예측 모델이 예측 변수와 결과 변수 간의 관계를 얼마나 잘 설명하는지 시각화
	- 특잇점과 함께 가장 중요한 모델 진단 방법
- 기본 개념 : 하나의 예측 변수와 응답 변수 사이의 관계를, 모든 다른 예측변수로부터 분리하는 것
- 예측변수 $X_i$의 편잔차는 $편잔차 = 잔차 + \hat{b_i}X_i$ 값이다.  $\hat{b_i}$는 회귀변수의 추정치를 나타냄.

## 7. 다항회귀와 스플라인회귀
- `Polynomial Regression` : 회귀모형에 다항식(제곱항 이상)을 추가한 형태
- `Spline Regression` : 다항 구간들을 부드러운 곡선 형태로 피팅
- `Knot` : 스플라인 구간을 구분
- `Generalized Additive Model` : 자동으로 구간을 결정하는 스플라인 몸델
------------------
- X-y의 관계가 반드시 선형일 필요는 없다. 회귀분석에 비선형효과를 담기 위한 방법이 몇 가지 있다.

### 1. 다항식
- 제곱항 이상 추가한 거 의미함

### 2. 스플라인
- 고차항을 추가하는 건 회귀방정식에 "흔들림"을 초래한다.
- 이 경우 `스플라인`을 사용하는 게 더 나은데, `스플라인`은 고정된 점들 사이를 부드럽게 보간하는 방법을 의미한다.
- 다항회귀보다 훨씬 복잡하다.
	- 그래서 스플라인 항의 계수는 해석하기 어렵다.
- 적합도를 확인하기 위해, 시각화 방법을 사용하는 것이 유용하다.

### 3. 일반화기법모형(GAM)
- X-y 간 비선형관계가 있다는 걸 알았다고 할 때, 다항은 유연성이 부족하고 스플라인은 매듭을 지정해야 한다는 점이 있다.
- **GAM은 스플라인 회귀를 자동으로 찾는 기술**이다.
- `python`에서는 `pyGAM`이라는 라이브러리가 있다고 하네요

