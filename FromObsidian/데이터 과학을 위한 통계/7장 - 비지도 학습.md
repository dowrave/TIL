- 응답 변수(y)가 없는 상태에서 데이터로부터 의미를 이끌어내는 통계적 기법들.
- 비지도학습의 목표
	- `클러스터링` : 데이터의 의미 있는 그룹 찾기
	- `차원 줄이기` : 회귀, 분류 등 예측 모델에 입력으로 전달할 수 있다.
	- `탐색적 데이터 분석의 연장`으로 볼 수도 있다. 데이터와 다른 변수들 사이의 관계에 대한 통찰을 얻는 게 목적이 됨.
- 비지도학습은 회귀, 분류 문제 모두에서 예측에 중요한 역할을 한다.
	- 예를 들면 패턴이 비슷한 데이터들을 분류해 학습 과정을 더 빨리 시작할 수 있게 한다.
	- 회귀, 분류를 위한 중요한 기본 요소가 될 수도 있다. 

## 1. 주성분분석(PCA)
- **다수의 수치형 X들을 더 적은 수의 변수의 집합으로 나타내는 것.**
	- 이 새로운 변수들은 원래 변수들에 가중치를 적용한 선형 집합이다.
	- 이 새로운 변수의 집합을 `주성분`이라고 하며, **데이터의 차원을 줄일 수 있다.**
	- 주성분을 만드는 데 기존 변수에 쓰이는 가중치의 집합을 주성분의 부하(load)라고 한다.
$$ Z_i = W_{i,1}X_1 + W_{i,2}X_2 $$
- $Z_i$ 값이 주성분, $W_{i,k}$ 값이 가중치

### 주성분 계산
1. 1번째 주성분을 구하고자 PCA는 전체 변동을 최대한 설명하기 위한 예측변수의 선형결합을 구한다. $Z_1$
2. 같은 변수들을 이용해 새로운 2번째 변수 $Z_2$를 만들기 위해, 다른 가중치로 이 과정을 반복한다. 가중치는 $Z_1$과 $Z_2$가 서로 상관성이 없도록 결정한다. 
3. 원래 변수 $X_i$의 개수만큼 새로운 변수 $Z_i$를 구할 때 까지 계속한다.
4. 필요한 만큼의 주성분을 선택해 남겨놓는다.
5. 결과적으로 각 주성분에 대한 가중치 집합을 얻게 된다.
6. 원래 데이터를 이 가중치들을 적용해 새로운 주성분으로 변형한다

### 주성분 해석
- 주성분에 대한 이해를 돕기 위해 사용되는 2가지 방법이 있다.
1. `스크리그래프Scree Graph` : 주성분의 상대적인 중요도를 표시`R에서는 ScreePlot`으로 제공
2. 상위 주성분들의 가중치를 표시해볼 수도 있다. `R : tidyr 패키지의 gather 함수`

- 몇 개의 성분을 골라야 하는가?
	- 대부분의 변동성을 설명할 수 있는 성분들을 고르기 위한 특별한 규칙을 사용하는 방법
	- 스크리그래프를 활용할 수도 있음
		- 상위 n개의 주성분을 고른다
		- 누적 분산이 어떤 임계치까지 나타나는 상위 주성분을 고른다
		- 부하`load`를 살펴본다
	- CV는 좀 더 공식적인 방법이라고 할 수 있다.

## 2. K - Means Clustering
- `클러스터링(군집화)` : 데이터를 서로 다른 그룹으로 분류하는 기술
- `K-Means`은 최초로 개발된 클러스터링 기법이며, 알고리즘이 간단하고 큰 데이터에도 손쉽게 쓸 수 있어 아직도 널리 쓰인다.
- 과정은 대충 알 거임
	1. 데이터를 K개의 클러스터로 나눈다
	2. 각 점에 대해, 가장 가까운 클러스터에 해당하는 클래스를 할당한다. 
	3. 클러스터 이동 : 할당된 **클러스터의 평균과 포함된 데이터들의 거리 제곱합이 최소**가 되도록 한다.
	4. 2, 3을 반복함.
- 참고 ) `표준화(정규화)`하지 않으면 다양한 변수 중 스케일이 큰 변수가 지배적이 됨
- 한편 정확한 K평균의 해를 계산하는건 매우 어렵기 때문에 국소 최적화된 해를 휴리스틱한 방법으로 계산한다. 

### 클러스터 해석
- 가장 중요한 건 `클러스터 크기`와 `클러스터 평균`이다.
	- `클러스터 크기`의 편차가 크다면 특잇점이 있거나 어떤 그룹이 유독 데이터로부터 멀리 떨어져있을 수 있다.

#### 클러스터 개수 선정
- `팔꿈치 방법Elbow Method` : 클러스터 세트가 데이터 분산의 대부분을 설명하는 시점을 알려줌.
	- 이 상황에 클러스터가 더 추가된다면, 분산에 대한 기여도가 상대적으로 작아진다.
	- 부드럽게 분산 증가율이 올라간다면 뚜렷하게 꼽을 지점이 없을 수도 있지만, 데이터의 특성을 밝혀준다는 점에서 팔꿈치 방법이 가치가 있다.
- 유지할 클러스터 수를 평가할 때 중요한 테스트들
	- 새로운 데이터에서 클러스터들이 유지될 가능성이 있는가?
	- 클러스터는 해석 가능한가?
	- 데이터의 일반적인 특성과 관련 있는가? 혹은 특정 데이터만 반영하는가?
- CV로 위 사항들을 평가할 수 있다. 

## 3. 계층적 클러스터링Hierarchical Clustering
- K평균 대신 사용함.
- K평균보다 유연
- 수치형 변수가 아니어도 쉽게 적용이 가능
- 특잇점, 비정상적인 레코드에 더 민감
- 직관적인 시각화 또한 가능
- 비용이 따른다 : 대규모 데이터에 적용하긴 힘들고 수만 개 정도의 적당한 크기에도 많은 리소스를 먹음.

- 기본 구성 요소
1. $d_{i, j}$ : 두 레코드 $i$, $j$ 사이의 거리를 측정하기 위한 거리 측정 지표
2. 두 클러스터 A, B 사이의 거리를 측정하기 위핸 비유사도 측정 지표 $D_{A, B}$

- 각 레코드를 개별 클러스터로 설정하여 시작하고, 가장 가까운 클러스터를 결합해나간다.

### 1. 덴드로그램
- 트리모델처럼 자연스러운 시각화가 가능하다.

### 2. 병합 알고리즘
- 유사한 클러스터들을 반복적으로 병합한다. 단일 레코드에서 시작해 점점 더 큰 클러스터들을 만든다.
1. 유클리드 거리를 이용해 두 레코드 사이의 거리 $d_{i, j}$를 측정한다.
2. 클러스터 간의 거리 : $A = (a_1, a_2, ..., a_m)$과 $B = (b_1, B_2, ..., b_q)$ 
	- 두 클러스터의 구성원들 사이의 거리를 사용해 비유사도 $D(A, B)$를 측정할 수 있다.
		- `완전연결 방식` : $D(A, B) = max {d(a_i, b_j)}$  (모든 i, j 쌍에 관해)
			- 비유사도를 모든 쌍 사이의 가장 큰 차이로 정의함

- 병합 알고리즘의 주요 단계
1. 모든 레코드에 대해 각각 단일 클러스터로 구성된 집합을 만듦
2. 모든 쌍의 클러스터 $k, l$ 사이의 비유사도 $D(C_k, C_l)$을 계산함
3. $D(C_k, C_l)$에 따라 가장 가까운 두 클러스터를 병합함
4. 둘 이상의 클러스터가 남았다면  2단계로 돌아간다. 1개만 남았을 때 알고리즘을 멈춘다.

### 3. 비유사도 측정
- `완전연결`, `단일연결single linkage`, `평균연결`, `최소분산` 4가지 지표가 있다. 
- `단일연결` : 두 클러스터의 레코드 간 최소 거리를 사용한다.$D(A, B) = min {d(a_i, b_j)}$ 
	- `greedy` 방법이며, 결과로 나온 클러스터는 서로 크게 다른 요소들을 포함할 수도 있다.
- `평균 연결` : 모든 거리 쌍의 평균을 사용, `완전과 단일을 절충`
- `워드 기법 = 최소 분산` : 클러스터 내의 제곱합을 최소화 - `K평균`과 유사함

## 4. 모델 기반 클러스터링
- `계층적, K-means는 모두 휴리스틱한 클러스터링 방법이다.`
- `모델 기반 클러스터링`은 통계 이론에 기초하며, 클러스터링의 성질과 수를 결정하는 더 엄격한 방법을 제공한다.

### 1. 다변량정규분포(Mutlivariate Normal Distribution)
- $p$개의 변수 집합 $X_i, ... X_p$에 대해 정규분포를 일반화함.
	- 분포는 평균 집합 $\mu = \mu_1, ..., \mu_p$와 공분산행렬 $\Sigma$로 정의된다. 
	- 공분산행렬 $\Sigma$는 결과적으로 $p \times (p-1) - p$개의 공분산 항이 존재하게 된다. 따라서 $p \times (p-1)$ 개의 변수를 갖게 된다. 
	- 이 분포는 $(X_1, X_2, ... ,X_p)\tilde{N_p}(\mu, \Sigma)$처럼 표시한다.
		- 변수들이 모두 정규분포를 따르고, 전체 분포가 변수의 평균 벡터와 공분산행렬에 의해 완전히 설명됨을 의미한다.

### 2. 정규혼합
- 모델 기반 클러스터링의 핵심 아이디어는 각 레코드가 K개의 다변량 정규 분포 중 하나로부터 발생했다고 가정하는 것이다. 
- 각 분포의 $\mu, \Sigma$ 값은 서로 다름. 
- 모델기반 클러스터링의 목적은 **데이터를 가장 잘 설명하는 다변량정규분포를 찾는 것**이다. 

### 3. 클러스터 개수 결정하기
- `mclust`는 클러스터 수를 자동으로 선택하는데, `베이즈 정보 기준(BIC)` 값이 가장 큰 클러스터의 개수를 선택하도록 동작하기 때문이다. 

## 5. 스케일링과 범주형 변수
- 순서가 없는 요인 변수는 원핫 인코딩을 이용한다. 
- 스크리그래프로 어떤 요소가 전체 부하량을 지배하는 지 알 수 있다. 
- 범주형의 경우, 순서형이나 이진형 변수를 이용해 수치형 데이터로 변환해야 한다. 연속형, 이진형 변수가 섞여있다면 비슷한 스케일이 되도록 변수의 크기를 조정해야 하며, `고워 거리`가 대표적인 방법이다. 
	1. 각 레코드의 변수 i, j의 모든 쌍에 대해 거리 $d_{i, j}$를 계산한다.
	2. 각 $d_{i, j}$의 크기를 최소가 0, 최대가 1이 되도록 스케일을 조정한다.
	3. 변수 간 스케일된 거리를 모두 더한 뒤 평균 혹은 가중평균을 이용해 거리 행렬을 구한다. 