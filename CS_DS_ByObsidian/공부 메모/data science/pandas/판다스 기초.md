#pandas

```python
print(df.columns) # 데이터프레임 column 이름들
print(df.dtypes) # 데이터프레임 각 column의 데이터타입
```

#### 판다스 자료형
1. **object** : (python : string) - **문자열**
2. int64 : (python : int) - 정수
3. float64 : python : float - 실수
4. **datetime64** : datetime - **파이썬** **표준 라이브러리 datetime이 반환하는 자료형**

##### 열 추출하기
- 열 이름만으로 특정 열을 뽑아낼 수 있다.
```python
new_srs = df['column'] # Series로 추출
new_df = df[['column']] # DataFrame으로 추출 : [[]] 내에는 1개 이상의 column name이 들어감
```

##### 행 추출하기
1. `.loc()` : **인덱스** 단위로 추출 
- `주의 : 파이썬에서 말하는 인덱스랑 의미가 조금 다름` : **Pandas의 인덱스는 행의 이름**
```python
df.loc['index_name'] # loc는 ()가 아니라 []를 이용한다
df.loc[['idx1', 'idx2']] # 여러 행을 추출하고 싶다면 [[]]를 쓴다
```

2. `.iloc()` : 행 번호 단위로 추출 (어떤 경우에도 가장 위에서 0부터 시작함)  
	- 따라서 loc와 달리 -1 인덱싱을 쓸 수 있다
```python
df.iloc[0]
df.iloc[-1]
df.iloc[[0, 99, 999]]
```

##### `.loc`, `iloc`으로 특정 행 특정 열의 위치 특정 가능


```python
df.loc[:, ['year', 'pop']] # 모든 행의 'year', 'pop' column만 특정
df.iloc[:, [2, 4, -1]] # 모든 행의 3개 열만 특정
```
- 여러 행/열을 특정하고 싶다면 슬라이싱 기능을 이용한다
	- 근데 보통 열을 슬라이싱 하는 경우는 잘 없음. (이름으로 특정하기 때문에 이름 하나하나를 직접 입력하는 식으로 출력함)
- 그래서 보통은 파이썬 리스트의 기능인 **슬라이싱**을 이용한다.
- 기존 데이터프레임에서 새로운 데이터프레임을 만들 때 이 기능을 사용하자
	- 헷갈려서 `df[:, [col]]` 이런 식으로 쓰는 경우가 있는데 Pandas에서 경고 문구 나옴


### 기초 통계 계산하기
###### `groupby('colName')`
   - 아래 예시와 같이 이용한다.
   - **groupby를 쓴다 = 묶어서 어떤 통계치를 보겠다**

1. 평균
```python
df.groupby('year')['lifeExp'].mean()
```
	1. 어떤 column으로 묶을 건지('year')
	2. 통계치를 이용하기 위해 어떤 column을 추출할 건지('lifeExp')
	3. 추출한 column의 어떤 통계값을 이용할 것인지
	- 위 3가지 요소가 다 들어가야 groupby를 제대로 쓰는 것임
	- 실사용시 헷갈리는 경우가 많아 서술해놓는다.

2. 빈도수(몇 번 등장했는가)
```python
df.groupby('continent')['country'].nunique()
```
- `nunique()` 메소드를 이용한다
	- **'country' 라는 column 고유값이 몇 개** 들어가 있는가를 나타냄
- `count()`와의 차이점
	- count는 그냥 각 continent에 들어간 **행의 수**임

##### 판다스 기초 통계 부분은 따로 마크다운으로 정리해도 좋을 듯
------------------------


### 시리즈와 데이터프레임
1. 시리즈 속성 & 메서드

```python
row.index # 시리즈의 인덱스
row.keys() # 시리즈의 인덱스
	# 인덱싱
	row.index[0]
	row.keys()[0]

row.values # 시리즈의 데이터
```
	- 속성 : 뒤에 딸려오는데 ()가 붙지 않음
	- 메서드 : 딸려오는데 ()가 붙음
```python
# 그 외 시리즈 메서드들
	# mean, min, max, std는 생략함
append : 2개 이상의 시리즈 연결
describe : 요약 통계량 계산
drop_duplicates : 중복값 없는 시리즈 반환
equals : 시리즈에 해당 값 가진 요소 있는지 확인
get_values : 시리즈 값 구하기(values와 동일)
isin : 시리즈에 포함된 값이 있는지 확인
median : 중간값
replace : 특정 값을 가진 시리즈 값 교체
sample : 시리즈에서 임의의 값 반환 (랜덤한 key - value 반환)
sort_values : 값 정렬
to_frame : 시리즈 -> 데이터프레임 전환
```

##### 써둘만한 것들
1. `Datetime`으로 변환 : `pd.to_datetime`
```python
pd.to_datetime(df['column'], format = '%Y-%m-%d')
```
2. 데이터 섞기 : `random.shuffle()`을 이용함
```python
import random
random.shuffle(df['column'])
print(df['column'])
```
3. 열 삭제하기
```python
new_df = df.drop(['column'], axis = 1)
```


##### 데이터 저장 및 불러오기
1. pickle 
- 시리즈도 가능
```python
df.to_pickle('저장경로')
pd.read_pickle('저장경로')
```
2. csv, tsv
```python
df.to_csv('저장경로') # csv
df.to_csv('저장경로', sep = '\t') # tsv
```
3. 엑셀
`.xls` 파일: `xlwt`라이브러리 필요
`xlsx` 파일 : `openpyxl` 라이브러리 필요

-------------------
##### 데이터 프레임에 함수 적용하기
```python
df['new_col'] = df['col'].apply(function)
```

##### 데이터 연결하기
1. `pd.concat([df1, df2, ...])`
	- df - df : 행방향(즉 새로운 행들이 추가됨)
		- 열방향으로 추가하고 싶다면 `pd.concat([df1, df2, ...], axis = 1)`
		- 이 때 column 이름들이 모두 같다면 같은 이름의 column들이 뒤에 더 생김
		- 다른 column들이 있다면 없는 값에는 `NaN`값이 뜰 것
			- 이를 방지하는 방법은 `pd.concat([dfs], axis = 1, join = 'inner'`
	- 시리즈 - df : 열방향(즉 새로운 열이 추가됨)
		- 시리즈는 열 이름이 없기 때문에 행으로 연결되지 않음
		- 새로운 행(시리즈)을 추가하고 싶다면, **시리즈를 데이터프레임으로 바꿔야 함(공통)**
			1. 그 후 `pd.concat([df1, series])`
			2. `df.append(series)`
			3. 인덱스 초기화까지 하고 싶다면, 데이터프레임으로 바꾼 시리즈에 column name을 부여해야 함
```python
data_dict = {'A' : 'n1', 'B' : 'n2', "C" : 'n3', 'D' : 'n4'}
print(df1.append(data_dict, ignore_index = True))
```

2. `pd.merge()`
- 기본적으로 `inner` 조인
- 메서드를 사용한 df를 left, 인자 df를 right로 지정함
- `df1.merge(df2, left_on = 'df1_col', right_on = 'df2_col')`
	- 두 `on`의 값이 일치한다면, 병합함
		- `on` 내에 들어가는 값은 여러 개여도 괜찮음

##### 누락값 처리하기

1. 누락값 수 파악하기
- `df.count()` : 결측치가 아닌 값의 개수
	- `df.shape[0] -  df.count()` 로 결측치의 개수를 알 수 있음
		- **브로드캐스팅이 적용**됨
- `np.count_nonzero(df.isnull())`
- `np.count_nonzero(df['col'].isnull())`
- 직관적이진 않으나 `df.info()`로도 파악 가능

2. 누락값 변경하기
	1. `df.fillna(0)` : 데이터프레임이 매우 크고, 메모리를 효율적으로 써야 할 필요가 있을 때
	2. `df.fillna(method = 'ffill')` 
		- 누락값이 나타나기 전 값으로 바꿈(6행이 누락값이라면 5행의 값을 씀)
			- 0행이 누락값이면 `NaN`으로 남아 있다.
	3. `df.fillna(method = 'bfill')`
		- 누락값 뒷값으로 바꿈
	4. `df.fillna(method = 'interpolate'`
		- 누락값 양쪽 값을 이용함
	5. `df.dropna()` : 누락값 삭제
		- **누락값을 포함한 행 모두가 삭제**되므로 분석가가 잘 판단해야 함

3. 누락값을 포함한 데이터 계산하기
	- 통계 메서드에는 `skipna = True`라는 옵션이 존재한다. 이걸 쓰시오
		- 통계 메서드 : `.sum()` 같은 거 

#### 넓은 데이터 다루기
- 열 자체가 어떤 값을 의미하는 경우, 열이 가로로 넓게 늘어선 형태가 됨 `넓은 데이터`
- **melt 메서드 : 지정한 열의 데이터들을 모두 행으로 정리함**
	- `wide` 데이터를 `long` 데이터로 정리할 수 있음
	- 
```python
pd.melt(df,
	   id_vars = "고정할 column 이름"(복수는 []로 받음)),
	   value_vars = "행으로 옮길 column 이름"
	   var_name = "합쳐진 데이터들 col 이름",
	   value_name = "데이터들의 갯수 col 이름"
```
- `value_vars`를 따로 지정하지 않으면 `id_vars` 빼고 나머지 열들이 행으로 감

##### (underscore)로 구분된 특성을 새로운 특성으로 추가하기
```python
Index(['Date', 'Day', 'Cases_Guinea', 'Cases_Liberia', 'Cases_SierraLeone',
       'Cases_Nigeria', 'Cases_Senegal', 'Cases_UnitedStates', 'Cases_Spain',
       'Cases_Mali', 'Deaths_Guinea', 'Deaths_Liberia', 'Deaths_SierraLeone',
       'Deaths_Nigeria', 'Deaths_Senegal', 'Deaths_UnitedStates',
       'Deaths_Spain', 'Deaths_Mali'],
# 라는 column들을 가진 에볼라 데이터가 있다고 할 때,
ebola = pd.melt(ebola, id_vars = ['Date', 'Day'])

# 결과 Variable Column 밑에 "Cases_Guinea", ... "Deaths_Mail" 까지 들어가게 됨
variable_split = ebola_long.variable.str.split('_')
	# [Cases, Guinea] 라는 식으로 나옴
ebola_long['status'] = variable_split.str.get(0)
ebola_long['country'] = variable_split.str.get(1)
```

###### melt, pivot_table 요약
1. `melt` : 열의 이름을 행으로 내림
	- 내렸으니까 값에 해당하는 열 이름을 새로 붙여줄 필요가 생김
2. `pivot_table` : 열에 있는 값들을 새로운 열로 만듦
	- 값을 열로 올렸으니까 값에 해당하는 것들을 넣을 필요가 생김
	- `**reset_index**` 를 적용해 꼭 접근할 수 있게 하자
예시
```python
"""
        id  year  month element  d1    d2    d3  d4    d5  d6  ...  d22   d23  \
0  MX17004  2010      1    tmax NaN   NaN   NaN NaN   NaN NaN  ...  NaN   NaN   
1  MX17004  2010      1    tmin NaN   NaN   NaN NaN   NaN NaN  ...  NaN   NaN   
2  MX17004  2010      2    tmax NaN  27.3  24.1 NaN   NaN NaN  ...  NaN  29.9   
3  MX17004  2010      2    tmin NaN  14.4  14.4 NaN   NaN NaN  ...  NaN  10.7   
4  MX17004  2010      3    tmax NaN   NaN   NaN NaN  32.1 NaN  ...  NaN   NaN   

   d24  d25  d26  d27  d28  d29   d30  d31  
0  NaN  NaN  NaN  NaN  NaN  NaN  27.8  NaN  
1  NaN  NaN  NaN  NaN  NaN  NaN  14.5  NaN  
2  NaN  NaN  NaN  NaN  NaN  NaN   NaN  NaN  
3  NaN  NaN  NaN  NaN  NaN  NaN   NaN  NaN  
4  NaN  NaN  NaN  NaN  NaN  NaN   NaN  NaN
"""

# 1. 위 df에 melt 적용
weather_melt = pd.melt(weather, id_vars=['id', 'year', 'month', 'element'], var_name = 'day', value_name = 'temp')
print(weather_melt.tail())

# 1. 결과 (melt : d1~d31(열)이 값으로 내려왔다)
"""
          id  year  month element  day  temp
677  MX17004  2010     10    tmin  d31   NaN
678  MX17004  2010     11    tmax  d31   NaN
679  MX17004  2010     11    tmin  d31   NaN
680  MX17004  2010     12    tmax  d31   NaN
681  MX17004  2010     12    tmin  d31   NaN
"""
##########################

# 2. melt에 pivot_table 적용
weather_tidy = weather_melt.pivot_table(index = ['id', 'year', 'month', 'day'], # 위치 유지할 열들 이름
                                        columns = 'element', # 피벗할 열 이름 
                                        values = 'temp', # 새로운 열의 데이터가 될 열 이름
                                        dropna = False
                                                )
weather_tidy.head()

# 2. 결과 (pivot_table : element의 값 tmax, tmin이 열로 올라갔다)
"""
element                 tmax  tmin
id      year month day            
MX17004 2010 1     d1    NaN   NaN
                   d10   NaN   NaN
                   d11   NaN   NaN
                   d12   NaN   NaN
                   d13   NaN   NaN
"""

# pivot_table 후에는 꼭 reset_index()를 해줘서 해당 column에 접근할 수 있게 한다
weather_tidy_flat = weather_tidy.reset_index()

"""
element       id  year  month  day  tmax  tmin
0        MX17004  2010      1   d1   NaN   NaN
1        MX17004  2010      1  d10   NaN   NaN
2        MX17004  2010      1  d11   NaN   NaN
3        MX17004  2010      1  d12   NaN   NaN
4        MX17004  2010      1  d13   NaN   NaN
"""
```

##### 중복 데이터 처리 : `drop_duplicate()`

##### 대규모 데이터 처리 
- 대규모 데이터는 `glob`을 이용해 여러 파일을 불러오며 `pd.concat`을 쓰라고 하는데, 프로젝트 01할 때를 생각해보면 좋은 방법이 아님
	- 데이터프레임의 크기가 `pd.cocnat`을 하면서 점점 커지는데, 이 때 그냥 붙는 게 아니라 `새로운 메모리 할당 - 복사 - 붙여넣기` 과정을 반복하기 때문임
	- 이 때 유용한 방법이 데이터프레임의 총 크기를 알 수 있다면, 빈 공간을 먼저 확보하는 방법이 있던 걸로 기억함

##### 데이터 타입 변환 : astype()
```python
df['columns'].astype(str)
```

##### Column에 숫자 + 문자가 섞인 경우
- 숫자 column에 문자가 섞이면 `object`로 바뀜
- `to_numeric`을 사용하면 문자 -> 실수 변환은 불가능하지만, `errors = (*raise, coerce, ignore)`등을 지정, 오류를 제어할 수 있다.
	- `ignore` : 오류를 무시 - 여전히 `object`
	- `coerce` : 문자열이 `NaN`으로 바뀌며 Column의 dtype을 바꿈
- `downcast` 메서드 : 숫자가 차지하는 용량을 줄임
	- ex) `float64` - > `float32`

##### 카테고리 자료형
- **이산형 자료라면 `Categorical`을 쓰는 게 용량, 속도에서 더 효율적**이다.
- 동일한 문자열이 반복되어 데이터를 구성할 때 사용함
```python
tips['sex'] = tips['sex'].astype('category')
```

#### apply 사용하기
- 그룹 연산 : **분할 + 반영 + 결합**
	- 분할 : 어떤 기준으로 데이터를 나눔
	- 반영 : 함수 등을 적용해 데이터 처리
	- 결합 : 처리한 결과를 다시 합침

##### 집계 메서드
```
count : 누락값 제외 데이터 수  
size : 누락값 포함 데이터 수
mean, std, min, max, var, sum
quantile(q = 0.25, 0.5, 0.75) : 1, 2, 3사분위수
sem : 평균의 표준편차
describe : 아는 그거 맞다
first : 1번째 행
last : 마지막 행
nth : n번째 행
```

##### 집계 메서드로 내가 원하는 걸 구현할 수 없다면?
- 이 때 `.agg` 메서드가 들어간다.
```python
def my_mean(values):
    n = len(values)
    sum = 0
    for value in values:
        sum += value
        
    return sum / n

agg_my_mean = df.groupby('year').lifeExp.agg(my_mean)
print(agg_my_mean)
```
- `year` column에 my_mean이라는 사용자 지정 함수를 적용함

- `.agg`에 들어가는 함수의 인자가 2개라면?
```python
def my_mean_diff(values, diff_value):
	어쩌구 저쩌구


agg_mean_diff = df.groupby('year').lifExp.agg(my_mean_diff, diff_value = global_mean)
```
- 1번째 인자는 `사용자 지정 함수`, 2번째 인자에 파라미터 값을 전달함

- 여러 집계 메서드 한번에 쓰기
```python
# 1. 한 column에 여러 집계 메서드 쓰기 
df.groupby('year').lifeExp.agg([np.count_nonzero, np.mean, np.std])

# 2. 여러 column에 각각의 집계 메서드 적용하기
df.groupby('year').agg({'lifeExp' : 'mean', 'pop' : 'median', 'gdpPer-cap' : 'median'})
```

#### 데이터 변환 메서드
1. 표준점수 계산
```python
def my_zscore(x):
	return (x - x.mean()) / x.std()

transform_z = df.groupby('year').lifeExp.transform(my_zscore)
```

2. 누락값 평균값으로 처리하기
- 중요) 어떤 평균값을 쓸 것이냐가 중요함
	- ex) 여성이 남성보다 적다면, 여성과 남성의 평균을 따로 구한 값을 넣어야 함
```python
def fill_na_mean(x):
	avg = x.mean()
	return x.fillna(avg)

total_bill_group_mean = tips_10.groupby('sex').total_bill.transform(fill_na_mean)
```

##### 데이터 필터링
- 그룹화한 데이터에서 원하는 데이터 걸러내기`.filter()`
```python
tips_filtered = tips.groupby('size').filter(lambda x : x['size'].count() >= 30)
```

##### 그룹 오브젝트
1. 소속된 그룹 보기 `.groups`
- 그룹 오브젝트에서 계산할 수 없는 열들은 자동으로 연산에서 제외된다.
- 특정 그룹만 뽑기 : `get_group('뽑을 값 이름')`
	- 반복문을 써서 뽑아낼 수도 있음

##### 파이썬의 `IN`을 조건식에서 쓰기 
- `df[df[column].isin(list)]`
  - `df.column` 값이 `list` 내에 있으면 `True`, 없으면 `False`를 반환한다.
 
##### groupby -> 데이터프레임
`groupby().agg().to_frame()`
 - 바로 데이터프레임이 나온다.

##### 연속형 값을 범주형 값으로 바꾸기
- `pd.cut(df['col'], bins, ...`

#### 조건을 만족하는 row에 열 추가하기
`df.loc[df['column name'] condition, 'new column name'] = 'value if condition is met'`

#### groupby 결과 column에 추가하기


#### 여러 column에 tuple 조건 가하기
```python
# 데이터프레임에서 특정 조건을 만족하는 column들을 빼고, 튜플로 저장
unique_id = test_df[test_df['program_number'].isin(['1A', '1X', '2B', '3X'])][['track_id', 'race_date', 'race_number']]
unique_id = list(unique_id.itertuples(index = False, name = None))

# unique_id는 튜플들을 담은 리스트

# 튜플 조건 가하기
test_df[test_df[['track_id', 'race_date', 'race_number']]
		.apply(tuple, 1)
		.isin(unique_id)][:].head(30)
```
- 조건식에는 튜플에 담은 열과 동일한 조건을 가하고, `.apply(tuple, 1)`을 가해줌.

#### Categorical 수동으로 sorting하기
- 예제) [원본 링크](https://towardsdatascience.com/how-to-do-a-custom-sort-on-pandas-dataframe-ac18e7ea5320)
```python
df = pd.DataFrame({  
'cloth_id': [1001, 1002, 1003, 1004, 1005, 1006],  
'size': ['S', 'XL', 'M', 'XS', 'L', 'S'],
})
```

- 그냥 더 간단하게 이용할 수 있는 방식만 보자 : `CategoricalDtype`
```python
from pandas import CategoricalDtype

# 범주 & 순서를 같이 전달함
cat_size_order = CategoricalDtype(  
['XS', 'S', 'M', 'L', 'XL'],  
ordered=True
)

# astype을 적용함
df['size'] = df['size'].astype(cat_size_order)

df.sort_values('size')
```