- 이전 : [[2-2. 모델 파이프라인]]

- db에서 최신 100개 데이터를 출력하는 쿼리문 작성
```SQL
SELECT *
  FROM iris_data
 ORDER BY id DESC
 LIMIT 100  
```

> - (복습) `psql`로 쿼리 접속하기 (이건 로컬에서 진행하는 듯)
```sh
PGPASSWORD=mypassword psql -h {name} -p 5432 -U myuser -d mydatabase
```
>- `name` : 로컬에서 접속하면 `localhost`, 컨테이너에서 접속하면 `postgres-server`
	- 컨테이너 접속 : `docker exec -it {container id} /bin/bash` 후 위 커맨드 입력


- 위 쿼리를 파이썬 파일에 넣어서 가져옴
- `db_train.py`
```python
import pandas as pd
import psycopg2

db_connect = psycopg2.connect(host='localhost',
							 database = 'mydatabase',
							 user = 'myuser',
							 password = 'mypassword')
							 
df = pd.read_sql("""SELECT * 
					FROM iris_data
					ORDER BY id DESC
					LIMIT 100""",
					db_connect)
print(df.head(5))
```

- 추가 : DB에는 데이터가 계속 쌓이고 있기 때문에 불러올 때마다 데이터가 바뀐다. 따라서 지금 학습한 모델에 사용된 데이터를 `csv`파일로 저장한다.
`db_train.py`
```python
df.to_csv('data.csv', index = False)
```

- 전체 `db_train.py`
```python
# db_train.py  
import joblib  
import pandas as pd  
import psycopg2  
from sklearn.metrics import accuracy_score  
from sklearn.model_selection import train_test_split  
from sklearn.preprocessing import StandardScaler  
from sklearn.pipeline import Pipeline  
from sklearn.svm import SVC  
  
# 1. get data  
db_connect = psycopg2.connect(host="localhost", database="mydatabase", user="myuser", password="mypassword")  
df = pd.read_sql("SELECT * FROM iris_data ORDER BY id DESC LIMIT 100", db_connect)  
df = df.drop("timestamp", axis = 1)
X = df.drop(["id", "target"], axis = 1)  
y = df["target"]  
X_train, X_valid, y_train, y_valid = train_test_split(X, y, train_size=0.8, random_state=2022)  
  
# 2. model development and train  
model_pipeline = Pipeline([("scaler", StandardScaler()), ("svc", SVC())])  
model_pipeline.fit(X_train, y_train)  
  
train_pred = model_pipeline.predict(X_train)  
valid_pred = model_pipeline.predict(X_valid)  
  
train_acc = accuracy_score(y_true=y_train, y_pred=train_pred)  
valid_acc = accuracy_score(y_true=y_valid, y_pred=valid_pred)  
  
print("Train Accuracy :", train_acc)  
print("Valid Accuracy :", valid_acc)  
  
# 3. save model  
joblib.dump(model_pipeline, "db_pipeline.joblib")  
  
# 4. save data  
df.to_csv("data.csv", index=False)
```

- 전체 `validate_save_model.py`
```python
# db_validate_save_model.py  
import joblib  
import pandas as pd  
from sklearn.metrics import accuracy_score  
from sklearn.model_selection import train_test_split  
  
# 1. reproduce data  
df = pd.read_csv("data.csv")  
X = df.drop(["id", "target"], axis="columns")  
y = df["target"]  
X_train, X_valid, y_train, y_valid = train_test_split(X, y, train_size=0.8, random_state=2022)  
  
# 2. load model  
pipeline_load = joblib.load("db_pipeline.joblib")  
  
# 3. validate  
load_train_pred = pipeline_load.predict(X_train)  
load_valid_pred = pipeline_load.predict(X_valid)  
  
load_train_acc = accuracy_score(y_true=y_train, y_pred=load_train_pred)  
load_valid_acc = accuracy_score(y_true=y_valid, y_pred=load_valid_pred)  
  
print("Load Model Train Accuracy :", load_train_acc)  
print("Load Model Valid Accuracy :", load_valid_acc)
```

- 다음 : [[3-1. MLflow Backend Store 구축]]