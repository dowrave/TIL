{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "2_NLP_Twitter_Disaster_Classification.ipynb",
      "private_outputs": true,
      "provenance": [],
      "authorship_tag": "ABX9TyPkjoKZur+Awan0Oxy1x+U5",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dowrave/PythonToKaggle/blob/main/2_NLP_Twitter_Disaster_Classification.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "uploaded = files.upload()\n",
        "for fn in uploaded.keys():\n",
        "  print('User uploaded file\"{name}\" with length {length} bytes'.format(name = fn, length = len(uploaded[fn])))\n",
        "\n",
        "!mkdir -p ~/.kaggle/\n",
        "!mv kaggle.json ~/.kaggle/ \n",
        "!chmod 600 ~/.kaggle/kaggle.json"
      ],
      "metadata": {
        "id": "BgGiVwlPjSws"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kCxAwHEVi4u3"
      },
      "outputs": [],
      "source": [
        "!kaggle competitions download -c nlp-getting-started"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip nlp-getting-started.zip"
      ],
      "metadata": {
        "id": "RZPwDPlajMlp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 용량 확인하는 코드\n",
        "import os\n",
        "DATA_PATH = \"./\"\n",
        "for file in os.listdir(DATA_PATH):\n",
        "  if 'csv' in file and 'zip' not in file:\n",
        "    print(file.ljust(30) + str(round(os.path.getsize(file)/ 1000000, 2)) + 'MB')"
      ],
      "metadata": {
        "id": "SRFiVhSMj8Bx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "train = pd.read_csv('./train.csv')\n",
        "test = pd.read_csv('./test.csv')"
      ],
      "metadata": {
        "id": "HDGcqT_7kIWC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train.head()"
      ],
      "metadata": {
        "id": "VSevBjBSkYzD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test.head()"
      ],
      "metadata": {
        "id": "ewE09qwfkcr2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train.info()"
      ],
      "metadata": {
        "id": "Yjeh2up2ke82"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test.info()"
      ],
      "metadata": {
        "id": "rqmhdVLcki1H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 탐색적 자료 분석\n",
        "- 분류 문제에서 가장 중요한 건 종속 변수를 시각화해서 분포를 확인하는 것이다\n",
        "  - 종속 변수(Y)는 보통 비대칭인 경우가 더 많기 때문에 이러한 비대칭 데이터를 어떻게 샘플링하여 학습시킬 것인지가 핵심임"
      ],
      "metadata": {
        "id": "AZkt2bWIknAf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns"
      ],
      "metadata": {
        "id": "56sxtQgekm7f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "news_class = train['target'].value_counts()\n",
        "labels = ['Non-Disaster', 'Disaster']\n",
        "\n",
        "fig, ax =  plt.subplots(figsize = (10, 6))\n",
        "ax.bar(labels, news_class, color = ['green', 'orange'])\n",
        "\n",
        "fig.show()"
      ],
      "metadata": {
        "id": "F5wZRPrqkjoo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "disaster_tweet_len = train[train['target'] == 1]['text'].str.len()\n",
        "non_disaster_tweet_len = train[train['target'] == 0]['text'].str.len()\n",
        "\n",
        "disaster_tweet_len # 단어 수가 아니라 글자 수임"
      ],
      "metadata": {
        "id": "rIuuSJQsk6EF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fig, ax = plt.subplots(1, 2, figsize=(12, 6))\n",
        "ax[0].hist(disaster_tweet_len, color = 'green')\n",
        "ax[0].set_title('Disaster Tweet Length')\n",
        "\n",
        "ax[1].hist(non_disaster_tweet_len, color = 'orange')\n",
        "ax[1].set_title('Non Diaster Tweet Length')\n",
        "\n",
        "fig.suptitle('All words in Tweets')\n",
        "fig.show()"
      ],
      "metadata": {
        "id": "41Oy5h8Glh9-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 두 그래프의 분포가 비슷한 모양이지만(?) 140자가 넘을 때 Non-Disaster의 글자 수가 급감하는 경향이 있음\n",
        "# 이를 Boxplot으로 확인해보자\n",
        "fig,ax = plt.subplots(1, 2, figsize = (12, 6))\n",
        "ax[0].boxplot(disaster_tweet_len, labels=['counts'], \n",
        "              showmeans = True) # 평균값을 그래프에 표시함\n",
        "ax[0].set_title(\"Disaster Tweet Length\")\n",
        "\n",
        "ax[1].boxplot(non_disaster_tweet_len, labels=['counts'], \n",
        "              showmeans = True) # 평균값을 그래프에 표시함\n",
        "ax[1].set_title(\"Non Disaster Tweet Length\")\n",
        "\n",
        "fig.suptitle('All words in Tweets')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "VWZv20-Oltqd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(disaster_tweet_len.describe(), '\\n', '-'*20, '\\n', non_disaster_tweet_len.describe())\n"
      ],
      "metadata": {
        "id": "_8PGQWYWmLUT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# WordCLoud로 사용된 데이터의 빈도수를 확인할 수 있다.\n",
        "from wordcloud import WordCloud, STOPWORDS\n",
        "\n",
        "disaster_tweet_keywords = dict(train[train['target'] == 1]['keyword'].value_counts())\n",
        "non_disaster_tweet_keywords = dict(train[train['target'] == 0]['keyword'].value_counts())\n",
        "\n",
        "\n",
        "stopwords = set(STOPWORDS)\n",
        "disaster_wordcloud = WordCloud(stopwords = stopwords,\n",
        "                               width = 800, height = 400,\n",
        "                               background_color = 'white').generate_from_frequencies(disaster_tweet_keywords)\n",
        "non_disaster_wordcloud = WordCloud(stopwords = stopwords,\n",
        "                               width = 800, height = 400,\n",
        "                               background_color = 'white').generate_from_frequencies(non_disaster_tweet_keywords)\n",
        "\n",
        "fig, ax = plt.subplots(1, 2, figsize=(16, 10))\n",
        "ax[0].imshow(disaster_wordcloud, interpolation = 'bilinear')\n",
        "ax[0].axis('off')\n",
        "ax[0].set_title('Disaster Tweet')\n",
        "ax[1].imshow(non_disaster_wordcloud, interpolation = 'bilinear')\n",
        "ax[1].axis('off')\n",
        "ax[1].set_title('Non Disaster Tweet')\n",
        "\n",
        "fig.show()"
      ],
      "metadata": {
        "id": "3m5roXDImssb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 와! 영어시간!\n",
        "- derailment : 탈선\n",
        "- wreckage : 난파 \n",
        "- 재난 트윗은 '명사'가 주로 쓰임\n",
        "- 비재난 트윗은 형용사나 동사가 주로 쓰임"
      ],
      "metadata": {
        "id": "YL8q0myxn5z5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Feature Engineering"
      ],
      "metadata": {
        "id": "G4LPLlLcoG8f"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. 결측치 확인"
      ],
      "metadata": {
        "id": "Yhu7UWsKoUR6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def check_na(data):\n",
        "  isnull_na = (data.isnull().sum() / len(data)) * 100\n",
        "  data_na = isnull_na.drop(isnull_na[isnull_na == 0].index).sort_values(ascending = False)\n",
        "  missing_data = pd.DataFrame({'Missing Ratio' : data_na,\n",
        "                               'Data Type' : data.dtypes[data_na.index]})\n",
        "  print('결측치 데이터 칼럼과 건수 : \\n', missing_data)\n",
        "\n",
        "check_na(train)\n",
        "check_na(test)"
      ],
      "metadata": {
        "id": "dDnQhfepnYUw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 이용할 데이터는 'text' 밖에 없기 때문에 나머지는 모두 제거하고, test_id만 따로 저장해둔다\n",
        "test_id = test['id']\n",
        "for datas in [train, test]:\n",
        "  datas = datas.drop(['id', 'keyword', 'location'], axis = 1, \n",
        "                     inplace = True)\n",
        "train.shape, test.shape"
      ],
      "metadata": {
        "id": "F8gz9XCZovZP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 텍스트 전처리 함수 만들기\n",
        "- 기본적으로 웹 크롤링으로 데이터를 불러옴(실무 종사자가 아니라면)\n",
        "- 처리 과정엔 이런 것들이 있다\n",
        "  - URL 문자 삭제, HTML 태그 삭제, 이모티콘 삭제, 특수 문자 공백화, 구두점 삭제, 대문자 -> 소문자 변환, 불용어 제거"
      ],
      "metadata": {
        "id": "xzN5C-XHpQ3s"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import string\n",
        "import re\n",
        "# NLP에 가장 많이 쓰이는 파이썬 라이브러리 : NLTK, Spacy\n",
        "from nltk.corpus import stopwords # NLTK에는 다양한 Corpus(말뭉치)가 존재하며, 그 중 불용어만 다운\n",
        "import nltk\n",
        "nltk.download('stopwords')"
      ],
      "metadata": {
        "id": "l18-1jl0pQpG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def data_cleansing(text, remove_stopwords = False):\n",
        "\n",
        "  # remove url\n",
        "  url = re.compile(r'https?//\\S+|www\\.\\S+')\n",
        "  \"\"\" 정규식 설명\n",
        "  '' 밖의 r : raw string - 탈출 문자(\\)의 작동을 막음.\n",
        "    - 뒤에 www\\. 여기의 .을 0문자 이상 매칭이 아니라 . 그 자체로 쓰기 위해 씀\n",
        "  ? - 0 or 1 : 즉 http or https 둘 다 매칭\n",
        "  \\S : Whitespace가 아닌 문자와 매칭 : \\s는 whitespace와 매칭\n",
        "    - 이건 탈출문자로 쓰인 게 아니다. 그래서 멀쩡히 잘 작동함\n",
        "  \"\"\"\n",
        "  cleaned_text = url.sub(r'', text) # compile 내부 정규식을 이용해 text를 ''로 바꾸겠다\n",
        "                                    # 알고 있듯이 compile을 이용하지 않고 sub 내부에서 바로 해결할 수도 있다.\n",
        "\n",
        "  # remove html\n",
        "  html = re.compile(r'<.*?>')\n",
        "  \"\"\"\n",
        "  꺾쇠(<, >)에는 특별한 의미가 있지 않음. html 태그는 <> 내부에 뭐가 들어가니까 쓴 거\n",
        "  . : \\n을 제외한 모든 문자 매칭\n",
        "  * : 0문자 이상의 '반복' 매칭 \n",
        "  ? : Greedy한 반복 매칭 문자를 Reluctant하게 바꿔준다. 가장 적은 반복에 대한 매칭을 찾아준다.\n",
        "  \"\"\"\n",
        "  cleaned_text = html.sub(r'', cleaned_text)\n",
        "\n",
        "  # remove emoji\n",
        "  emoji_pattern = re.compile(\"[\"\n",
        "                            u\"\\U0001F600-\\U0001F64F\" # 이모티콘\n",
        "                            u\"\\U0001F300-\\U0001F5FF\" # symbol & pictograph\n",
        "                            u\"\\U0001F680-\\U0001F6FF\" # transport & map symbol\n",
        "                            u\"\\U0001F1E0-\\U0001F1FF\" # flags (iOS)\n",
        "                            u\"\\U00002702-\\U000027B0\"\n",
        "                            u\"\\U000024C2-\\U0001F251\"\n",
        "                            \"]+\", flags = re.UNICODE)\n",
        "  cleaned_text = emoji_pattern.sub(r'', cleaned_text)\n",
        "\n",
        "  # special letters to empty space\n",
        "  cleaned_text = re.sub(\"[^a-zA-Z]\", \"\", cleaned_text)\n",
        "\n",
        "  # Remove Punctuation\n",
        "  table = str.maketrans('', '', string.punctuation)\n",
        "  cleaned_text = cleaned_text.translate(table)\n",
        "\n",
        "  # Lowercase\n",
        "  cleaned_text = cleaned_text.lower().split()\n",
        "\n",
        "  if remove_stopwords:\n",
        "    stops = set(stopwords.words(\"english\"))\n",
        "    cleaned_text = [word for word in cleaned_text if not word in stops]\n",
        "    clean_review = ' '.join(cleaned_text)\n",
        "  else:\n",
        "    clean_review = ' '.join(cleaned_text)\n",
        "\n",
        "  return clean_review\n",
        "\n"
      ],
      "metadata": {
        "id": "66GW5OITpBQw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "clean_train_reviews = []\n",
        "for datas in [train, test]:\n",
        "  datas['cleaned_text'] = datas['text'].apply(lambda x: data_cleansing(x, remove_stopwords = True))\n",
        "\n",
        "train.head()"
      ],
      "metadata": {
        "id": "LMJ0S0yMri-z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 불용어 확인\n",
        "print(len(stopwords.words('english')))\n",
        "print(stopwords.words('english')[:10])"
      ],
      "metadata": {
        "id": "hxsSyhc2sdqe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 특징 추출하기\n",
        "- 단어, 문장들을 개별적인 값으로 바꾸는 게 매우매우 중요하다\n",
        "- 방법으로는 2가지가 있다\n"
      ],
      "metadata": {
        "id": "u9OEUvSPEeC5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "  1. CountVectorizer\n",
        "    - 텍스트 데이터가 단순히 몇 번 나왔는가\n"
      ],
      "metadata": {
        "id": "ZMXzIEopFJSJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 예제 \n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "corpus = ['As you know, I want to be with you',\n",
        "          'Thank you, but I cannot be with you']\n",
        "vector = CountVectorizer()\n",
        "print(vector.fit_transform(corpus).toarray()) # 각 단어의 해당 문장 내에서의 등장 빈도, 인덱스는 아래를 따름\n",
        "print(vector.vocabulary_) # 위 리스트의 각 인덱스의 의미. "
      ],
      "metadata": {
        "id": "NCeD6ZuAFMjq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- 단점 : 빈도만이 중요하다면, 대명사 등은 중요한 단어로 취급될 것이다.\n",
        "  - 위의 불용어 리스트에서도 보이듯이, 대명사는 제거 대상이다. 즉 빈도가 잦더라도 중요성을 담고 있지 못함을 의미한다"
      ],
      "metadata": {
        "id": "5x1sYzy6Gjbz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. TfidfVectorizer\n",
        "    - TF-IDF를 따른다. (이 두 값을 곱함)\n",
        "      - TF(Term Frequency) : 단어의 데이터 '내'에서의 등장빈도\n",
        "      - IDF(Inverse Document Frequency) : 특정 단어의 여러 문서에서의 등장 빈도(DF).. 의 역수\n",
        "        - 즉 어떤 단어가 여러 문서에 걸쳐 고루 나타난다면 이는 중요하지 않다는 의미가 된다."
      ],
      "metadata": {
        "id": "NxNPSTtXFKia"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "import numpy as np\n",
        "corpus = ['Can I have lunch with you?', \n",
        "          'No, I cannot have it with you',\n",
        "          'Because, I need to study later']\n",
        "tfidfv = TfidfVectorizer().fit(corpus)\n",
        "print(np.round(tfidfv.transform(corpus).toarray(), 2)) # 각 단어의 \"점수\"가 표현됨. 등장하지 않았다면 0\n",
        "print(tfidfv.vocabulary_)"
      ],
      "metadata": {
        "id": "MNy1U9y1w_9t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- TF-IDF의 단점 \n",
        "  - 희소 행렬(Sparse Matrix : 행렬 구조 내에 0이 겁나게 많은 거)의 발생으로 인한 저장 공간의 낭비 \n",
        "  - ML 연산 학습에서의 메모리 낭비"
      ],
      "metadata": {
        "id": "yL3c1FZaH_uV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 이번 예제에서는 TfidfVectorizer를 씁니다\n",
        "vectorizer = TfidfVectorizer(min_df = 0.0, # 디폴트 1 : 문서에서 이 값보다 등장빈도가 적다면 취급 안함(Threshold 개념)\n",
        "                             analyzer = 'char', # word, char 등이 올 수 있고, character N-gram을 의미함 (???)\n",
        "                             sublinear_tf = True, # tf값을 1 + log(tf) 값으로 바꿈\n",
        "                             ngram_range = (1, 3), # Uni, BI, Trigram까지 이용하겠다\n",
        "                             max_features = 10000) # Corpus 내에서 상위 빈도 10000개만 뽑음\n",
        "X = vectorizer.fit_transform(train['cleaned_text']).todense()\n",
        "y = train['target'].values\n",
        "print(X.shape, y.shape) # (학습할 데이터 숫자, 현재 데이터에 사용되는 전체 단어의 개수)"
      ],
      "metadata": {
        "id": "JD4xcP6O_aEd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ML 모델 학습 및 평가"
      ],
      "metadata": {
        "id": "AndIs9KILlCl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. 로지스틱 회귀 모델\n",
        "- 분류에서 제일 많이 등장하는 '초기 모델'\n",
        "- 초기 값은 0.5로 설정, 임계값을 설정해 1에 가까우면 1, 0에 가까우면 0으로 표시한다"
      ],
      "metadata": {
        "id": "YyLaW2dILomU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size = 0.3,\n",
        "                                                      random_state = 0)\n",
        "X_train.shape, X_valid.shape, y_train.shape, y_valid.shape"
      ],
      "metadata": {
        "id": "g-pCdLTe_6MM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "lgs = LogisticRegression(class_weight = 'balanced') # 데이터가 비대칭임을 반영하는 파라미터\n",
        "lgs.fit(X_train, y_train)"
      ],
      "metadata": {
        "id": "3OtV6QXeL95M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 바로 제출까지\n",
        "X_testset = vectorizer.transform(test['cleaned_text']).todense() # 희소행렬 변환(todense)\n",
        "print(X_testset.shape)"
      ],
      "metadata": {
        "id": "2nvV8RnbMOYd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_test_pred = lgs.predict(X_testset)\n",
        "print(y_test_pred)\n",
        "y_test_pred = np.where(y_test_pred>=0.5, 1, 0)\n",
        "print(y_test_pred)\n",
        "\n",
        "submission_file = pd.DataFrame({'id' : test_id, 'target' : y_test_pred})\n",
        "submission_file.to_csv('submission_lgs.csv', index = False)"
      ],
      "metadata": {
        "id": "tZSWyAmCMiiG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ML 모델의 평가"
      ],
      "metadata": {
        "id": "DvMqwL5qNnlI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- 위랑 직접적인 관련이 있는 건 아니지만..\n",
        "\n"
      ],
      "metadata": {
        "id": "kakAnWB0VuC_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Confusion Matrix, Accuracy, Precision, Recall 등\n",
        "- 최적의 Threshold 값 : G-Mean\n",
        "  - https://towardsdatascience.com/optimal-threshold-for-imbalanced-classification-5884e870c293\n",
        "- ROC Curve, AUC"
      ],
      "metadata": {
        "id": "Wtkj5jI9VyKG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 함수 만드는 중.. 아마 numpy로 하면 훨씬 빠를걸? 싶지만 그냥 연습용이다.\n",
        "y_true = [0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1]\n",
        "# y_pred = [0.1, 0.3, 0.2, .6, .8, .05, .9, .5, .3, .66, .3, .2, .85, .15, .99]\n",
        "thresholds = [0, .1, .2, .3, .4, .5, .6, .7, .8, .85, .9, .99, 1.0]\n",
        "cf = {\"tp\" : 0,\n",
        "      \"fn\" : 0,\n",
        "      \"tn\" : 0,\n",
        "      \"fp\" : 0}\n",
        "tpr_list = []\n",
        "fpr_list = []\n",
        "auc_list = []\n",
        "\n",
        "\n",
        "for th in thresholds:\n",
        "  y_pred = [0.1, 0.3, 0.2, .6, .8, .05, .9, .5, .3, .66, .3, .2, .85, .15, .99]\n",
        "  for idx, val in enumerate(y_pred):\n",
        "    if val < th:\n",
        "      y_pred[idx] = 0\n",
        "    else:\n",
        "      y_pred[idx] = 1\n",
        "  print(y_pred)\n",
        "\n",
        "  for lst in zip(y_true, y_pred):\n",
        "    if (lst[0] == 0) & (lst[1] == 0):\n",
        "      cf['tn'] += 1\n",
        "    elif (lst[0] == 1) & (lst[1] == 0):\n",
        "      cf['fn'] += 1\n",
        "    elif (lst[0] == 0) & (lst[1] == 1):\n",
        "      cf['fp'] += 1\n",
        "    else:\n",
        "      cf['tp'] += 1\n",
        "\n",
        "  print(cf)\n",
        "\n",
        "  tpr = cf['tp'] / (cf['tp'] + cf['fn'])\n",
        "  fpr = cf['fp'] / (cf['fp'] + cf['tn'])\n",
        "\n",
        "  tpr_list.append(tpr)\n",
        "  fpr_list.append(fpr)\n",
        "\n",
        "  # return tpr_list, fpr_li"
      ],
      "metadata": {
        "id": "wBXpB7tcWXVP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\" 이 책은 이런 지점이 아쉽다.. \n",
        "tpr이나 fpr 함수를 찾을 수가 없음. 빵꾸가 많다.\n",
        "그러면 만들어야지 ㅋㅋ; \"\"\"\n",
        "print(pd.DataFrame({\"Thresholds\" : thresholds,\n",
        "                    \"fpr\" : fpr_list,\n",
        "                    \"tpr\" : tpr_list}))\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(10, 6))\n",
        "ax.fill_between(fpr_list, tpr_list, alpha = 0.4)\n",
        "ax.plot(fpr_list, tpr_list, lw = 2, label = 'ROC')\n",
        "plt.plot([0, 1], [0, 1], lw = 2, color = 'r', label = 'Random')\n",
        "ax.set_xlim(0, 1.0)\n",
        "ax.set_ylim(0, 1.0)\n",
        "ax.set_xlabel(\"FPR\", fontsize = 15)\n",
        "ax.set_ylabel(\"TPR\", fontsize = 15)\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "6S3MqfGNNIqW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import roc_auc_score\n",
        "\n",
        "y_true = [0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1]\n",
        "y_pred = [.1, .3, .2, .6, .8, .05, .9, .5, .3, .66, .3, .2, .85, .15, .99]\n",
        "\n",
        "print(roc_auc_score(y_true, y_pred))"
      ],
      "metadata": {
        "id": "e_N0eG62b3g6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "TBoVE-IMhO3k"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}