{"cells":[{"cell_type":"markdown","metadata":{"id":"L-rwAtozG-wJ"},"source":["## 스팀 리뷰 데이터 감성 분석 모델(이진 분류)\n","- 사용 데이터 : 스팀 보유자 수 기준 상위 5000개 + a 게임의 한국어 리뷰 데이터, 총 194263개\n","  - 각 리뷰는 최대 8000글자를 담을 수 있음\n","- 모델 : [krBERT](https://github.com/snunlp/KR-BERT)을 Fine tuning함\n"]},{"cell_type":"markdown","metadata":{"id":"a3I92SYG_3TA"},"source":["## tf 2.x 버전 실행\n"]},{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"J0QpWZtX_5-w","executionInfo":{"status":"ok","timestamp":1701961837721,"user_tz":-540,"elapsed":373343,"user":{"displayName":"Hyeontae Lee","userId":"18343648907583635281"}},"outputId":"0ec008bd-70ad-432c-cbcb-47e8a5ffcc68"},"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.35.2)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.13.1)\n","Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.19.4)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.23.5)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.2)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.1)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2023.6.3)\n","Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.31.0)\n","Requirement already satisfied: tokenizers<0.19,>=0.14 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.15.0)\n","Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.1)\n","Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.1)\n","Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->transformers) (2023.6.0)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->transformers) (4.5.0)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.3.2)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.6)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.7)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2023.11.17)\n","Requirement already satisfied: tf-models-official in /usr/local/lib/python3.10/dist-packages (2.15.0)\n","Requirement already satisfied: Cython in /usr/local/lib/python3.10/dist-packages (from tf-models-official) (3.0.6)\n","Requirement already satisfied: Pillow in /usr/local/lib/python3.10/dist-packages (from tf-models-official) (9.4.0)\n","Requirement already satisfied: gin-config in /usr/local/lib/python3.10/dist-packages (from tf-models-official) (0.5.0)\n","Requirement already satisfied: google-api-python-client>=1.6.7 in /usr/local/lib/python3.10/dist-packages (from tf-models-official) (2.84.0)\n","Requirement already satisfied: immutabledict in /usr/local/lib/python3.10/dist-packages (from tf-models-official) (4.0.0)\n","Requirement already satisfied: kaggle>=1.3.9 in /usr/local/lib/python3.10/dist-packages (from tf-models-official) (1.5.16)\n","Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (from tf-models-official) (3.7.1)\n","Requirement already satisfied: numpy>=1.20 in /usr/local/lib/python3.10/dist-packages (from tf-models-official) (1.23.5)\n","Requirement already satisfied: oauth2client in /usr/local/lib/python3.10/dist-packages (from tf-models-official) (4.1.3)\n","Requirement already satisfied: opencv-python-headless in /usr/local/lib/python3.10/dist-packages (from tf-models-official) (4.8.1.78)\n","Requirement already satisfied: pandas>=0.22.0 in /usr/local/lib/python3.10/dist-packages (from tf-models-official) (1.5.3)\n","Requirement already satisfied: psutil>=5.4.3 in /usr/local/lib/python3.10/dist-packages (from tf-models-official) (5.9.5)\n","Requirement already satisfied: py-cpuinfo>=3.3.0 in /usr/local/lib/python3.10/dist-packages (from tf-models-official) (9.0.0)\n","Requirement already satisfied: pycocotools in /usr/local/lib/python3.10/dist-packages (from tf-models-official) (2.0.7)\n","Requirement already satisfied: pyyaml>=6.0.0 in /usr/local/lib/python3.10/dist-packages (from tf-models-official) (6.0.1)\n","Requirement already satisfied: sacrebleu in /usr/local/lib/python3.10/dist-packages (from tf-models-official) (2.3.3)\n","Requirement already satisfied: scipy>=0.19.1 in /usr/local/lib/python3.10/dist-packages (from tf-models-official) (1.11.4)\n","Requirement already satisfied: sentencepiece in /usr/local/lib/python3.10/dist-packages (from tf-models-official) (0.1.99)\n","Requirement already satisfied: seqeval in /usr/local/lib/python3.10/dist-packages (from tf-models-official) (1.2.2)\n","Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from tf-models-official) (1.16.0)\n","Requirement already satisfied: tensorflow-datasets in /usr/local/lib/python3.10/dist-packages (from tf-models-official) (4.9.3)\n","Requirement already satisfied: tensorflow-hub>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from tf-models-official) (0.15.0)\n","Requirement already satisfied: tensorflow-model-optimization>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from tf-models-official) (0.7.5)\n","Requirement already satisfied: tensorflow-text~=2.15.0 in /usr/local/lib/python3.10/dist-packages (from tf-models-official) (2.15.0)\n","Requirement already satisfied: tensorflow~=2.15.0 in /usr/local/lib/python3.10/dist-packages (from tf-models-official) (2.15.0.post1)\n","Requirement already satisfied: tf-slim>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from tf-models-official) (1.1.0)\n","Requirement already satisfied: httplib2<1dev,>=0.15.0 in /usr/local/lib/python3.10/dist-packages (from google-api-python-client>=1.6.7->tf-models-official) (0.22.0)\n","Requirement already satisfied: google-auth<3.0.0dev,>=1.19.0 in /usr/local/lib/python3.10/dist-packages (from google-api-python-client>=1.6.7->tf-models-official) (2.17.3)\n","Requirement already satisfied: google-auth-httplib2>=0.1.0 in /usr/local/lib/python3.10/dist-packages (from google-api-python-client>=1.6.7->tf-models-official) (0.1.1)\n","Requirement already satisfied: google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5 in /usr/local/lib/python3.10/dist-packages (from google-api-python-client>=1.6.7->tf-models-official) (2.11.1)\n","Requirement already satisfied: uritemplate<5,>=3.0.1 in /usr/local/lib/python3.10/dist-packages (from google-api-python-client>=1.6.7->tf-models-official) (4.1.1)\n","Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from kaggle>=1.3.9->tf-models-official) (2023.11.17)\n","Requirement already satisfied: python-dateutil in /usr/local/lib/python3.10/dist-packages (from kaggle>=1.3.9->tf-models-official) (2.8.2)\n","Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from kaggle>=1.3.9->tf-models-official) (2.31.0)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from kaggle>=1.3.9->tf-models-official) (4.66.1)\n","Requirement already satisfied: python-slugify in /usr/local/lib/python3.10/dist-packages (from kaggle>=1.3.9->tf-models-official) (8.0.1)\n","Requirement already satisfied: urllib3 in /usr/local/lib/python3.10/dist-packages (from kaggle>=1.3.9->tf-models-official) (2.0.7)\n","Requirement already satisfied: bleach in /usr/local/lib/python3.10/dist-packages (from kaggle>=1.3.9->tf-models-official) (6.1.0)\n","Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=0.22.0->tf-models-official) (2023.3.post1)\n","Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow~=2.15.0->tf-models-official) (1.4.0)\n","Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow~=2.15.0->tf-models-official) (1.6.3)\n","Requirement already satisfied: flatbuffers>=23.5.26 in /usr/local/lib/python3.10/dist-packages (from tensorflow~=2.15.0->tf-models-official) (23.5.26)\n","Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow~=2.15.0->tf-models-official) (0.4.0)\n","Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow~=2.15.0->tf-models-official) (0.2.0)\n","Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow~=2.15.0->tf-models-official) (3.9.0)\n","Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow~=2.15.0->tf-models-official) (16.0.6)\n","Requirement already satisfied: ml-dtypes~=0.2.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow~=2.15.0->tf-models-official) (0.2.0)\n","Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow~=2.15.0->tf-models-official) (3.3.0)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from tensorflow~=2.15.0->tf-models-official) (23.2)\n","Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow~=2.15.0->tf-models-official) (3.20.3)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from tensorflow~=2.15.0->tf-models-official) (67.7.2)\n","Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow~=2.15.0->tf-models-official) (2.3.0)\n","Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.10/dist-packages (from tensorflow~=2.15.0->tf-models-official) (4.5.0)\n","Requirement already satisfied: wrapt<1.15,>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow~=2.15.0->tf-models-official) (1.14.1)\n","Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow~=2.15.0->tf-models-official) (0.34.0)\n","Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow~=2.15.0->tf-models-official) (1.59.3)\n","Requirement already satisfied: tensorboard<2.16,>=2.15 in /usr/local/lib/python3.10/dist-packages (from tensorflow~=2.15.0->tf-models-official) (2.15.1)\n","Requirement already satisfied: tensorflow-estimator<2.16,>=2.15.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow~=2.15.0->tf-models-official) (2.15.0)\n","Requirement already satisfied: keras<2.16,>=2.15.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow~=2.15.0->tf-models-official) (2.15.0)\n","Requirement already satisfied: dm-tree~=0.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow-model-optimization>=0.4.1->tf-models-official) (0.1.8)\n","Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->tf-models-official) (1.2.0)\n","Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib->tf-models-official) (0.12.1)\n","Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->tf-models-official) (4.45.1)\n","Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->tf-models-official) (1.4.5)\n","Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->tf-models-official) (3.1.1)\n","Requirement already satisfied: pyasn1>=0.1.7 in /usr/local/lib/python3.10/dist-packages (from oauth2client->tf-models-official) (0.5.1)\n","Requirement already satisfied: pyasn1-modules>=0.0.5 in /usr/local/lib/python3.10/dist-packages (from oauth2client->tf-models-official) (0.3.0)\n","Requirement already satisfied: rsa>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from oauth2client->tf-models-official) (4.9)\n","Requirement already satisfied: portalocker in /usr/local/lib/python3.10/dist-packages (from sacrebleu->tf-models-official) (2.8.2)\n","Requirement already satisfied: regex in /usr/local/lib/python3.10/dist-packages (from sacrebleu->tf-models-official) (2023.6.3)\n","Requirement already satisfied: tabulate>=0.8.9 in /usr/local/lib/python3.10/dist-packages (from sacrebleu->tf-models-official) (0.9.0)\n","Requirement already satisfied: colorama in /usr/local/lib/python3.10/dist-packages (from sacrebleu->tf-models-official) (0.4.6)\n","Requirement already satisfied: lxml in /usr/local/lib/python3.10/dist-packages (from sacrebleu->tf-models-official) (4.9.3)\n","Requirement already satisfied: scikit-learn>=0.21.3 in /usr/local/lib/python3.10/dist-packages (from seqeval->tf-models-official) (1.2.2)\n","Requirement already satisfied: array-record in /usr/local/lib/python3.10/dist-packages (from tensorflow-datasets->tf-models-official) (0.5.0)\n","Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from tensorflow-datasets->tf-models-official) (8.1.7)\n","Requirement already satisfied: etils[enp,epath,etree]>=0.9.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow-datasets->tf-models-official) (1.5.2)\n","Requirement already satisfied: promise in /usr/local/lib/python3.10/dist-packages (from tensorflow-datasets->tf-models-official) (2.3)\n","Requirement already satisfied: tensorflow-metadata in /usr/local/lib/python3.10/dist-packages (from tensorflow-datasets->tf-models-official) (1.14.0)\n","Requirement already satisfied: toml in /usr/local/lib/python3.10/dist-packages (from tensorflow-datasets->tf-models-official) (0.10.2)\n","Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from astunparse>=1.6.0->tensorflow~=2.15.0->tf-models-official) (0.42.0)\n","Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from etils[enp,epath,etree]>=0.9.0->tensorflow-datasets->tf-models-official) (2023.6.0)\n","Requirement already satisfied: importlib_resources in /usr/local/lib/python3.10/dist-packages (from etils[enp,epath,etree]>=0.9.0->tensorflow-datasets->tf-models-official) (6.1.1)\n","Requirement already satisfied: zipp in /usr/local/lib/python3.10/dist-packages (from etils[enp,epath,etree]>=0.9.0->tensorflow-datasets->tf-models-official) (3.17.0)\n","Requirement already satisfied: googleapis-common-protos<2.0.dev0,>=1.56.2 in /usr/local/lib/python3.10/dist-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5->google-api-python-client>=1.6.7->tf-models-official) (1.61.0)\n","Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3.0.0dev,>=1.19.0->google-api-python-client>=1.6.7->tf-models-official) (5.3.2)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->kaggle>=1.3.9->tf-models-official) (3.3.2)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->kaggle>=1.3.9->tf-models-official) (3.6)\n","Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.21.3->seqeval->tf-models-official) (1.3.2)\n","Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.21.3->seqeval->tf-models-official) (3.2.0)\n","Requirement already satisfied: google-auth-oauthlib<2,>=0.5 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow~=2.15.0->tf-models-official) (1.1.0)\n","Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow~=2.15.0->tf-models-official) (3.5.1)\n","Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow~=2.15.0->tf-models-official) (0.7.2)\n","Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow~=2.15.0->tf-models-official) (3.0.1)\n","Requirement already satisfied: webencodings in /usr/local/lib/python3.10/dist-packages (from bleach->kaggle>=1.3.9->tf-models-official) (0.5.1)\n","Requirement already satisfied: text-unidecode>=1.3 in /usr/local/lib/python3.10/dist-packages (from python-slugify->kaggle>=1.3.9->tf-models-official) (1.3)\n","Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from google-auth-oauthlib<2,>=0.5->tensorboard<2.16,>=2.15->tensorflow~=2.15.0->tf-models-official) (1.3.1)\n","Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard<2.16,>=2.15->tensorflow~=2.15.0->tf-models-official) (2.1.3)\n","Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<2,>=0.5->tensorboard<2.16,>=2.15->tensorflow~=2.15.0->tf-models-official) (3.2.2)\n","Requirement already satisfied: keras-tuner in /usr/local/lib/python3.10/dist-packages (1.4.6)\n","Requirement already satisfied: keras in /usr/local/lib/python3.10/dist-packages (from keras-tuner) (2.15.0)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from keras-tuner) (23.2)\n","Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from keras-tuner) (2.31.0)\n","Requirement already satisfied: kt-legacy in /usr/local/lib/python3.10/dist-packages (from keras-tuner) (1.0.5)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->keras-tuner) (3.3.2)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->keras-tuner) (3.6)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->keras-tuner) (2.0.7)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->keras-tuner) (2023.11.17)\n","Updated property [core/account].\n","Go to the following link in your browser:\n","\n","    https://accounts.google.com/o/oauth2/auth?response_type=code&client_id=32555940559.apps.googleusercontent.com&redirect_uri=https%3A%2F%2Fsdk.cloud.google.com%2Fauthcode.html&scope=openid+https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fuserinfo.email+https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fcloud-platform+https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fappengine.admin+https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fsqlservice.login+https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fcompute+https%3A%2F%2Fwww.googleapis.com%2Fauth%2Faccounts.reauth&state=d4NbSQDMtKVLYJcRkALJi9MeUL9CNS&prompt=consent&access_type=offline&code_challenge=1FFcEkZgj5AGiDvrEydsNBPY0lxrXlXNHJMqrpjLGzA&code_challenge_method=S256\n","\n","Enter authorization code: 4/0AfJohXnQs_TNCRF9IVsWcuk2IIh-r5UKMQbYSvpIbD-9VV12YTl84cQfW67flPRMRMFGSQ\n","\n","You are now logged in as [dowrave@gmail.com].\n","Your current project is [copying-book].  You can change this setting by running:\n","  $ gcloud config set project PROJECT_ID\n","Updated property [core/project].\n","10.15.121.146:8470\n"]},{"output_type":"stream","name":"stderr","text":["INFO:tensorflow:Deallocate tpu buffers before initializing tpu system.\n","INFO:tensorflow:Initializing the TPU system: grpc://10.15.121.146:8470\n","INFO:tensorflow:Finished initializing TPU system.\n","INFO:tensorflow:Found TPU system:\n","INFO:tensorflow:*** Num TPU Cores: 8\n","INFO:tensorflow:*** Num TPU Workers: 1\n","INFO:tensorflow:*** Num TPU Cores Per Worker: 8\n","INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:localhost/replica:0/task:0/device:CPU:0, CPU, 0, 0)\n","INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:CPU:0, CPU, 0, 0)\n","INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:0, TPU, 0, 0)\n","INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:1, TPU, 0, 0)\n","INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:2, TPU, 0, 0)\n","INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:3, TPU, 0, 0)\n","INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:4, TPU, 0, 0)\n","INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:5, TPU, 0, 0)\n","INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:6, TPU, 0, 0)\n","INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:7, TPU, 0, 0)\n","INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU_SYSTEM:0, TPU_SYSTEM, 0, 0)\n","INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:XLA_CPU:0, XLA_CPU, 0, 0)\n"]},{"output_type":"stream","name":"stdout","text":["이미 tfrecord 파일이 있음\n"]},{"output_type":"stream","name":"stderr","text":["WARNING:tensorflow:From /usr/local/lib/python3.10/dist-packages/tensorflow/python/util/dispatch.py:1260: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Use `tf.cast` instead.\n"]},{"output_type":"stream","name":"stdout","text":["Reloading Tuner from gs://steam-project-bucket/krbert_tensorflow/ModelTrain/TEST1_steam_krbert_BO_seq512_64_32/tuner0.json\n"]}],"source":["!pip install transformers\n","!pip install tf-models-official\n","!pip install keras-tuner\n","\n","import os\n","os.chdir('drive/MyDrive/Now/KR-BERT/krbert_tensorflow')\n","from transformers import BertConfig, TFBertModel, load_tf_weights_in_bert\n","import logging\n","import pandas as pd\n","import tensorflow as tf\n","import tensorflow_models as tfm\n","import numpy as np\n","from sklearn.model_selection import train_test_split\n","import re\n","import csv\n","from google.colab import auth\n","from google.cloud import storage\n","import subprocess\n","import collections\n","import matplotlib.pyplot as plt\n","from tensorflow.keras import backend as K\n","import keras_tuner\n","\n","# krbert의 `.py` 파일\n","import modeling\n","import tokenization_ranked as tokenization\n","import optimization\n","\n","tf.get_logger().setLevel(logging.INFO)\n","\n","real_init_checkpoint = 'gs://steam-project-bucket/krbert_tensorflow/MyModel/'\n","real_bert_config_file = 'bert_config_char16424.json'\n","real_data_dir = './data/steam/'\n","real_vocab_file = 'vocab_char_16424.txt'\n","\n","# 모델 훈련 체크포인트 & 결과가 저장되는 디렉토리\n","# output_dir = \"gs://steam-project-bucket/krbert_tensorflow/ModelTrain/\"\n","\n","\n","class InputExample(object):\n","  \"\"\"A single training/test example for simple sequence classification.\"\"\"\n","\n","  def __init__(self, guid, text_a, text_b=None, label=None):\n","    \"\"\"Constructs a InputExample.\n","\n","    Args:\n","      guid: Unique id for the example.\n","      text_a: string. The untokenized text of the first sequence. For single\n","        sequence tasks, only this sequence must be specified.\n","      text_b: (Optional) string. The untokenized text of the second sequence.\n","        Only must be specified for sequence pair tasks.\n","      label: (Optional) string. The label of the example. This should be\n","        specified for train and dev examples, but not for test examples.\n","    \"\"\"\n","    self.guid = guid\n","    self.text_a = text_a\n","    self.text_b = text_b\n","    self.label = label\n","\n","class DataProcessor(object):\n","  \"\"\"Base class for data converters for sequence classification data sets.\"\"\"\n","\n","  def get_train_examples(self, data_dir):\n","    \"\"\"Gets a collection of `InputExample`s for the train set.\"\"\"\n","    raise NotImplementedError()\n","\n","  def get_dev_examples(self, data_dir):\n","    \"\"\"Gets a collection of `InputExample`s for the dev set.\"\"\"\n","    raise NotImplementedError()\n","\n","  def get_test_examples(self, data_dir):\n","    \"\"\"Gets a collection of `InputExample`s for prediction.\"\"\"\n","    raise NotImplementedError()\n","\n","  def get_labels(self):\n","    \"\"\"Gets the list of labels for this data set.\"\"\"\n","    raise NotImplementedError()\n","\n","  @classmethod\n","  def _read_tsv(cls, input_file, quotechar=None):\n","    \"\"\"Reads a tab separated value file.\"\"\"\n","    with tf.io.gfile.GFile(input_file, \"r\") as f:\n","      reader = csv.reader(f, delimiter=\"\\t\", quotechar=quotechar)\n","      lines = []\n","      for line in reader:\n","        lines.append(line)\n","      return lines\n","\n","class SteamProcessor(DataProcessor):\n","  \"\"\"\n","  직접 수정 : 상속 받아서 메서드 오버라이드함\n","  \"\"\"\n","  def get_train_examples(self, data_dir, csv_name):\n","    \"\"\"See base class.\"\"\"\n","    return self._create_examples(\n","        self._read_csv(os.path.join(data_dir, csv_name)), \"train\")\n","\n","  def get_dev_examples(self, data_dir, csv_name):\n","    \"\"\"See base class.\"\"\"\n","    return self._create_examples(\n","        self._read_csv(os.path.join(data_dir, csv_name)), \"dev\")\n","\n","  def get_test_examples(self, data_dir, csv_name):\n","    \"\"\"See base class.\"\"\"\n","    return self._create_examples(\n","        self._read_csv(os.path.join(data_dir, csv_name)), \"test\")\n","\n","  def get_labels(self):\n","    \"\"\"See base class.\"\"\"\n","    return [\"0\", \"1\"]\n","\n","  def _create_examples(self, lines, set_type):\n","    \"\"\"Creates examples for the training and dev sets.\"\"\"\n","    examples = []\n","    for (i, line) in enumerate(lines):\n","      if i == 0: continue\n","      guid = \"%s-%s\" % (set_type, i)\n","\n","      # 230831 수정\n","      # text_a는 모든 문장을 가져옴\n","      # 또한, 일부 문장의 양 끝에 \"이 나타나므로 이를 제거함\n","      entire_text = \"\".join(line[1:-1]) # 문장 합침\n","      entire_text = self.preprocess_text(entire_text)\n","\n","      text_a = tokenization.convert_to_unicode(entire_text)\n","      label = tokenization.convert_to_unicode(line[-1])\n","\n","      examples.append(\n","          InputExample(guid=guid, text_a=text_a, text_b=None, label=label))\n","      # if i == 10:\n","      #   break\n","    return examples\n","\n","  # 230831 : 텍스트 가공(필요 없는 문자 제거, 문장 통합..)\n","  def preprocess_text(self, text):\n","\n","    processed_text = text.strip('\"')\n","\n","    # http:// 로 시작하는 링크 제거\n","    http_pattern = r\"https?://\\S+\" # http://나 https://로 시작하며 공백문자가 아닌 패턴\n","    processed_text = re.sub(http_pattern, '', processed_text)\n","\n","    # HTML 태그 제거\n","    html_pattern = r\"\\[.*?\\]\"\n","    processed_text = re.sub(html_pattern, '', processed_text)\n","\n","    return processed_text\n","\n","  @classmethod\n","  def _read_csv(cls, input_file, quotechar = None):\n","    with tf.io.gfile.GFile(input_file, \"r\") as f:\n","      reader = csv.reader(f, quotechar=quotechar)\n","      lines = []\n","      for line in reader:\n","        lines.append(line)\n","      return lines\n","\n","def file_based_convert_examples_to_features(\n","    examples, label_list, max_seq_length, tokenizer, output_file):\n","  \"\"\"Convert a set of `InputExample`s to a TFRecord file.\"\"\"\n","\n","  writer = tf.compat.v1.python_io.TFRecordWriter(output_file)\n","  # writer = tf.io.TFRecordWriter(output_file)\n","\n","  for (ex_index, example) in enumerate(examples):\n","\n","    if ex_index % 10000 == 0:\n","      tf.compat.v1.logging.info(\"Writing example %d of %d\" % (ex_index, len(examples)))\n","\n","    feature = convert_single_example(ex_index, example, label_list,\n","                                     max_seq_length, tokenizer)\n","\n","    def create_int_feature(values):\n","      f = tf.compat.v1.train.Feature(int64_list=tf.train.Int64List(value=list(values)))\n","      return f\n","\n","    features = collections.OrderedDict()\n","    features[\"input_ids\"] = create_int_feature(feature.input_ids)\n","    features[\"input_mask\"] = create_int_feature(feature.input_mask)\n","    features[\"segment_ids\"] = create_int_feature(feature.segment_ids)\n","    features[\"label_ids\"] = create_int_feature([feature.label_id])\n","    features[\"is_real_example\"] = create_int_feature(\n","        [int(feature.is_real_example)])\n","\n","    tf_example = tf.compat.v1.train.Example(features=tf.train.Features(feature=features))\n","    writer.write(tf_example.SerializeToString())\n","  writer.close()\n","\n","\n","# 사용 함수 모음\n","\n","def _truncate_seq_pair(tokens_a, tokens_b, max_length):\n","  \"\"\"Truncates a sequence pair in place to the maximum length.\"\"\"\n","\n","  # This is a simple heuristic which will always truncate the longer sequence\n","  # one token at a time. This makes more sense than truncating an equal percent\n","  # of tokens from each, since if one sequence is very short then each token\n","  # that's truncated likely contains more information than a longer sequence.\n","  while True:\n","    total_length = len(tokens_a) + len(tokens_b)\n","    if total_length <= max_length:\n","      break\n","    if len(tokens_a) > len(tokens_b):\n","      tokens_a.pop()\n","    else:\n","      tokens_b.pop()\n","\n","def preprocess_text(text):\n","  \"\"\"\n","  http:// 제거 및 HTML 태그 제거\n","  \"\"\"\n","\n","  processed_text = text.strip('\"')\n","\n","  # http:// 로 시작하는 링크 제거\n","  http_pattern = r\"https?://\\S+\" # http://나 https://로 시작하며 공백문자가 아닌 패턴\n","  processed_text = re.sub(http_pattern, '', processed_text)\n","\n","  # HTML 태그 제거\n","  html_pattern = r\"\\[.*?\\]\"\n","  processed_text = re.sub(html_pattern, '', processed_text)\n","\n","  return processed_text\n","\n","\n","def convert_single_example(ex_index, example, label_list, max_seq_length,\n","                           tokenizer):\n","  \"\"\"Converts a single `InputExample` into a single `InputFeatures`.\"\"\"\n","\n","  if isinstance(example, PaddingInputExample):\n","    return InputFeatures(\n","        input_ids=[0] * max_seq_length,\n","        input_mask=[0] * max_seq_length,\n","        segment_ids=[0] * max_seq_length,\n","        label_id=0,\n","        is_real_example=False)\n","\n","  label_map = {}\n","  for (i, label) in enumerate(label_list):\n","    label_map[label] = i\n","\n","  # 230918 : 토큰화 전, 필요없어 보이는 텍스트 제거\n","  text_a = preprocess_text(example.text_a)\n","\n","  tokens_a = tokenizer.tokenize(text_a)\n","  tokens_b = None\n","  if example.text_b:\n","    tokens_b = tokenizer.tokenize(example.text_b)\n","\n","  if tokens_b:\n","    # Modifies `tokens_a` and `tokens_b` in place so that the total\n","    # length is less than the specified length.\n","    # Account for [CLS], [SEP], [SEP] with \"- 3\"\n","    _truncate_seq_pair(tokens_a, tokens_b, max_seq_length - 3)\n","  else:\n","    # Account for [CLS] and [SEP] with \"- 2\"\n","    if len(tokens_a) > max_seq_length - 2:\n","      tokens_a = tokens_a[0:(max_seq_length - 2)]\n","\n","  # The convention in BERT is:\n","  # (a) For sequence pairs:\n","  #  tokens:   [CLS] is this jack ##son ##ville ? [SEP] no it is not . [SEP]\n","  #  type_ids: 0     0  0    0    0     0       0 0     1  1  1  1   1 1\n","  # (b) For single sequences:\n","  #  tokens:   [CLS] the dog is hairy . [SEP]\n","  #  type_ids: 0     0   0   0  0     0 0\n","  #\n","  # Where \"type_ids\" are used to indicate whether this is the first\n","  # sequence or the second sequence. The embedding vectors for `type=0` and\n","  # `type=1` were learned during pre-training and are added to the wordpiece\n","  # embedding vector (and position vector). This is not *strictly* necessary\n","  # since the [SEP] token unambiguously separates the sequences, but it makes\n","  # it easier for the model to learn the concept of sequences.\n","  #\n","  # For classification tasks, the first vector (corresponding to [CLS]) is\n","  # used as the \"sentence vector\". Note that this only makes sense because\n","  # the entire model is fine-tuned.\n","  tokens = []\n","  segment_ids = []\n","  tokens.append(\"[CLS]\")\n","  segment_ids.append(0)\n","  for token in tokens_a:\n","    tokens.append(token)\n","    segment_ids.append(0)\n","  tokens.append(\"[SEP]\")\n","  segment_ids.append(0)\n","\n","  if tokens_b:\n","    for token in tokens_b:\n","      tokens.append(token)\n","      segment_ids.append(1)\n","    tokens.append(\"[SEP]\")\n","    segment_ids.append(1)\n","\n","  input_ids = tokenizer.convert_tokens_to_ids(tokens)\n","\n","  # The mask has 1 for real tokens and 0 for padding tokens. Only real\n","  # tokens are attended to.\n","  input_mask = [1] * len(input_ids)\n","\n","  # Zero-pad up to the sequence length.\n","  while len(input_ids) < max_seq_length:\n","    input_ids.append(0)\n","    input_mask.append(0)\n","    segment_ids.append(0)\n","\n","  assert len(input_ids) == max_seq_length\n","  assert len(input_mask) == max_seq_length\n","  assert len(segment_ids) == max_seq_length\n","\n","  label_id = label_map[example.label]\n","  if ex_index < 5:\n","    tf.compat.v1.logging.info(\"*** Example ***\")\n","    tf.compat.v1.logging.info(\"guid: %s\" % (example.guid))\n","    tf.compat.v1.logging.info(\"tokens: %s\" % \" \".join(\n","        [tokenization.printable_text(x) for x in tokens]))\n","    tf.compat.v1.logging.info(\"input_ids: %s\" % \" \".join([str(x) for x in input_ids]))\n","    tf.compat.v1.logging.info(\"input_mask: %s\" % \" \".join([str(x) for x in input_mask]))\n","    tf.compat.v1.logging.info(\"segment_ids: %s\" % \" \".join([str(x) for x in segment_ids]))\n","    tf.compat.v1.logging.info(\"label: %s (id = %d)\" % (example.label, label_id))\n","\n","  feature = InputFeatures(\n","      input_ids=input_ids,\n","      input_mask=input_mask,\n","      segment_ids=segment_ids,\n","      label_id=label_id,\n","      is_real_example=True)\n","\n","  return feature\n","\n","class InputExample(object):\n","  \"\"\"A single training/test example for simple sequence classification.\"\"\"\n","\n","  def __init__(self, guid, text_a, text_b=None, label=None):\n","    \"\"\"Constructs a InputExample.\n","\n","    Args:\n","      guid: Unique id for the example.\n","      text_a: string. The untokenized text of the first sequence. For single\n","        sequence tasks, only this sequence must be specified.\n","      text_b: (Optional) string. The untokenized text of the second sequence.\n","        Only must be specified for sequence pair tasks.\n","      label: (Optional) string. The label of the example. This should be\n","        specified for train and dev examples, but not for test examples.\n","    \"\"\"\n","    self.guid = guid\n","    self.text_a = text_a\n","    self.text_b = text_b\n","    self.label = label\n","\n","class PaddingInputExample(object):\n","  \"\"\"Fake example so the num input examples is a multiple of the batch size.\n","\n","  When running eval/predict on the TPU, we need to pad the number of examples\n","  to be a multiple of the batch size, because the TPU requires a fixed batch\n","  size. The alternative is to drop the last batch, which is bad because it means\n","  the entire output data won't be generated.\n","\n","  We use this class instead of `None` because treating `None` as padding\n","  batches could cause silent errors.\n","  \"\"\"\n","\n","class InputFeatures(object):\n","  \"\"\"A single set of features of data.\"\"\"\n","\n","  def __init__(self,\n","               input_ids,\n","               input_mask,\n","               segment_ids,\n","               label_id,\n","               is_real_example=True):\n","    self.input_ids = input_ids\n","    self.input_mask = input_mask\n","    self.segment_ids = segment_ids\n","    self.label_id = label_id\n","    self.is_real_example = is_real_example\n","\n","def _decode_record(record, name_to_features):\n","  \"\"\"Decodes a record to a TensorFlow example.\"\"\"\n","  example = tf.compat.v1.io.parse_single_example(record, name_to_features)\n","\n","  # tf.Example only supports tf.int64, but the TPU only supports tf.int32.\n","  # So cast all int64 to int32.\n","  for name in list(example.keys()):\n","    t = example[name]\n","    if t.dtype == tf.compat.v1.int64:\n","      t = tf.compat.v1.to_int32(t)\n","    example[name] = t\n","\n","  return example\n","\n","\n","def create_krbert_model(bert_config, num_labels, dropout_rate = 0.1):\n","    \"\"\"\n","    Creates a classification model.\n","    230919 : BERT Model 자체는 Hugging Face로 구축하지만 손실함수 구축 때문에 가져온다.\n","    + 텐서플로우 2버전에 맞게 코드를 변경한다.\n","    \"\"\"\n","    bert_model = TFBertModel(config=bert_config)\n","\n","    input_ids_input = tf.keras.layers.Input(shape=(None, ), dtype=tf.int32, name='input_ids')\n","    input_mask_input = tf.keras.layers.Input(shape=(None, ), dtype=tf.int32, name='input_mask')\n","    segment_ids_input = tf.keras.layers.Input(shape=(None, ), dtype=tf.int32, name='segment_ids')\n","\n","    outputs = bert_model(\n","        input_ids=input_ids_input,\n","        attention_mask=input_mask_input\n","    )\n","\n","    output_layer = outputs[1]\n","    dropout_layer = tf.keras.layers.Dropout(dropout_rate)(output_layer)\n","\n","    classification_layer = tf.keras.layers.Dense(1,\n","                                         activation = 'sigmoid',\n","                                         name = 'output_layer')(dropout_layer)\n","\n","\n","    model = tf.keras.models.Model(inputs = [input_ids_input,\n","                                         input_mask_input,\n","                                         segment_ids_input],\n","                               outputs = classification_layer)\n","\n","    return model\n","\n","def parse_record(record):\n","  return _decode_record(record, name_to_features)\n","\n","def split_data_and_label(dataset):\n","  features = {\n","      'input_ids' : dataset['input_ids'],\n","      'input_mask' : dataset['input_mask'],\n","      'is_real_example' : dataset['is_real_example'],\n","      'segment_ids' : dataset['segment_ids']\n","  }\n","  target = dataset['label_ids']\n","  return features, target\n","\n","\n","# def lr_warmup(learning_rate, warmup_steps_ratio, global_steps, total_steps):\n","\n","#   \"\"\"\n","#   warmup_step 동안 학습률이 선형적으로 증가하고, 이후는 일정한 값을 유지\n","#   KrBERT는 AdamW를 이용하므로 이후에 학습률은 Decay됨\n","\n","#   <혼동 방지>\n","#   global_steps : 모든 에포크를 통틀어 한 배치 당 1스텝\n","#   total_steps : 한 에포크 당 스텝(=전체 데이터를 배치 수로 나눈 값) * 에포크 수\n","#   \"\"\"\n","\n","#   warmup_steps = total_steps * warmup_steps_ratio\n","#   warmup_steps = tf.constant(num_warmup_steps, dtype=tf.float32)\n","#   global_steps = tf.cast(global_steps, tf.float32)\n","\n","#   warmup_percent_done = global_steps / warmup_steps\n","#   warmup_learning_rate = learning_rate * warmup_percent_done\n","\n","#   is_warmup = tf.cast(global_steps < warmup_steps, tf.float32)\n","#   learning_rate = (\n","#       (1.0 - is_warmup) * learning_rate + is_warmup * warmup_learning_rate)\n","\n","#   return learning_rate\n","\n","class WarmupCallback(tf.keras.callbacks.Callback):\n","    def __init__(self, total_steps=0, warmup_ratio = 0.0):\n","\n","        super(WarmupCallback, self).__init__()\n","        # self.learning_rate = learning_rate\n","        self.total_steps = total_steps\n","        self.warmup_ratio = warmup_ratio\n","        self.global_step = 0\n","        self.lrs = []\n","\n","    def on_train_batch_begin(self, batch, logs=None):\n","        previous_lr = (self.lrs[-1] if self.lrs\n","                                    else K.get_value(self.model.optimizer.learning_rate))\n","\n","        lr = self.lr_warmup(learning_rate = previous_lr,\n","                        global_steps=self.global_step,\n","                        total_steps=self.total_steps,\n","                        warmup_steps_ratio=self.warmup_ratio,\n","                       )\n","        # print('step start :', lr)\n","        K.set_value(self.model.optimizer.lr, lr)\n","\n","    def on_train_batch_end(self, batch, logs=None):\n","        self.global_step = self.global_step + 1\n","\n","        # 학습률 저장\n","        # lr = K.get_value(self.model.optimizer.lr)\n","        lr = K.get_value(self.model.optimizer.learning_rate)\n","        # print('step end : ', lr)\n","\n","        self.lrs.append(lr)\n","\n","    def lr_warmup(self, learning_rate, warmup_steps_ratio, global_steps, total_steps):\n","\n","      \"\"\"\n","      warmup_step 동안 학습률이 선형적으로 증가하고, 이후는 일정한 값을 유지\n","      KrBERT는 AdamW를 이용하므로 이후에 학습률은 Decay됨\n","\n","      <혼동 방지>\n","      global_steps : 모든 에포크를 통틀어 한 배치 당 1스텝\n","      total_steps : 한 에포크 당 스텝(=전체 데이터를 배치 수로 나눈 값) * 에포크 수\n","      \"\"\"\n","\n","      warmup_steps = total_steps * warmup_steps_ratio\n","      warmup_steps = tf.constant(warmup_steps, dtype=tf.float32)\n","      global_steps = tf.cast(global_steps, tf.float32)\n","\n","      warmup_percent_done = global_steps / warmup_steps\n","      warmup_learning_rate = learning_rate * warmup_percent_done\n","\n","      is_warmup = tf.cast(global_steps < warmup_steps, tf.float32)\n","      learning_rate = (\n","          (1.0 - is_warmup) * learning_rate + is_warmup * warmup_learning_rate)\n","\n","      return learning_rate\n","\n","# 230927 : 학습률 변화 추적 콜백함수\n","class LearningRateMonitor(tf.keras.callbacks.Callback):\n","  def on_epoch_end(self, epoch, logs = None):\n","    lr = self.model.optimizer.learning_rate\n","    print(f\"Epoch {epoch + 1} : Learning Rate : {lr.numpy()}\")\n","\n","# 230927 : 웜업 스텝 추적 콜백함수\n","class WarmupLrPrintCallback(tf.keras.callbacks.Callback):\n","    def __init__(self, warmup_steps):\n","        super(WarmupLrPrintCallback, self).__init__()\n","        self.warmup_steps = warmup_steps\n","        self.current_step = 1\n","\n","    def on_train_batch_end(self, batch, logs=None):\n","        # 웜업 스텝 범위 내에서만 실행\n","        if self.current_step % 500 == 0 and self.current_step < self.warmup_steps:\n","          learning_rate = self.model.optimizer.learning_rate.numpy()\n","          print(f\"Now Warm-up Step : {self.current_step}, Learning Rate = {learning_rate:.6f}\")\n","\n","        if self.current_step == self.warmup_steps:\n","            end_warmup_lr = self.model.optimizer.learning_rate.numpy()\n","            print(f'End Warm-up Step :{self.current_step}: Learning Rate = {end_warmup_lr:.6f}')\n","        self.current_step += 1\n","\n","# 하이퍼파라미터 튜닝\n","EPOCHS = 15\n","\n","def build_model_hpo(hp, hpo = True):\n","\n","  if hpo:\n","    # freeze_body = hp.Boolean('freeze_body')\n","    dropout_rate = hp.Float('dropout_rate', min_value = 0, max_value = 0.5, step = 0.1)\n","    weight_decay = hp.Float('weight_decay', min_value = 0.01, max_value = 0.1, step = 0.01)\n","    learning_rate = hp.Float('Learning_rate', min_value = 5e-6, max_value = 5e-5, step = 5e-6)\n","\n","    # 모델 훈련 파라미터\n","    # warmup_ratio = 0.1\n","    # warmup_steps = int(train_total_steps * warmup_ratio)\n","  # {'dropout_rate': 0.0,\n","  # 'learning_rate': 1.5000000000000002e-05,\n","  # 'warmup_steps': 1500,\n","  # 'end_learning_rate': 1e-07,\n","  # 'weight_decay': 0.05}\n","  else:\n","    droput_rate = 0.1\n","    weight_decay = 5e-2\n","\n","  # learning_rate = 1.5e-5\n","  warmup_steps = 1500\n","  end_learning_rate = 1e-7\n","\n","  epochs = EPOCHS\n","  train_steps_per_epoch = len(train_ex) // train_batch_size\n","  train_total_steps = train_steps_per_epoch * epochs\n","  decay_steps = train_total_steps - warmup_steps\n","\n","  # 모델 정의 및 훈련\n","  config = BertConfig.from_json_file(real_bert_config_file)\n","\n","  with strategy.scope():\n","    model = create_krbert_model(\n","        bert_config = config,\n","        num_labels = 2,\n","        dropout_rate = dropout_rate\n","    )\n","    load_tf_weights_in_bert(model,\n","                          config,\n","                          \"./MyModel_raw/model.ckpt-2000000.index\")\n","\n","  loss = tf.keras.losses.BinaryCrossentropy()\n","\n","  lr_schedule = tf.keras.optimizers.schedules.PolynomialDecay(\n","      learning_rate,\n","      decay_steps,\n","      end_learning_rate = end_learning_rate,\n","      power = 1.0,\n","      cycle = False\n","  )\n","\n","  warmup_lr_schedule = tfm.optimization.PolynomialWarmUp(\n","      after_warmup_lr_sched=lr_schedule,\n","      warmup_steps=warmup_steps,\n","      power=1.0\n","  )\n","\n","  optimizer = tf.keras.optimizers.AdamW(\n","      learning_rate = warmup_lr_schedule,\n","      weight_decay = weight_decay,\n","      epsilon = 1e-6,\n","      clipnorm = 1.0\n","  )\n","\n","  model.compile(\n","      optimizer = optimizer,\n","      loss = loss,\n","      metrics = ['accuracy']\n","  )\n","\n","  return model\n","\n","# GCS 연동, TPU 설정, csv 파일에서 데이터를 나눠서 다시 저장\n","\n","# GCS 연동 과정\n","auth.authenticate_user()\n","\n","# Google Cloud 프로젝트 ID\n","project_id = 'copying-book'\n","# project_id = 'service-copying-book@cloud-tpu.iam.gserviceaccount.com'\n","\n","!gcloud config set account dowrave@gmail.com\n","\n","# 이거 자동화는 무리일 듯 : 2개의 확인을 거쳐 들어가야 하고, 보안 키는 계속 달라짐\n","!gcloud auth login --no-launch-browser --quiet --force\n","!gcloud config set project 'copying-book'\n","# !gsutil acl ch -u dowrave@gmail.com:WRITE gs://steam-project-bucket\n","\n","# GCS 클라이언트\n","client = storage.Client()\n","\n","# GCS 버킷 이름\n","bucket_name = 'steam-project-bucket'\n","\n","# GCS 버킷 객체\n","bucket = client.get_bucket(bucket_name)\n","blob = bucket.blob('krbert_tensorflow/ModelTrain')\n","\n","!echo $COLAB_TPU_ADDR # TPU의 IP와 포트 확인\n","\n","try:\n","  TPU_PATH = f\"grpc://{os.environ['COLAB_TPU_ADDR']}\"\n","  use_tpu = True\n","except:\n","  use_tpu = False\n","\n","record_dir = \"gs://steam-project-bucket/krbert_tensorflow/ModelTrain/\"\n","tf.io.gfile.makedirs(record_dir)\n","\n","\n","if use_tpu:\n","  # TPU와 런타임 연결\n","\n","  tpu = tf.distribute.cluster_resolver.TPUClusterResolver(tpu=TPU_PATH)\n","\n","  tf.config.experimental_connect_to_cluster(tpu)\n","  tf.tpu.experimental.initialize_tpu_system(tpu)\n","  strategy = tf.distribute.TPUStrategy(tpu)\n","\n","else:\n","  strategy = tf.distribute.get_strategy()\n","\n","# 토크나이저\n","vocab_file = './vocab_char_16424.txt'\n","tokenizer = tokenization.FullTokenizer(vocab_file = real_vocab_file,\n","                                      do_lower_case = False)\n","max_seq_length = 512 # 고정\n","\n","train_batch_size = 64\n","# val_batch_size = train_batch_size // 8 # 231021 : 데이터 사이즈에 맞춤\n","val_batch_size = 32\n","\n","project_name = f\"TEST1_steam_krbert_BO_seq{max_seq_length}_{train_batch_size}_{val_batch_size}/\"\n","project_exists = bucket.blob(f'krbert_tensorflow/ModelTrain/' + project_name).exists()\n","\n","# tfrecord 파일 경로\n","train_tfrecord_name = f'REAL_LAST_train{train_batch_size}_seq{max_seq_length}.tfrecord'\n","val_tfrecord_name = f'REAL_LAST_val{train_batch_size}_seq{max_seq_length}.tfrecord'\n","test_tfrecord_name = f'REAL_LAST_test{train_batch_size}_seq{max_seq_length}.tfrecord'\n","\n","train_exists = bucket.blob(f'krbert_tensorflow/ModelTrain/' + train_tfrecord_name).exists()\n","val_exists = bucket.blob(f'krbert_tensorflow/ModelTrain/' + val_tfrecord_name).exists()\n","test_exists = bucket.blob(f'krbert_tensorflow/ModelTrain/' + test_tfrecord_name).exists()\n","\n","record_dir = \"gs://steam-project-bucket/krbert_tensorflow/ModelTrain/\"\n","train_file_dir = record_dir + train_tfrecord_name\n","val_file_dir = record_dir + val_tfrecord_name\n","test_file_dir = record_dir + test_tfrecord_name\n","\n","train_csv_name = f'train_data_{train_batch_size}.csv'\n","val_csv_name = f'val_data_{train_batch_size}.csv'\n","test_csv_name = f'test_data_{train_batch_size}.csv'\n","\n","# tfrecord가 없으면 데이터 분리 = csv 파일을 다시 만든다.\n","# if train_exists == False or val_exists == False:\n","\n","# 프로젝트명의 폴더가 없으면 데이터 분리부터 작업 진행.\n","if project_exists == False:\n","  print(\"데이터 재생성 및 분리\")\n","\n","  # 인풋 데이터 훈련, 검증, 테스트 데이터로 분리\n","  raw_df = pd.read_csv('../../korean_review_raw.csv', index_col = 0)\n","  df = raw_df[['id', 'review', 'recommend']]\n","  df = df.dropna(subset = ['review'])\n","\n","\n","  # 비율은 8 : 1 : 1\n","  train_data, temp_data = train_test_split(df,\n","                                          test_size = 0.2,\n","                                          stratify = df['recommend'])\n","\n","  val_data, test_data = train_test_split(temp_data,\n","                                          test_size = 0.5,\n","                                          stratify = temp_data['recommend'])\n","\n","  train_data.to_csv(real_data_dir + train_csv_name, index=False)\n","  val_data.to_csv(real_data_dir + val_csv_name, index=False)\n","  test_data.to_csv(real_data_dir + test_csv_name, index=False)\n","\n","  processor = SteamProcessor()\n","\n","  train_ex = processor.get_train_examples(real_data_dir, train_csv_name)\n","  val_ex = processor.get_dev_examples(real_data_dir, val_csv_name)\n","  test_ex = processor.get_test_examples(real_data_dir, test_csv_name)\n","  label_list = processor.get_labels()\n","\n","  # train\n","  file_based_convert_examples_to_features(train_ex,\n","                                          label_list,\n","                                          max_seq_length,\n","                                          tokenizer,\n","                                          train_file_dir)\n","  # val\n","  file_based_convert_examples_to_features(val_ex,\n","                                          label_list,\n","                                          max_seq_length,\n","                                          tokenizer,\n","                                          val_file_dir)\n","\n","  # test\n","  file_based_convert_examples_to_features(test_ex,\n","                                          label_list,\n","                                          max_seq_length,\n","                                          tokenizer,\n","                                          test_file_dir)\n","\n","else:\n","  print(\"이미 tfrecord 파일이 있음\")\n","  processor = SteamProcessor()\n","  train_ex = processor.get_train_examples(real_data_dir, train_csv_name)\n","  val_ex = processor.get_dev_examples(real_data_dir, val_csv_name)\n","  test_ex = processor.get_test_examples(real_data_dir, test_csv_name)\n","\n","buffer_size = 100\n","name_to_features = {\n","    \"input_ids\": tf.io.FixedLenFeature([max_seq_length], tf.int64),\n","    \"input_mask\": tf.io.FixedLenFeature([max_seq_length], tf.int64),\n","    \"segment_ids\": tf.io.FixedLenFeature([max_seq_length], tf.int64),\n","    \"label_ids\": tf.io.FixedLenFeature([], tf.int64),\n","    \"is_real_example\": tf.io.FixedLenFeature([], tf.int64),\n","}\n","\n","\n","# tfrecord -> dataset\n","train_ds = (tf.data.TFRecordDataset(train_file_dir).shuffle(buffer_size = buffer_size)\n","                                                .map(parse_record)\n","                                                .batch(train_batch_size,\n","                                                      drop_remainder = True)\n","                                                # .repeat()\n","                                                )\n","\n","val_ds = (tf.data.TFRecordDataset(val_file_dir).shuffle(buffer_size = buffer_size)\n","                                                .map(parse_record)\n","                                                .batch(val_batch_size,\n","                                                      drop_remainder = True)\n","                                                )\n","# test_ds = (tf.data.TFRecordDataset(test_file_dir).shuffle(buffer_size = 100)\n","#                                                 .map(parse_record)\n","#                                                 .batch(test_batch_size,\n","#                                                       drop_remainder = True)\n","#                                                 # .repeat()\n","#                                                 )\n","\n","# 데이터셋에서 타겟 데이터 분리\n","train_ds = train_ds.map(lambda x : split_data_and_label(x))\n","val_ds = val_ds.map(lambda x : split_data_and_label(x))\n","# test_ds = test_ds.map(lambda x : split_data_and_label(x))\n","# ----------------------------------------------------------------------------\n","\n","# 하이퍼파라미터 튜닝 모델 생성\n","hp = keras_tuner.HyperParameters()\n","# model = build_model_hpo(hp)\n","\n","# 튜너 설정\n","use_tuner = 'BO'\n","\n","# Bayesian Optimzation\n","if use_tuner == 'BO':\n","  tuner = keras_tuner.BayesianOptimization(\n","      hypermodel = build_model_hpo,\n","      objective = 'val_loss',\n","      max_trials = 15,\n","      executions_per_trial = 3,\n","      overwrite = False,\n","      directory = 'gs://steam-project-bucket/krbert_tensorflow/ModelTrain',\n","      # directory = 'ModelTrain',\n","      project_name= project_name,\n","  )\n","\n","# Hyper Band\n","elif use_tuner == 'HB':\n","  tuner = keras_tuner.Hyperband(\n","    hypermodel=build_model_hpo,\n","    objective=\"val_loss\",\n","    max_epochs = EPOCHS, # 최종적으로 가장 많이 수행할 1개의 모델의 최대 에포크 수\n","                    # factor 값을 3으로 잡음\n","    factor = 3, # eta값 - bracket 별로 증가되는 에포크 배수 (= 줄어드는 하이퍼파라미터 쌍 비율)\n","    hyperband_iterations = 1, # 전체 하이퍼밴드 알고리즘 반복 횟수\n","    distribution_strategy = strategy,\n","    overwrite = False,\n","      # directory = 'gs://steam-project-bucket/krbert_tensorflow/ModelTrain',\n","      directory = 'ModelTrain',\n","      project_name= project_name,\n","  )\n","\n","# 콜백함수 지정\n","earlystopping_cb = tf.keras.callbacks.EarlyStopping(\n","  monitor = 'val_loss',\n","  min_delta = 0.001, # \"향상\"의 기준치\n","  patience = 3,\n","  restore_best_weights = True\n",")\n","lr_monitor_cb = LearningRateMonitor()\n","csv_logger_cb = tf.keras.callbacks.CSVLogger(f\"steam_krbert_hpo_seq{max_seq_length}_{train_batch_size}_{val_batch_size}.csv\", append=True)\n","tb_cb = tf.keras.callbacks.TensorBoard('gs://steam-project-bucket/krbert_tensorflow/ModelTrain/logs')\n","\n","tuner.search(train_ds,\n","              epochs = EPOCHS,\n","              validation_data = val_ds,\n","              callbacks = [earlystopping_cb,\n","                           lr_monitor_cb,\n","                           csv_logger_cb,\n","                           tb_cb])\n","\n","best_hps = tuner.get_best_hyperparameters(num_trials = 1)[0]"]},{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"6mkyjZi1Jezf","outputId":"0e03b867-4e21-4143-9842-e129e3531ce0","executionInfo":{"status":"ok","timestamp":1701965036602,"user_tz":-540,"elapsed":3198885,"user":{"displayName":"Hyeontae Lee","userId":"18343648907583635281"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["best hp combinations :  {'dropout_rate': 0.1, 'weight_decay': 0.01, 'Learning_rate': 1.5000000000000002e-05}\n","best_loss :  0.34772299726804096\n","Epoch 1/10\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/keras/src/engine/functional.py:642: UserWarning: Input dict contained keys ['is_real_example'] which did not match any model input. They will be ignored by the model.\n","  inputs = self._flatten_to_reference_inputs(inputs)\n"]},{"output_type":"stream","name":"stdout","text":["      6/Unknown - 62s 241ms/step - loss: 0.5871 - accuracy: 0.7474"]},{"output_type":"stream","name":"stderr","text":["WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.2270s vs `on_train_batch_end` time: 0.7899s). Check your callbacks.\n"]},{"output_type":"stream","name":"stdout","text":["   2426/Unknown - 633s 236ms/step - loss: 0.4570 - accuracy: 0.8011Epoch 1 : Learning Rate : 1.3964388017484453e-05\n","2426/2426 [==============================] - 684s 257ms/step - loss: 0.4570 - accuracy: 0.8011 - val_loss: 0.3665 - val_accuracy: 0.8430\n","Epoch 2/10\n","2426/2426 [==============================] - ETA: 0s - loss: 0.3701 - accuracy: 0.8396Epoch 2 : Learning Rate : 1.2928348951390944e-05\n","2426/2426 [==============================] - 612s 252ms/step - loss: 0.3701 - accuracy: 0.8396 - val_loss: 0.3517 - val_accuracy: 0.8483\n","Epoch 3/10\n","2426/2426 [==============================] - ETA: 0s - loss: 0.3476 - accuracy: 0.8497Epoch 3 : Learning Rate : 1.1892309885297436e-05\n","2426/2426 [==============================] - 608s 250ms/step - loss: 0.3476 - accuracy: 0.8497 - val_loss: 0.3515 - val_accuracy: 0.8493\n","Epoch 4/10\n","2426/2426 [==============================] - ETA: 0s - loss: 0.3324 - accuracy: 0.8575Epoch 4 : Learning Rate : 1.0856270819203928e-05\n","2426/2426 [==============================] - 609s 251ms/step - loss: 0.3324 - accuracy: 0.8575 - val_loss: 0.3670 - val_accuracy: 0.8486\n","Epoch 5/10\n","2426/2426 [==============================] - ETA: 0s - loss: 0.3225 - accuracy: 0.8629Epoch 5 : Learning Rate : 9.820232662605122e-06\n","2426/2426 [==============================] - 637s 262ms/step - loss: 0.3225 - accuracy: 0.8629 - val_loss: 0.3605 - val_accuracy: 0.8500\n"]},{"output_type":"execute_result","data":{"text/plain":["<keras.src.callbacks.History at 0x7e1eda0ee020>"]},"metadata":{},"execution_count":2}],"source":["print(\"best hp combinations : \", best_hps.values)\n","print(\"best_loss : \", tuner.oracle.get_best_trials(1)[0].score)\n","\n","model = tuner.hypermodel.build(best_hps)\n","model.fit(train_ds, epochs = 10,\n","          validation_data = (val_ds),\n","          callbacks = [earlystopping_cb,\n","                           lr_monitor_cb,\n","                           csv_logger_cb,\n","                           tb_cb])\n","\n"]},{"cell_type":"code","execution_count":11,"metadata":{"executionInfo":{"elapsed":25524,"status":"ok","timestamp":1701966744673,"user":{"displayName":"Hyeontae Lee","userId":"18343648907583635281"},"user_tz":-540},"id":"VAgRdTQtlUz3"},"outputs":[],"source":["model.save('gs://steam-project-bucket/krbert_tensorflow/realLastModel.h5')\n","model.save('gs://steam-project-bucket/krbert_tensorflow/realLastModel.keras')"]},{"cell_type":"code","source":["!pip install google-cloud-storage\n","\n","from google.cloud import storage\n","client = storage.Client()\n","bucket = client.bucket('steam-project-bucket')\n","\n","\n","local_directory = 'RealLastModel/'\n","os.makedirs(local_directory, exist_ok=True)\n","\n","# print(blobs)\n","\n","\n","# print(f'폴더 다운로드 완료: {local_directory}')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ABexXaOqyA2C","executionInfo":{"status":"ok","timestamp":1701969036759,"user_tz":-540,"elapsed":9207,"user":{"displayName":"Hyeontae Lee","userId":"18343648907583635281"}},"outputId":"58cdf181-d3d6-4c81-a587-3bc9662e9f0f"},"execution_count":25,"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: google-cloud-storage in /usr/local/lib/python3.10/dist-packages (2.8.0)\n","Requirement already satisfied: google-auth<3.0dev,>=1.25.0 in /usr/local/lib/python3.10/dist-packages (from google-cloud-storage) (2.17.3)\n","Requirement already satisfied: google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5 in /usr/local/lib/python3.10/dist-packages (from google-cloud-storage) (2.11.1)\n","Requirement already satisfied: google-cloud-core<3.0dev,>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from google-cloud-storage) (2.3.3)\n","Requirement already satisfied: google-resumable-media>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from google-cloud-storage) (2.6.0)\n","Requirement already satisfied: requests<3.0.0dev,>=2.18.0 in /usr/local/lib/python3.10/dist-packages (from google-cloud-storage) (2.31.0)\n","Requirement already satisfied: googleapis-common-protos<2.0.dev0,>=1.56.2 in /usr/local/lib/python3.10/dist-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5->google-cloud-storage) (1.61.0)\n","Requirement already satisfied: protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0.dev0,>=3.19.5 in /usr/local/lib/python3.10/dist-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5->google-cloud-storage) (3.20.3)\n","Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3.0dev,>=1.25.0->google-cloud-storage) (5.3.2)\n","Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth<3.0dev,>=1.25.0->google-cloud-storage) (0.3.0)\n","Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3.0dev,>=1.25.0->google-cloud-storage) (1.16.0)\n","Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth<3.0dev,>=1.25.0->google-cloud-storage) (4.9)\n","Requirement already satisfied: google-crc32c<2.0dev,>=1.0 in /usr/local/lib/python3.10/dist-packages (from google-resumable-media>=2.3.2->google-cloud-storage) (1.5.0)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0dev,>=2.18.0->google-cloud-storage) (3.3.2)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0dev,>=2.18.0->google-cloud-storage) (3.6)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0dev,>=2.18.0->google-cloud-storage) (2.0.7)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0dev,>=2.18.0->google-cloud-storage) (2023.11.17)\n","Requirement already satisfied: pyasn1<0.6.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3.0dev,>=1.25.0->google-cloud-storage) (0.5.1)\n"]}]},{"cell_type":"code","source":["blobs = bucket.list_blobs(prefix='krbert_tensorflow/mymodel/mymodel')\n","local_directory = './231208_RealLastModel/'\n","\n","if os.path.exists(local_directory) == False:\n","  os.mkdir(local_directory)\n","\n","for blob in blobs:\n","    # 로컬 디렉토리에 파일 다운로드\n","\n","    relative_path = os.path.relpath(blob.name, 'krbert_tensorflow/mymodel/mymodel')\n","    if relative_path == '.':\n","      continue\n","    # print(blob.name)\n","    print(relative_path)\n","\n","    # \"원하는 폴더 이름\"이 포함된 파일 또는 폴더인 경우에 다운로드\n","    if relative_path == 'assets' or relative_path == 'variables':\n","        # 로컬 디렉토리에 상대 경로에 해당하는 폴더 생성\n","        os.mkdir(relative_path)\n","        continue\n","\n","    # 로컬 디렉토리에 파일 다운로드\n","    local_filename = os.path.join(local_directory, relative_path)\n","    blob.download_to_filename(local_filename)\n","    print(f'다운로드 완료: {local_filename}')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":560},"id":"wuLPzrCZPnv5","executionInfo":{"status":"error","timestamp":1701970797973,"user_tz":-540,"elapsed":3388,"user":{"displayName":"Hyeontae Lee","userId":"18343648907583635281"}},"outputId":"de0613b4-a6b5-4d04-aa39-6051838e0a10"},"execution_count":47,"outputs":[{"output_type":"stream","name":"stdout","text":["assets\n","fingerprint.pb\n","다운로드 완료: ./231208_RealLastModel/fingerprint.pb\n","keras_metadata.pb\n","다운로드 완료: ./231208_RealLastModel/keras_metadata.pb\n","saved_model.pb\n","다운로드 완료: ./231208_RealLastModel/saved_model.pb\n","variables\n","variables/variables.data-00000-of-00001\n"]},{"output_type":"error","ename":"FileNotFoundError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-47-0ce5b42cdb24>\u001b[0m in \u001b[0;36m<cell line: 7>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     22\u001b[0m     \u001b[0;31m# 로컬 디렉토리에 파일 다운로드\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m     \u001b[0mlocal_filename\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlocal_directory\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrelative_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m     \u001b[0mblob\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdownload_to_filename\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlocal_filename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'다운로드 완료: {local_filename}'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/google/cloud/storage/blob.py\u001b[0m in \u001b[0;36mdownload_to_filename\u001b[0;34m(self, filename, client, start, end, raw_download, if_etag_match, if_etag_not_match, if_generation_match, if_generation_not_match, if_metageneration_match, if_metageneration_not_match, timeout, checksum, retry)\u001b[0m\n\u001b[1;32m   1253\u001b[0m         \u001b[0mclient\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_require_client\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclient\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1254\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1255\u001b[0;31m             \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"wb\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mfile_obj\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1256\u001b[0m                 client.download_blob_to_file(\n\u001b[1;32m   1257\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: './231208_RealLastModel/variables/variables.data-00000-of-00001'"]}]},{"cell_type":"code","execution_count":null,"metadata":{"id":"IaubFGyogvd0","executionInfo":{"status":"aborted","timestamp":1701965107206,"user_tz":-540,"elapsed":8,"user":{"displayName":"Hyeontae Lee","userId":"18343648907583635281"}}},"outputs":[],"source":["\n","%tensorboard --logdir=\"/ModelTrain/logs\""]},{"cell_type":"code","execution_count":null,"metadata":{"id":"FnLMpk7b07Bc","executionInfo":{"status":"aborted","timestamp":1701965107206,"user_tz":-540,"elapsed":8,"user":{"displayName":"Hyeontae Lee","userId":"18343648907583635281"}}},"outputs":[],"source":[]},{"cell_type":"markdown","metadata":{"id":"8Mfm5HKS_oug"},"source":["# tf 1.x 버전으로 시도\n","- 에포크를 돌려도 학습률이 개선되지 않는 문제가 있다. `try_tf1.py`에 저장함.\n","- `estimator`를 어떻게 사용할지 감이 오지 않아서 익숙한 텐서플로우 2.x 버전을 사용하기로 했다. `ckpt`에 저장된 가중치를 로드하는 방법을 알게 되기도 했고."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"0nZT3lroSq00","executionInfo":{"status":"aborted","timestamp":1701965107206,"user_tz":-540,"elapsed":8,"user":{"displayName":"Hyeontae Lee","userId":"18343648907583635281"}}},"outputs":[],"source":[]}],"metadata":{"accelerator":"TPU","colab":{"provenance":[],"mount_file_id":"1ieO5z4EB23fKtzUBMIcw5jsvWBlevxBh","authorship_tag":"ABX9TyMc9ajjYWuaETRzEwm7tW2z"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}