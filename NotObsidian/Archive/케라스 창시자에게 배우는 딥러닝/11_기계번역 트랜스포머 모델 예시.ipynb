{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!wget http://storage.googleapis.com/download.tensorflow.org/data/spa-eng.zip\n!unzip -q spa-eng.zip","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2023-07-26T04:11:10.249410Z","iopub.execute_input":"2023-07-26T04:11:10.250172Z","iopub.status.idle":"2023-07-26T04:11:12.780691Z","shell.execute_reply.started":"2023-07-26T04:11:10.250121Z","shell.execute_reply":"2023-07-26T04:11:12.779020Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"--2023-07-26 04:11:11--  http://storage.googleapis.com/download.tensorflow.org/data/spa-eng.zip\nResolving storage.googleapis.com (storage.googleapis.com)... 172.253.114.128, 142.250.1.128, 108.177.111.128, ...\nConnecting to storage.googleapis.com (storage.googleapis.com)|172.253.114.128|:80... connected.\nHTTP request sent, awaiting response... 200 OK\nLength: 2638744 (2.5M) [application/zip]\nSaving to: ‘spa-eng.zip’\n\nspa-eng.zip         100%[===================>]   2.52M  --.-KB/s    in 0.02s   \n\n2023-07-26 04:11:11 (153 MB/s) - ‘spa-eng.zip’ saved [2638744/2638744]\n\n","output_type":"stream"}]},{"cell_type":"code","source":"import tensorflow as tf\nimport string\nimport re\nimport random\nfrom tensorflow import keras\nfrom tensorflow.keras import layers","metadata":{"execution":{"iopub.status.busy":"2023-07-26T04:12:03.246591Z","iopub.execute_input":"2023-07-26T04:12:03.246977Z","iopub.status.idle":"2023-07-26T04:12:03.253494Z","shell.execute_reply.started":"2023-07-26T04:12:03.246944Z","shell.execute_reply":"2023-07-26T04:12:03.252233Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"# 데이터 확인하기\ntext_file = 'spa-eng/spa.txt'\nwith open(text_file) as f:\n\tlines = f.read().split(\"\\n\")[:-1]\ntext_pairs = []\nfor line in lines:\n\tenglish, spanish = line.split(\"\\t\")\n\tspanish = '[start] ' + spanish + ' [end]'\n\ttext_pairs.append((english, spanish))\n\n# text_pairs 내용\n\nprint(random.choice(text_pairs))","metadata":{"execution":{"iopub.status.busy":"2023-07-26T04:12:04.143718Z","iopub.execute_input":"2023-07-26T04:12:04.144114Z","iopub.status.idle":"2023-07-26T04:12:04.366256Z","shell.execute_reply.started":"2023-07-26T04:12:04.144056Z","shell.execute_reply":"2023-07-26T04:12:04.365144Z"},"trusted":true},"execution_count":7,"outputs":[{"name":"stdout","text":"('This wine tastes great.', '[start] Este vino sabe estupendo. [end]')\n","output_type":"stream"}]},{"cell_type":"code","source":"# 데이터 섞은 뒤 훈련/검증/테스트 나누기\nimport random\n\nrandom.shuffle(text_pairs)\nnum_val_samples = int(0.15 * len(text_pairs))\nnum_train_samples = len(text_pairs) - 2 * num_val_samples\ntrain_pairs = text_pairs[:num_train_samples]\nval_pairs = text_pairs[num_train_samples:num_train_samples + num_val_samples]\ntest_pairs = text_pairs[num_train_samples + num_val_samples : ]","metadata":{"execution":{"iopub.status.busy":"2023-07-26T04:12:04.535382Z","iopub.execute_input":"2023-07-26T04:12:04.536040Z","iopub.status.idle":"2023-07-26T04:12:04.652445Z","shell.execute_reply.started":"2023-07-26T04:12:04.536000Z","shell.execute_reply":"2023-07-26T04:12:04.651540Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"# 11-26. 영어 & 스페인어 텍스트 쌍 벡터화\nstrip_chars = string.punctuation + \"¿\"\nstrip_chars = strip_chars.replace(\"[\", \"\")\nstrip_chars = strip_chars.replace(\"]\", \"\")\n\ndef custom_standardization(input_string):\n    # `[, ]` 문자 유지, ¿ 문자를 포함한 strings.punctuation에 있는 다른 문자 삭제\n\tlower_case = tf.strings.lower(input_string)\n\treturn tf.strings.regex_replace(\n\tlower_case, f\"[{re.escape(strip_chars)}]\", \"\")\n\nvoca_size = 15000\nsequence_length = 20\n\n# 영어\nsource_vectorization = layers.TextVectorization(\n\t\t\t\t\t\t\t\t\t\t\t\tmax_tokens = voca_size,\n\t\t\t\t\t\t\t\t\t\t\t\toutput_mode = 'int',\n\t\t\t\t\t\t\t\t\t\t\t\toutput_sequence_length = sequence_length\n)\n\n# 스페인어\ntarget_vectorization = layers.TextVectorization(\n\t\t\t\t\t\t\t\t\t\t\t\tmax_tokens = voca_size,\n\t\t\t\t\t\t\t\t\t\t\t\toutput_mode = 'int',\n\t\t\t\t\t\t\t\t\t\t\t\toutput_sequence_length = sequence_length + 1, # 훈련 중 한 스텝 앞선 문장이 필요하기 때문에 토큰 1개 추가\n\t\t\t\t\t\t\t\t\t\t\t\tstandardize = custom_standardization\n)\n\ntrain_english_texts = [pair[0] for pair in train_pairs]\ntrain_spanish_texts = [pair[1] for pair in train_pairs]\nsource_vectorization.adapt(train_english_texts)\ntarget_vectorization.adapt(train_spanish_texts)","metadata":{"execution":{"iopub.status.busy":"2023-07-26T04:56:47.847843Z","iopub.execute_input":"2023-07-26T04:56:47.848449Z","iopub.status.idle":"2023-07-26T04:56:59.546464Z","shell.execute_reply.started":"2023-07-26T04:56:47.848410Z","shell.execute_reply":"2023-07-26T04:56:59.545333Z"},"trusted":true},"execution_count":45,"outputs":[]},{"cell_type":"code","source":"# 데이터 -> 파이프라인 변환\nbatch_size = 64\n\ndef format_dataset(eng, spa):\n\teng = source_vectorization(eng)\n\tspa = target_vectorization(spa)\n\treturn ({\n\t\t'english' : eng,\n\t\t'spanish' : spa[:, :-1] # 입력 스페인어 문장은 마지막 토큰 포함 X라서 입출력 길이 동일\n\t}, spa[:, 1:]) # 타깃 스페인어 문장\n\ndef make_dataset(pairs):\n\teng_texts, spa_texts = zip(*pairs)\n\teng_texts = list(eng_texts)\n\tspa_texts = list(spa_texts)\n\tdataset = tf.data.Dataset.from_tensor_slices((eng_texts, spa_texts))\n\tdataset = dataset.batch(batch_size)\n\tdataset = dataset.map(format_dataset, num_parallel_calls = 4)\n\treturn dataset.shuffle(2048).prefetch(16).cache() # 전처리 속도 높이기 위한 캐싱\n\ntrain_ds = make_dataset(train_pairs)\nval_ds = make_dataset(val_pairs)\n\n# 크기 확인\nfor inputs, targets in train_ds.take(1):\n\tprint(f\"inputs['english'].shape : {inputs['english'].shape}\")\n\tprint(f\"inputs['spanish'].shape : {inputs['spanish'].shape}\")\n\tprint(f\"targets.shape : {targets.shape}\")\n\tprint(inputs['english'][0])","metadata":{"execution":{"iopub.status.busy":"2023-07-26T04:56:59.548528Z","iopub.execute_input":"2023-07-26T04:56:59.548848Z","iopub.status.idle":"2023-07-26T04:57:00.995921Z","shell.execute_reply.started":"2023-07-26T04:56:59.548822Z","shell.execute_reply":"2023-07-26T04:57:00.994775Z"},"trusted":true},"execution_count":46,"outputs":[{"name":"stdout","text":"inputs['english'].shape : (64, 20)\ninputs['spanish'].shape : (64, 20)\ntargets.shape : (64, 20)\ntf.Tensor(\n[   6   14 1312   10  813    0    0    0    0    0    0    0    0    0\n    0    0    0    0    0    0], shape=(20,), dtype=int64)\n","output_type":"stream"}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class PositionalEmbedding(layers.Layer):\n\n  # 위치 임베딩 : 시퀀스 길이를 미리 알아야 한다는 게 단점임\n  def __init__(self, sequence_length, input_dim, output_dim, **kwargs):\n    super().__init__(**kwargs)\n    self.token_embeddings = layers.Embedding(\n        input_dim = input_dim, output_dim = output_dim\n    )\n    self.position_embeddings = layers.Embedding(\n        input_dim = sequence_length, output_dim = output_dim\n    )\n    self.sequence_length = sequence_length\n    self.input_dim = input_dim\n    self.output_dim = output_dim\n\n  def call(self, inputs):\n    length = tf.shape(inputs)[-1]\n    positions = tf.range(start = 0, limit = length, delta = 1)\n    embedded_tokens = self.token_embeddings(inputs)\n    embedded_positions = self.position_embeddings(positions)\n    return embedded_tokens + embedded_positions\n\n  # 입력 0 패딩을 무시하는 마스킹 생성.\n  # 프레임워크에 의해 자동으로 호출되며, 마스킹은 다음 층으로 전달된다.\n  def compute_mask(self, inputs, mask = None):\n    return tf.math.not_equal(inputs, 0)\n\n  def get_config(self):\n    config = super().get_config()\n    config.update({\n        \"output_dim\" : self.output_dim,\n        \"sequence_length\" : self.sequence_length,\n        \"input_dim\" : self.input_dim\n    })\n    return config\n","metadata":{"execution":{"iopub.status.busy":"2023-07-26T04:57:00.997816Z","iopub.execute_input":"2023-07-26T04:57:00.998250Z","iopub.status.idle":"2023-07-26T04:57:01.008640Z","shell.execute_reply.started":"2023-07-26T04:57:00.998212Z","shell.execute_reply":"2023-07-26T04:57:01.007675Z"},"trusted":true},"execution_count":47,"outputs":[]},{"cell_type":"code","source":"class TransformerEncoder(layers.Layer):\n  def __init__(self, embed_dim, dense_dim, num_heads, **kwargs):\n    super().__init__(**kwargs)\n    self.embed_dim = embed_dim # 입력 토큰 벡터 크기\n    self.dense_dim = dense_dim # 내부 밀집 층 크기\n    self.num_heads = num_heads # 어텐션 헤드 개수\n    self.attention = layers.MultiHeadAttention(\n        num_heads = num_heads, key_dim = embed_dim\n    )\n    self.dense_proj = keras.Sequential(\n        [\n            layers.Dense(dense_dim, activation = 'relu'),\n            layers.Dense(embed_dim),\n        ]\n    )\n    self.layernorm_1 = layers.LayerNormalization()\n    self.layernorm_2 = layers.LayerNormalization()\n\n  # 연산 수행\n  def call(self, inputs, mask = None):\n\n    # Embedding층의 마스크는 2D이나, 어텐션층은 3D나 4D 기대\n    if mask is not None:\n      mask = mask[:, tf.newaxis, :]\n\n    attention_output = self.attention(inputs, inputs, attention_mask = mask)\n    proj_input = self.layernorm_1(inputs + attention_output)\n    proj_output = self.dense_proj(proj_input)\n    return self.layernorm_2(proj_input + proj_output)\n\n  # 모델 저장을 위한 직렬화 구현\n  def get_config(self):\n    config = super().get_config()\n    config.update({\n        'embed_dim' : self.embed_dim,\n        'num_heads' : self.num_heads,\n        'dense_dim' : self.dense_dim\n    })\n    return config","metadata":{"execution":{"iopub.status.busy":"2023-07-26T04:57:01.011140Z","iopub.execute_input":"2023-07-26T04:57:01.011903Z","iopub.status.idle":"2023-07-26T04:57:01.024344Z","shell.execute_reply.started":"2023-07-26T04:57:01.011869Z","shell.execute_reply":"2023-07-26T04:57:01.023409Z"},"trusted":true},"execution_count":48,"outputs":[]},{"cell_type":"code","source":"class TransformerDecoder(layers.Layer):\n\tdef __init__(self, embed_dim, dense_dim, num_heads, **kwargs):\n\t\tsuper().__init__(**kwargs)\n\t\tself.embed_dim = embed_dim\n\t\tself.dense_dim = dense_dim\n\t\tself.num_heads = num_heads\n\t\tself.attention_1 = layers.MultiHeadAttention(num_heads = num_heads,\n\t\tkey_dim = embed_dim)\n\t\tself.attention_2 = layers.MultiHeadAttention(num_heads = num_heads,\n\t\tkey_dim = embed_dim)\n\t\tself.attention_3 = layers.MultiHeadAttention(num_heads = num_heads,\n\t\tkey_dim = embed_dim)\n\t\tself.dense_proj = keras.Sequential([layers.Dense(dense_dim, activation = 'relu'),\n\t\tlayers.Dense(embed_dim),])\n\t\tself.layernorm_1 = layers.LayerNormalization()\n\t\tself.layernorm_2 = layers.LayerNormalization()\n\t\tself.layernorm_3 = layers.LayerNormalization()\n\t\tself.supports_masking = True # 입력 마스킹을 출력으로 전달하게 함\n\t\t# layer.compute_mask()는 위 속성이 False이면 에러를 반환한다.\n\tdef get_config(self):\n\t\tconfig = super().get_config()\n\t\tconfig.update({\n\t\t'embed_dim' : self.embed_dim,\n\t\t'num_heads' : self.num_heads,\n\t\t'dense_dim' : self.dense_dim\n\t\t  })\n\t\treturn config\n\n\t# 코잘 마스킹 : 미래 타임스텝의 데이터를 사용하지 못하게 한다\n\tdef get_casual_attention_mask(self, inputs):\n\t\tinput_shape= tf.shape(inputs)\n\t\tbatch_size, sequence_length = input_shape[0], input_shape[1]\n\t\ti = tf.range(sequence_length)[:, tf.newaxis]\n\t\tj = tf.range(sequence_length)\n\t\tmask = tf.cast(i >= j, dtype = 'int32') #\n\t\tmask = tf.reshape(mask, (1, input_shape[1], input_shape[1]))\n\n\t\tmult = tf.concat([tf.expand_dims(batch_size, -1),\n\t\t\t\t\t\t  tf.constant([1, 1], dtype = tf.int32)], axis = 0)\n\t\treturn tf.tile(mask, mult)\n\n\tdef call(self, inputs, encoder_outputs, mask = None):\n\t\tcasual_mask = self.get_casual_attention_mask(inputs)\n\n\t\tif mask is not None:\n\t\t\tpadding_mask = tf.cast(mask[:, tf.newaxis, :], dtype = 'int32')\n\t\t\tpadding_mask = tf.minimum(padding_mask, casual_mask)\n\n\t\tattention_output_1 = self.attention_1(\n\t\t\tquery = inputs,\n\t\t\tvalue = inputs,\n\t\t\tkey = inputs,\n\t\t\tattention_mask = casual_mask\n\t\t)\n\t\tattention_output_1 = self.layernorm_1(inputs + attention_output_1)\n\n\t\tattention_output_2 = self.attention_2(\n\t\t\tquery = attention_output_1,\n\t\t\tvalue = encoder_outputs,\n\t\t\tkey = encoder_outputs,\n\t\t\tattention_mask = padding_mask, # 합친 마스킹을 소스 + 타깃 시퀀스를 연결시키는 2번째 어텐션 층에 전달\n\t\t)\n\t\tattention_output_2 = self.layernorm_2(\n\t\t\tattention_output_1 + attention_output_2\n\t\t)\n\n\t\tproj_output = self.dense_proj(attention_output_2)\n\t\treturn self.layernorm_3(attention_output_2 + proj_output)","metadata":{"execution":{"iopub.status.busy":"2023-07-26T04:57:01.027678Z","iopub.execute_input":"2023-07-26T04:57:01.028191Z","iopub.status.idle":"2023-07-26T04:57:01.046071Z","shell.execute_reply.started":"2023-07-26T04:57:01.028165Z","shell.execute_reply":"2023-07-26T04:57:01.045122Z"},"trusted":true},"execution_count":49,"outputs":[]},{"cell_type":"code","source":"# 엔드 투 엔드 트랜스포머\nembed_dim = 256\ndense_dim = 2048\nnum_heads = 8\n\nencoder_inputs = keras.Input(shape = (None,), dtype = 'int64', name = 'english')\nx = PositionalEmbedding(sequence_length, voca_size, embed_dim)(encoder_inputs)\nencoder_outputs = TransformerEncoder(embed_dim, dense_dim, num_heads)(x)\n\ndecoder_inputs = keras.Input(shape = (None, ), dtype = 'int64', name = 'spanish')\nx = PositionalEmbedding(sequence_length, voca_size, embed_dim)(decoder_inputs)\nx = TransformerDecoder(embed_dim, dense_dim, num_heads)(x, encoder_outputs)\nx = layers.Dropout(0.5)(x)\ndecoder_outputs = layers.Dense(voca_size, activation = 'softmax')(x)\n\ntransformer = keras.Model([encoder_inputs, decoder_inputs], decoder_outputs)\n\ntransformer.compile(\n\t\t\t\t\t optimizer = 'rmsprop',\n\t\t\t\t\t loss = 'sparse_categorical_crossentropy',\n\t\t\t\t\t metrics = ['accuracy']\n)\ntransformer.fit(train_ds, epochs = 5, validation_data = val_ds)","metadata":{"execution":{"iopub.status.busy":"2023-07-26T04:57:01.047485Z","iopub.execute_input":"2023-07-26T04:57:01.048162Z","iopub.status.idle":"2023-07-26T05:05:28.540807Z","shell.execute_reply.started":"2023-07-26T04:57:01.048129Z","shell.execute_reply":"2023-07-26T05:05:28.539624Z"},"trusted":true},"execution_count":50,"outputs":[{"name":"stdout","text":"Epoch 1/5\n1302/1302 [==============================] - 109s 77ms/step - loss: 3.7898 - accuracy: 0.4411 - val_loss: 2.9112 - val_accuracy: 0.5309\nEpoch 2/5\n1302/1302 [==============================] - 86s 66ms/step - loss: 2.8509 - accuracy: 0.5506 - val_loss: 2.5258 - val_accuracy: 0.5891\nEpoch 3/5\n1302/1302 [==============================] - 85s 65ms/step - loss: 2.5564 - accuracy: 0.5936 - val_loss: 2.3801 - val_accuracy: 0.6122\nEpoch 4/5\n1302/1302 [==============================] - 85s 66ms/step - loss: 2.3929 - accuracy: 0.6197 - val_loss: 2.3272 - val_accuracy: 0.6241\nEpoch 5/5\n1302/1302 [==============================] - 85s 66ms/step - loss: 2.2883 - accuracy: 0.6381 - val_loss: 2.2975 - val_accuracy: 0.6338\n","output_type":"stream"},{"execution_count":50,"output_type":"execute_result","data":{"text/plain":"<keras.callbacks.History at 0x7860be282800>"},"metadata":{}}]},{"cell_type":"code","source":"transformer.summary()","metadata":{"execution":{"iopub.status.busy":"2023-07-26T05:05:28.542308Z","iopub.execute_input":"2023-07-26T05:05:28.542875Z","iopub.status.idle":"2023-07-26T05:05:28.580736Z","shell.execute_reply.started":"2023-07-26T05:05:28.542840Z","shell.execute_reply":"2023-07-26T05:05:28.580000Z"},"trusted":true},"execution_count":51,"outputs":[{"name":"stdout","text":"Model: \"model_2\"\n__________________________________________________________________________________________________\n Layer (type)                   Output Shape         Param #     Connected to                     \n==================================================================================================\n english (InputLayer)           [(None, None)]       0           []                               \n                                                                                                  \n spanish (InputLayer)           [(None, None)]       0           []                               \n                                                                                                  \n positional_embedding_11 (Posit  (None, None, 256)   3845120     ['english[0][0]']                \n ionalEmbedding)                                                                                  \n                                                                                                  \n positional_embedding_12 (Posit  (None, None, 256)   3845120     ['spanish[0][0]']                \n ionalEmbedding)                                                                                  \n                                                                                                  \n transformer_encoder_5 (Transfo  (None, None, 256)   3155456     ['positional_embedding_11[0][0]']\n rmerEncoder)                                                                                     \n                                                                                                  \n transformer_decoder_4 (Transfo  (None, None, 256)   5259520     ['positional_embedding_12[0][0]',\n rmerDecoder)                                                     'transformer_encoder_5[0][0]']  \n                                                                                                  \n dropout_3 (Dropout)            (None, None, 256)    0           ['transformer_decoder_4[0][0]']  \n                                                                                                  \n dense_24 (Dense)               (None, None, 15000)  3855000     ['dropout_3[0][0]']              \n                                                                                                  \n==================================================================================================\nTotal params: 19,960,216\nTrainable params: 19,960,216\nNon-trainable params: 0\n__________________________________________________________________________________________________\n","output_type":"stream"}]},{"cell_type":"code","source":"import numpy as np\n\nspa_voca = target_vectorization.get_vocabulary()\nspa_index_lookup = dict(zip(range(len(spa_voca)), spa_voca))\nmax_decoded_sentence_length = 20\n\ndef decode_sequence(input_sentence):\n\ttokenized_input_sentence = source_vectorization([input_sentence])\n\tdecoded_sentence = \"[start]\"\n\tfor i in range(max_decoded_sentence_length):\n\t\ttokenized_target_sentence = target_vectorization(\n\t\t[decoded_sentence])[:, :-1]\n\t\tpredictions = transformer([tokenized_input_sentence, tokenized_target_sentence])\n\t\tsampled_token_index = np.argmax(predictions[0, i, :])\n\n\t\t# 다음 토큰 예측 문자열로 바꾸고 생성된 문장에 추가\n\t\tsampled_token = spa_index_lookup[sampled_token_index]\n\t\tdecoded_sentence += \" \" + sampled_token\n\t\tif sampled_token == \"[end]\":\n\t\t\tbreak\n\treturn decoded_sentence\n\ntest_eng_texts = [pair[0] for pair in test_pairs]\nfor _ in range(20):\n\tinput_sentence = random.choice(test_eng_texts)\n\tprint(\"-\")\n\tprint(input_sentence)\n\tprint(decode_sequence(input_sentence))","metadata":{"execution":{"iopub.status.busy":"2023-07-26T05:05:28.581977Z","iopub.execute_input":"2023-07-26T05:05:28.582323Z","iopub.status.idle":"2023-07-26T05:05:37.954453Z","shell.execute_reply.started":"2023-07-26T05:05:28.582291Z","shell.execute_reply":"2023-07-26T05:05:37.953328Z"},"trusted":true},"execution_count":52,"outputs":[{"name":"stdout","text":"-\nWe saw another ship far ahead.\n[start] vi a otra alta le lejos [end]\n-\nWho do you want to speak to?\n[start] a quién quieres hablar [end]\n-\nWhy does everybody love cats?\n[start] por qué todos ellos pueden los gatos [end]\n-\nI don't even want to be here now.\n[start] no quiero ni siquiera aquí ahora [end]\n-\nHe writes Arabic.\n[start] Él escribe escribe [end]\n-\nI won't forget what you did.\n[start] no te [UNK] lo que te [UNK] [end]\n-\nThe trouble with him is that he is lazy.\n[start] la problemas con él es que él es el ruido [end]\n-\nThat scandal cost him his reputation.\n[start] esa oportunidad le costó su cuenta [end]\n-\nDo you mind if I smoke here?\n[start] te importa si aquí me gusta aquí [end]\n-\nTom enlisted in the Army.\n[start] tom se [UNK] en el ejército [end]\n-\nA drunk man fell down the stairs.\n[start] un [UNK] [UNK] se cayó de las escaleras [end]\n-\nTom doesn't feel like studying.\n[start] a tom no tiene ganas de estudiar [end]\n-\nPhysics is my favorite subject.\n[start] la solución es mi deporte favorito [end]\n-\nMay I have a road map, please?\n[start] puedo favor puedo dar un favor de su camino [end]\n-\nIt was out of his reach.\n[start] se fue fuera de su casa [end]\n-\nI owe him a debt.\n[start] le debo una decisión [end]\n-\nAs long as you do what you've been told, we'll have absolutely no problem.\n[start] como el largo no hagas lo que te has dicho nada bien sin problema [end]\n-\nTom is closing the store.\n[start] tom está [UNK] la tienda [end]\n-\nI miss the hustle and bustle of city life.\n[start] me [UNK] la ciudad y [UNK] de la ciudad [end]\n-\nI'm awake now.\n[start] ahora estoy libre [end]\n","output_type":"stream"}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}