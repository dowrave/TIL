{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "220509_Neural Style Transfer.ipynb",
      "private_outputs": true,
      "provenance": [],
      "authorship_tag": "ABX9TyP03N+PpRSd7UcLFSpVHrFt",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dowrave/RoadToImageSeg_GAN/blob/main/220509_Neural_Style_Transfer.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "[텐서플로우 튜토리얼 : 신경 스타일 전이](https://www.tensorflow.org/tutorials/generative/style_transfer?hl=ko)"
      ],
      "metadata": {
        "id": "rFbG56JnoWec"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2015년 논문을 따르는 방식\n",
        "- 이 알고리즘은 이미지 콘텐츠를 특정 스타일로 최적화시키는 방식이다.\n",
        "- 콘텐츠 이미지 + 스타일 참조 이미지 -> 콘텐츠 유지 & 스타일 참조 이미지의 화풍으로 채색한 느낌의 최적화 기술\n",
        "- 최근의 CycleGan 등은 모델이 변이된 이미지를 직접 생성하게 만듦 \n",
        "  - 생성 알고리즘이 스타일 전이 알고리즘보다 1000배 가까이 빠름\n"
      ],
      "metadata": {
        "id": "0TyaaiOv8PZu"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pmYFNrNA8ENH"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "import IPython.display as display\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib as mpl\n",
        "mpl.rcParams['figure.figsize'] = (12, 12)\n",
        "mpl.rcParams['axes.grid'] = False\n",
        "\n",
        "import numpy as np\n",
        "import PIL.Image\n",
        "import time\n",
        "import functools"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def tensor_to_image(tensor):\n",
        "  tensor = tensor * 255\n",
        "  tensor = np.array(tensor, dtype = np.uint8)\n",
        "  if np.ndim(tensor) > 3:\n",
        "    assert tensor.shape[0] == 1\n",
        "    tensor = tensor[0]\n",
        "  return PIL.Image.fromarray(tensor)"
      ],
      "metadata": {
        "id": "K5_QATwL81-c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 이미지 다운\n",
        "- 스타일, 컨텐츠 별도"
      ],
      "metadata": {
        "id": "7JjpA0Ds9MwH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 둘 다 1장의 이미지네?\n",
        "content_path = tf.keras.utils.get_file('YellowLabradorLooking_new.jpg', \n",
        "                                       'https://storage.googleapis.com/download.tensorflow.org/example_images/YellowLabradorLooking_new.jpg')\n",
        "style_path = tf.keras.utils.get_file('kandinsky5.jpg', \n",
        "                                     'https://storage.googleapis.com/download.tensorflow.org/example_images/Vassily_Kandinsky%2C_1913_-_Composition_7.jpg')"
      ],
      "metadata": {
        "id": "aWOzUApe9G6g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 입력 시각화\n",
        "- 이미지 불러오는 함수 정의 & 최대 이미지 크기 512개 픽셀"
      ],
      "metadata": {
        "id": "YcfsALed9riG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# img = tf.random.normal([60, 50, 100, 120], dtype = tf.float32)\n",
        "# # print(img.shape)\n",
        "# # tf.shape(img[:-1])"
      ],
      "metadata": {
        "id": "cJFXXY81-fUy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# print(tf.shape(img)[:-1])\n",
        "# shape = tf.cast(tf.shape(img)[:-1], tf.float32)\n",
        "# max(shape)"
      ],
      "metadata": {
        "id": "0HOdFcWC_Apq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def load_img(path_to_img):\n",
        "  max_dim = 512 # 최대 이미지 크기 : 512픽셀\n",
        "\n",
        "  # 파일 읽기 -> 채널 3개로 디코딩하기(RGB?) -> 데이터 타입 변환하기\n",
        "  img = tf.io.read_file(path_to_img)\n",
        "  img = tf.image.decode_image(img, channels = 3)\n",
        "  img = tf.image.convert_image_dtype(img, tf.float32)\n",
        "\n",
        " # tf.cast(a, b) : a에 b함수를 가함\n",
        "  shape = tf.cast(tf.shape(img)[:-1], # tf.shape(img)[:-1] : shape로 갖는 (a,b,c,..z)가 있다면 z 뺀 나머지 값들임\n",
        "                           tf.float32)\n",
        "  long_dim = max(shape) # 그 중 최댓값 : 보통은 샘플 수가 제일 많기 때문에 max함수를 지정함\n",
        "  scale = max_dim / long_dim # 512 / 이미지 가로 세로 중 넓은 거\n",
        "\n",
        "  new_shape = tf.cast(shape * scale, tf.int32) \n",
        "\n",
        "  img = tf.image.resize(img, new_shape) \n",
        "  img = img[tf.newaxis, :]\n",
        "  return img"
      ],
      "metadata": {
        "id": "_FsLP0iQ9ivU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # # 함수 뜯어보기\n",
        "# max_dim = 512\n",
        "# img = tf.io.read_file(content_path) # 이 자체로는 사용할 수 없음(b'\\xff\\xd8\\xff\\xe0\\x00\\x10JFIF\\x00\\x01\\x01\\x01\\x00`\\x00`...)\n",
        "# img = tf.image.decode_image(img, channels = 3) # decode는 각 픽셀의 RGB값(채널3)을 드러냄\n",
        "# img = tf.image.convert_image_dtype(img, tf.float32)\n",
        "# print(img.shape) # (577, 700, 3) : 577, 700은 이미지 해상도?\n",
        "\n",
        "# shape = tf.cast(tf.shape(img)[:-1], tf.float32) # (577, 700)\n",
        "# long_dim = max(shape) # 700\n",
        "# scale = max_dim / long_dim # 이거 왜 나누는거임? : 0.7314\n",
        "\n",
        "# new_shape = tf.cast(shape * scale , tf.int32)\n",
        "# new_shape # [422, 512]\n",
        "\n",
        "# img = tf.image.resize(img, new_shape)\n",
        "# # print(img.shape)\n",
        "# img = img[tf.newaxis, :] # 새로운 축을 위치에 따라 추가함. \n",
        "# # print(img.shape) # (1, 422, 512, 3)\n",
        "# print(img)\n",
        "# tf.squeeze(img, axis = 0)"
      ],
      "metadata": {
        "id": "rqljCc8Srpv0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 이미지 출력 함수 정의\n",
        "def imshow(image, title = None):\n",
        "  if len(image.shape) > 3:\n",
        "    image = tf.squeeze(image, axis = 0) # squeeze : 크기 1인 차원을 제거(tf.newaxis로 추가된 차원을 없앰) / axis = 0 : 0번째 축에 대해서만 squeeze 실행\n",
        "  plt.imshow(image)\n",
        "  if title:\n",
        "    plt.title(title)"
      ],
      "metadata": {
        "id": "KnshqjLDCYqr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "content_image = load_img(content_path)\n",
        "style_image = load_img(style_path)\n",
        "\n",
        "plt.subplot(1, 2, 1)\n",
        "imshow(content_image, 'Content Image')\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "imshow(style_image, 'Style Image')"
      ],
      "metadata": {
        "id": "Wsf7vIcsCg8s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 콘텐츠, 스타일 표현 정의하기\n",
        "- CNN의 입력층 쪽은 저차원적 특성, 깊어질수록 고차원적 특성임\n",
        "- 여기선 사전학습된 이미지 분류 네트워크 VGG19를 사용함\n",
        "- 중간층은 이미지에서 컨텐츠, 스타일 표현 정의에 필요함\n",
        "- 스타일 전이 알고리즘은 입력 이미지에 대해 중간층에서 콘텐츠 - 스타일에 해당하는 타깃 표현들을 일치시키려 할 것"
      ],
      "metadata": {
        "id": "jMypvK-GCzMX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x = tf.keras.applications.vgg19.preprocess_input(content_image * 255)\n",
        "x = tf.image.resize(x, (224, 224))\n",
        "vgg = tf.keras.applications.VGG19(include_top = True, weights = 'imagenet')\n",
        "predictions_probabilities = vgg(x)\n",
        "predictions_probabilities.shape"
      ],
      "metadata": {
        "id": "JWQ9mdhcCq_S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "predicted_top_5 = tf.keras.applications.vgg19.decode_predictions(predictions_probabilities.numpy())[0]\n",
        "[(class_name, prob) for (number, class_name, prob) in predicted_top_5]"
      ],
      "metadata": {
        "id": "Nu1MslUeDcYR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vgg = tf.keras.applications.VGG19(include_top = False, weights = 'imagenet')\n",
        "\n",
        "print()\n",
        "for layer in vgg.layers:\n",
        "  print(layer.name)"
      ],
      "metadata": {
        "id": "OQKYSMmvDptj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "content_layers = ['block5_conv2']\n",
        "\n",
        "style_layers = [\n",
        "                'block1_conv1',\n",
        "                'block2_conv1',\n",
        "                'block3_conv1',\n",
        "                'block4_conv1',\n",
        "                'block5_conv1',\n",
        "                ]\n",
        "                \n",
        "num_content_layers = len(content_layers)\n",
        "num_style_layers = len(style_layers)"
      ],
      "metadata": {
        "id": "j1Oq0oL8D4s5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 중간 출력으로 스타일, 컨텐츠 표현 정의하기\n",
        "- 고수준의 이미지 분류 : 네트워크의 이미지 이해가 필수\n",
        "  - 이는 미가공 이미지를 특성에 대한 복합적인 이해로 변환하는 내부 표현을 만드는 작업이 들어감\n",
        "  - 또한, 배경잡음 & 기타 잡음에 관계 없이 불변성, 특징 포착 가능\n",
        "- 따라서 중간층은 복합 특성 추출기의 역할을 수행한다"
      ],
      "metadata": {
        "id": "vYI_Y0SoEVv1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 함수형 API로 중간층에 접근하기"
      ],
      "metadata": {
        "id": "O-AvPkqFEpGl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def vgg_layers(layer_names):\n",
        "  \"\"\"중간층의 출력값 -> 배열로 반환하는 vgg 모델 만들기\"\"\"\n",
        "  vgg = tf.keras.applications.VGG19(include_top = False, weights = 'imagenet')\n",
        "  vgg.trainable = False\n",
        "\n",
        "  outputs = [vgg.get_layer(name).output for name in layer_names]\n",
        "\n",
        "  model = tf.keras.Model([vgg.input], outputs) # input에 []를 씌운 건 2차원이 필요해서?\n",
        "  return model"
      ],
      "metadata": {
        "id": "dDKnZ3tNEHOP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 모델 생성\n",
        "style_extractor = vgg_layers(style_layers) # vgg의 층의 이름을 가진 모델 생성\n",
        "style_outputs = style_extractor(style_image * 255) # 모델에 input 데이터를 통과시킴\n",
        "\n",
        "# 각 층의 출력에 대한 통계량\n",
        "for name, output in zip(style_layers, style_outputs):\n",
        "  print(name)\n",
        "  print(\"크기 : \", output.numpy().shape)\n",
        "  print(\"최솟값 : \", output.numpy().min())\n",
        "  print(\"최댓값 : \", output.numpy().max())\n",
        "  print(\"평균 : \", output.numpy().mean())"
      ],
      "metadata": {
        "id": "UrEKgoVSE4rc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 스타일 계산하기\n",
        "- 콘텐츠는 중간 층의 feature map의 값들로 표시된다\n",
        "- 스타일은 각 특성맵의 평균 & 피쳐맵들 사이의 상관관계로 표시되며, <b>그람 행렬</b>은 이를 담고 있음. 자세한 건 해당 튜토리얼 참조\n",
        "- `tf.linalg.einsum`으로 그람 함수 계산 가능"
      ],
      "metadata": {
        "id": "VHpDYUaZFQj8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def gram_matrix(input_tensor):\n",
        "  result = tf.linalg.einsum('bijc, bijd->bcd', input_tensor, input_tensor)\n",
        "  input_shape = tf.shape(input_tensor)\n",
        "  num_locations = tf.cast(input_shape[1] * input_shape[2], tf.float32)\n",
        "  return result/(num_locations)"
      ],
      "metadata": {
        "id": "F26C9RtrFMgB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 스타일, 컨텐츠 추출하기"
      ],
      "metadata": {
        "id": "4G2MQsoaFw_h"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class StyleContentModel(tf.keras.models.Model):\n",
        "  def __init__(self, style_layers, content_layers):\n",
        "    super(StyleContentModel, self).__init__()\n",
        "    self.vgg = vgg_layers(style_layers + content_layers)\n",
        "    self.style_layers = style_layers\n",
        "    self.content_layers = content_layers\n",
        "    self.num_style_layers = len(style_layers)\n",
        "    self.vgg.trainable = False\n",
        "\n",
        "  def call(self, inputs):\n",
        "    \"\"\"input : 0과 1 사이\"\"\"\n",
        "    inputs = inputs * 255.0\n",
        "    preprocessed_input = tf.keras.applications.vgg19.preprocess_input(inputs)\n",
        "    outputs = self.vgg(preprocessed_input)\n",
        "    style_outputs, content_outputs = (outputs[:self.num_style_layers],\n",
        "                                      outputs[self.num_style_layers:])\n",
        "    \n",
        "    style_outputs = [gram_matrix(style_output) for style_output in style_outputs]\n",
        "\n",
        "    content_dict = {content_name : value for content_name, value in zip(self.content_layers, content_outputs)}\n",
        "\n",
        "    style_dict = {style_name : value for style_name , value in zip(self.style_layers, style_outputs)}\n",
        "\n",
        "    return {\"content\" : content_dict, 'style' : style_dict}\n",
        "    "
      ],
      "metadata": {
        "id": "03D6TUPWFdz8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "extractor = StyleContentModel(style_layers, content_layers)\n",
        "\n",
        "results = extractor(tf.constant(content_image))\n",
        "print('스타일 : ')\n",
        "for name, output in sorted(results['style'].items()):\n",
        "  print(\" \", name)\n",
        "  print(\"크기 : \", output.numpy().shape)\n",
        "  print(\"최솟값 : \", output.numpy().min())\n",
        "  print(\"최댓값 : \", output.numpy().max())\n",
        "  print(\"평균 : \", output.numpy().mean())\n",
        "print('-'*40)\n",
        "print('콘텐츠 : ')\n",
        "for name, output in sorted(results['content'].items()):\n",
        "  print(\" \", name)\n",
        "  print(\"크기 : \", output.numpy().shape)\n",
        "  print(\"최솟값 : \", output.numpy().min())\n",
        "  print(\"최댓값 : \", output.numpy().max())\n",
        "  print(\"평균 : \", output.numpy().mean())"
      ],
      "metadata": {
        "id": "vkpEJp3sGrbb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 경사하강법\n",
        "- 입력 이미지의 평균제곱오차 계산 후 가중합 구함"
      ],
      "metadata": {
        "id": "ScPy7uRRHLym"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "style_targets = extractor(style_image)['style']\n",
        "content_targets = extractor(content_image)['content']\n"
      ],
      "metadata": {
        "id": "V0ZwyhrvG-p5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "image = tf.Variable(content_image)"
      ],
      "metadata": {
        "id": "Uu-ElwJVHVhe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 픽셀 값 0과 1로 클리핑\n",
        "def clip_0_1(image):\n",
        "  return tf.clip_by_value(image, clip_value_min = 0.0, clip_value_max = 1.0)"
      ],
      "metadata": {
        "id": "n4TgApiEHYJ-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 옵티마이저 : Adam도 충분히 적합하다\n",
        "opt = tf.optimizers.Adam(learning_rate = 0.02, beta_1 = 0.99, epsilon = 1e-1)\n"
      ],
      "metadata": {
        "id": "-2_nlPnCHduH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 최적화 진행 : 전체 오차는 컨텐츠와 스타일 오차의 가중 합\n",
        "style_weight = 1e-2\n",
        "content_weight = 1e4"
      ],
      "metadata": {
        "id": "OLOa1N1ZHjoM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def style_content_loss(outputs):\n",
        "  style_outputs = outputs['style']\n",
        "  content_outputs = outputs['content']\n",
        "\n",
        "  style_loss = tf.add_n([tf.reduce_mean((style_outputs[name] - style_targets[name]) ** 2) for name in style_outputs.keys()])\n",
        "  style_loss *= style_weight / num_style_layers\n",
        "\n",
        "  content_loss = tf.add_n([tf.reduce_mean((content_outputs[name] - content_targets[name]) ** 2) for name in content_outputs.keys()])\n",
        "  content_loss *= content_weight / num_content_layers\n",
        "\n",
        "  loss = style_loss + content_loss\n",
        "  return loss"
      ],
      "metadata": {
        "id": "ZnL3Kfr0HoSV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# tf.GradientTape로 이미지 업데이트\n",
        "@tf.function\n",
        "def train_step(image):\n",
        "  with tf.GradientTape() as tape:\n",
        "    outputs = extractor(image)\n",
        "    loss = style_content_loss(outputs)\n",
        "\n",
        "  grad = tape.gradient(loss, image)\n",
        "  opt.apply_gradients([(grad, image)])\n",
        "  image.assign(clip_0_1(image))"
      ],
      "metadata": {
        "id": "NhodmbB1H-CJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_step(image)\n",
        "train_step(image)\n",
        "train_step(image)"
      ],
      "metadata": {
        "id": "cTQwFb-cIMk_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # tensor_to_image 분석\n",
        "\n",
        "# # print(image.shape) # image : 0 ~ 1 사이의 값을 가진 3차원 텐서\n",
        "# image = image * 255\n",
        "# image = np.array(image, dtype = np.uint8)\n",
        "# # print(image)\n",
        "# # print('-'*40)\n",
        "# if np.ndim(image) > 3: \n",
        "#   assert image.shape[0] == 1 # 이미지의 차원이 3을 넘으면 0번째 차원의 값은 1일 것이며\n",
        "#   image = image[0] # 이건 나머지 3개 차원만 보겠다는 말과 동일함\n",
        "# # image2 = image[tf.newaxis, :]\n",
        "# # print(image2[0])\n",
        "\n",
        "# print(image[0]) \n",
        "# print(image.shape)\n",
        "# PIL.Image.fromarray(image) # 배열 -> 이미지로"
      ],
      "metadata": {
        "id": "GPr-ucfX2zNy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "start = time.time()\n",
        "epochs = 10\n",
        "steps_per_epoch = 100\n",
        "\n",
        "step = 0\n",
        "for n in range(epochs):\n",
        "  for m in range(steps_per_epoch):\n",
        "    step += 1\n",
        "    train_step(image)\n",
        "    print(\".\", end = '')\n",
        "  display.clear_output(wait = True)\n",
        "  display.display(tensor_to_image(image))\n",
        "  print(\"훈련 스텝 {}\".format(step))\n",
        "\n",
        "end = time.time()\n",
        "print(end - start)"
      ],
      "metadata": {
        "id": "RV33XK1qIPvD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 총 변위 손실\n",
        "- 이 방식의 단점 : 고주파 아티팩트가 다수 존재함\n",
        "- 이를 위해 규제 항을 추가한다. 변형된 오차 값을 총 변위 손실(Total Variation Loss)이라고 함"
      ],
      "metadata": {
        "id": "P9URiUE6JKv1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def high_pass_x_y(image):\n",
        "  x_var = image[:, :, 1:, :] - image[:, :, :-1, :]\n",
        "  y_var = image[:, :, 1:, :] - image[:, :, :-1, :]\n",
        "\n",
        "  return x_var, y_var"
      ],
      "metadata": {
        "id": "-cwKbNpgJKG5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x_deltas, y_deltas = high_pass_x_y(content_image)\n",
        "\n",
        "plt.figure(figsize=(14, 10))\n",
        "\n",
        "plt.subplot(2, 2, 1)\n",
        "imshow(clip_0_1(2*y_deltas+0.5), \"Horizontal Deltas : Original\")\n",
        "\n",
        "plt.subplot(2, 2, 2)\n",
        "imshow(clip_0_1(2*x_deltas+0.5), \"Vertical Deltas : Original\")\n",
        "\n",
        "x_deltas, y_deltas = high_pass_x_y(image)\n",
        "\n",
        "plt.subplot(2, 2, 3)\n",
        "imshow(clip_0_1(2*y_deltas + 0.5), \"Horizontal Deltas : Styled\")\n",
        "\n",
        "plt.subplot(2, 2, 4)\n",
        "imshow(clip_0_1(2*x_deltas + 0.5), \"Vertical Deltas : Styled\")\n"
      ],
      "metadata": {
        "id": "A-yvLaSYJKKI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 고주파 요소는 경계선 탐지기의 일종이다\n",
        "- sobel dege detector를 사용하면 유사한 출력을 얻을 수 있음"
      ],
      "metadata": {
        "id": "3gwK3MqxJ3IA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(14, 10))\n",
        "\n",
        "sobel = tf.image.sobel_edges(content_image)\n",
        "plt.subplot(1, 2, 1)\n",
        "imshow(clip_0_1(sobel[...,0]/4+0.5), \"Horizontal Sobel - Edges\")\n",
        "plt.subplot(1, 2, 2)\n",
        "imshow(clip_0_1(sobel[...,1]/4+0.5), \"Vertical Sobel - Edges\")"
      ],
      "metadata": {
        "id": "lQesJu8HJKMQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 정규화 오차 : 각 값의 절댓값의 합\n",
        "def total_variation_loss(image):\n",
        "  x_deltas, y_deltas = high_pass_x_y(image)\n",
        "  return tf.reduce_sum(tf.abs(x_deltas)) + tf.reduce_sum(tf.abs(y_deltas))\n",
        "  "
      ],
      "metadata": {
        "id": "Y4t8HHmsKKhG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "total_variation_loss(image).numpy()"
      ],
      "metadata": {
        "id": "2iH_Lb37KVSp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 정보 : 텐서플로우에 이미 있음\n",
        "tf.image.total_variation(image).numpy()"
      ],
      "metadata": {
        "id": "zdkvt7ubKXZP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 다시 최적화 하기"
      ],
      "metadata": {
        "id": "MOabBBpdKgKo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "total_variation_weight = 30"
      ],
      "metadata": {
        "id": "8R237TasKf88"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "@tf.function\n",
        "def train_step(image):\n",
        "  with tf.GradientTape() as tape:\n",
        "    outputs = extractor(image)\n",
        "    loss = style_content_loss(outputs)\n",
        "    loss += total_variation_weight * tf.image.total_variation(image)\n",
        "\n",
        "  grad = tape.gradient(loss, image)\n",
        "  opt.apply_gradients([(grad, image)])\n",
        "  image.assign(clip_0_1(image))"
      ],
      "metadata": {
        "id": "vIL1YMhnKcaq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 최적화할 변수를 다시 초기화\n",
        "image = tf.Variable(content_image)"
      ],
      "metadata": {
        "id": "Qt7rpVroKrB5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 함수 재실행\n",
        "import time\n",
        "start = time.time()\n",
        "epochs = 10\n",
        "steps_per_epoch = 100\n",
        "\n",
        "step = 0\n",
        "for n in range(epochs):\n",
        "  for m in range(steps_per_epoch):\n",
        "    step += 1\n",
        "    train_step(image)\n",
        "    print(\".\", end = '')\n",
        "  display.clear_output(wait = True)\n",
        "  display.display(tensor_to_image(image))\n",
        "  print(\"훈련 스텝 {}\".format(step))\n",
        "\n",
        "end = time.time()\n",
        "print(end - start)"
      ],
      "metadata": {
        "id": "3jrF0u-YKwPT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 결과 저장\n",
        "file_name = 'stylized-image.png'\n",
        "tensor_to_image(image).save(file_name)\n",
        "\n",
        "try : \n",
        "  from google.colab import files \n",
        "except ImportError:\n",
        "  pass\n",
        "else:\n",
        "  files.download(file_name)"
      ],
      "metadata": {
        "id": "6epILJqzKzVz"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}