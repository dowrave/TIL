#코테

# 기본
1. 2부터 구하고자 하는 수까지 모두 나열
2. **2부터 숫자를 올리며 자신의 배수는 제거**
3. 이를 반복하면 구간의 모든 소수가 남게 됨
>  2번에서 올라가는 숫자는 구하고자 하는 수까지 가지 않아도 된다. **`구하고자 하는 수의 제곱근`까지만 가도 됨.**

# 구현
- 파이썬 코드 구현하기
```python
# 정수 n까지의 소수 리스트 얻기
def sieve_of_eratosthenes(n):
    prime = [True] * (n + 1)
    prime[0] = False
    prime[1] = False

    for p in range(2, int(n**0.5) + 1):
        if prime[p]:
            for i in range(p * p, n + 1, p):
                prime[i] = False

    return [p for p in range(2, n + 1) if prime[p]]

print(sieve_of_eratosthenes(n))
```
> 1. 외부 반복문의 상한이 `n`이 아니라 `sqrt(n)`인 이유
> -  합성수(1과 자기 자신을 제외한 약수가 있는 수)는 반드시 `a * b`로 쪼갤 수 있다.
> - 이 때, `a, b` 중 한 값은 반드시 `sqrt(n)` 이하이다. 그렇지 않다면 `a*b` 모두 `sqrt(n)`보다 크다는 의미가 되어 `a * b > n`이라 모순이 발생한다.
> - 따라서, **`sqrt(n)`까지의 수만 확인해도, `sqrt(n)`보다 큰 합성수는 이미 이전 소수의 배수에서 제거되었기 때문에 생각하지 않아도 무방하다**는 것이다.

> 2. 내부 반복문에서 `p*p`에서 시작하는 이유) 
> - **`p^2`보다 작은 `p`의 배수들은 이미 이전 단계에서 처리됐기 때문이다.** 
> - 예를 들어 `p = 5`라고 한다면, `p`의 배수인 `10, 15, 20`은 이미 `2`의 배수(`10, 20`)와 `3`의 배수`15`에서 처리됐다는 의미이다.
> - 따라서 `p^2`에서 시작할 수 있게 된다.

- 참고) 내부 반복문의 테크닉까지 사용하는 경우 시간복잡도는 $O(nlog(log(n)))$ 이되고, 이를 사용하지 않고 `2p`에서 시작하는 경우 시간복잡도는 $O(nlog(n))$ 이 된다.

## 추가) 시간복잡도 계산
- 개인적으로 **시간복잡도가 어떻게 저렇게 딱 나오는지 이해가 안돼서, Claude님한테 물어봄**
- 특히 `p^2`은 어려우니까 그냥 넘겨도 무방하다. 더 효율적이라는 것만 생각해도 되겠음.

### 2p 에서 시작
- 2의 배수를 가정하면, `4,6, , ..., n`까지 있다. 따라서 `n/2`번 반복된다.
- 3도 마찬가지일 거다. 이를 일반화하면 모든 소수에 대해 `n/p`번 반복된다.

- 일반적으로 소수의 개수는 `n/ln(n)`개로 알려져 있다(소수 정리). <-- 몰라도 됨
- 그러나 간단하게, `2`부터 `sqrt(n)`까지의 모든 소수를 고려해서 계산해보자
- 그러면 아래처럼 된다.
```
n * (1/2 + 1/3 + 1/5 + ... + 1/sqrt(n))
```
> 위 식은 조화급수 `H(n) = (1 + 1/2 + 1/3 + 1/4 + ...)`와 유사하다. **조화급수의 값은 `log n + gamma`의 값과 근사적으로 같다고 알려져 있다** (gamma를 몰라도 된다. log n만 보자.)

- 그런 유사성으로 접근했을 때, 위에서 얻은 식은 `n * H(sqrt(n))`이 된다. 따라서 `n * 1/2 * log n`이 되고, Big O로 쓰면 $O(n(log(n)))$이 되는 것이다.

### 2. p^2 에서 시작
- 2^2에서 시작하는 2의 배수는 4, 6, 8, 10, ..., n까지 있다.
- 3^2에서 시작하는 3의 배수는 9, 12, 15, ..., n 까지 있다.
- 일반적으로 따지면 **n부터 p^2 까지의 모든 수를 p으로 나눈 게 배수의 갯수가 될 거다**
- 수식으로 치면 $\frac{(n-p^2)}{p} = \frac{n}{p} - p = \frac{n}{p^2} - 1$이 되는 것.

- 이를 모든 소수에 대해 계산하면, 위와 동일하게 `sqrt(n)`까지 계산, 
```
n * (1/2^2 + 1/3^2 + 1/5^2 + ... + + 1/(sqrt(n))^2) - (sqrt(n) - 1)
```
> 여기서 n과 곱해진 값의 합은 `basel` 문제로 알려져 있다. 1부터 시작해서 `1/n^2`의 값을 모두 더한 값이 $\frac{\pi^{2}}{6}$으로 알려져 있다고 하는데, 이 경우는 보다 더 작을 것이다. 중요한 건 저 값을 상수 C로 근사할 수 있다는 점이다. 따라서 위 식은

```
n * C - (sqrt(n) - 1)
```
이 된다. 이는 상한값이며, 실제 시간 복잡도는 더 파고 들면 $O(log(log(n)))$ 이라고 함.
