## 현재 이슈
- `polynomialWarmup`까지 적용한 뒤, 모델을 학습시키고 있다. 
- 특정 에포크까지는 `훈련` 손실함수가 감소하고, 정확도가 증가하는 현상을 보이다가, 갑자기 어떤 에포크부터 손실함수가 증가하고 정확도가 감소하며, 결국 `0.7836`이라는 값에 수렴하는 현상이 발생함.


## 일자별 진행상황

### 230927

- `마침내 학습다운 학습이 되고 있습니다!!!!!! (스텝마다 accuracy 값이 올라감)` 라고 생각했는데 에포크가 넘어가면 결국 `0.7836` 으로 수렴하는 현상이 있다.  훈련 데이터에 과적합되는 것도 아닌 이상한 상황이라서 5에포크 단위로 **여러 테스트**를 해보는 중.
```
1. 콜백 함수 제거 -> 상관 없음. 콜백이 원인은 아니다.
2. warmup 스텝 적용 : 효과 있음. 0.7836으로 수렴하는 이슈도 개선되었음.
	- 대신 warmup step이 끝났을 때의 학습률에 도달하지 않고, 그 직전의 학습률에서 warmup이 끝나는 현상이 있다
		- init_lr이 5e-5면 4.5e-5
		- 1e-4면 9e-5에서 학습률이 내려가기 시작.
```

- 그리고 지금 **Warmup Step도 구현**되지 않은 상황이다. 특정 스텝까지는 학습률이 올라가다가, `warmup step`이 지나면 아래의 `lr_schedule`이 적용되면 된다.
	- 원래 코드의 구현 내용 
		1. `warmup step`과 현재 `step` 의 비율을 구한다
		2. 이를 초기 학습률과 곱한다
		3. 그 결과 `warmup_step` 까지는 학습률이 선형적으로 증가하다가, `warmup_step` 이 지나면 아래의 `lr_schedule`이 적용된다.
	- `tensorflow-model` 라이브러리에 `polynomialWarmup`이 있다. 
```python
!pip install tf-models-official # 공식에는 tensorflow-models-official 이라고 되어 있는데, 그렇게 하면 설치 안됨. (PyPI 참고)

import tensorflow_models as tfm
```

### 230926
-  230926 : 아예 AdamW에 학습률 스케쥴을 넣는 방식으로 구현해봄 : 콜백 함수를 넣으면 학습에 방해가 되는 문제가 있어서 이렇게 구현함
```python
lr_schedule = tf.keras.optimizers.schedules.PolynomialDecay(
    learning_rate,
    train_total_steps,
    end_learning_rate = 0.0,
    power = 1.0,
    cycle = False
)

optimizer = tf.keras.optimizers.AdamW(
    learning_rate = lr_schedule,
    weight_decay = 0.01,
    epsilon = 1e-6,
    clipnorm = 1.0
)
```
- 모델의 마지막 출력층
	- 유닛 수 : 2개 -> 1개
	- 활성화함수 : `linear` -> `sigmoid`

- 마침내 학습다운 학습이 되고 있습니다!!!!!! (스텝마다 accuracy 값이 올라감)
- 일단 50 에포크 정도로 늘린 다음 과대적합 되는 지점을 찾기로 한다.
### 230925
- `val_accuracy` 성능에 변동이 생기지 않는 이유 추적 시작
	1. 일단 기존의 `create_optimizer`를 **`AdamW`로 바꾸니까 에포크마다 검증 값에 변화가 있었음**
	2. 추가로 구현하는 중 : `warmup step` 과정
		- 구현은 했는데, `accuracy`가 갑자기 28%대로 뚝 떨어졌다. ???
		- 정상적으로 `learning_rate`가 추적되지 않는 듯. 전부 0으로 뜬다.
		- `tf.keras.backend.get_value(self.model.optimizer.learning_rate)` 로 모델의 `learning_rate`를 가져올 수 있다. `optimizer.lr`로는 안되는 듯.

- 학습 속도 느려지는 현상.
```
WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0272s vs `on_train_batch_end` time: 8.0370s). Check your callbacks.
```
- 스택 오버플로우에서 찾은 해결법으로는
	1. 배치 크기를 늘린다
	2. `model.fit(use_multiprocessing = True)`로 한다. (다른 문제 발생 가능)


### 230922
```
`ValueError: Target data is missing. Your model was compiled with loss=<keras.src.losses.SparseCategoricalCrossentropy object at 0x7a8ec3d5e500>, and therefore expects target data to be provided in `fit()`.` 
```
- 데이터 셋에 아래의 수정을 가함
```python
def split_data_and_label(dataset):
  features = {
      'input_ids' : dataset['input_ids'],
      'input_mask' : dataset['input_mask'],
      'is_real_example' : dataset['is_real_example'],
      'segment_ids' : dataset['segment_ids']
  }
  target = dataset['label_ids']
  return features, target

train_ds = train_ds.map(lambda x : split_data_and_label(x))
```

- 케라스로 훈련 시작했다 : 일단 5에포크 정도 테스트하고, 검증을 붙이고 등등.. 으로 적용할 예정.

### 230921
- `create_optimizer` 이슈 : `tf.GradientTape`와 `KerasTensor` 관련 이슈
	- `AttributeError: 'KerasTensor' object has no attribute '_id'`
- `create_model` 내에 손실함수를 정의하는 부분도 같이 들어가 있는데, 2.x 버전에서 손실함수는 따로 정의하는 게 일반적이므로 그렇게 작업을 진행하겠음.
- 아예 전부 keras 처럼 바꿔서 작업, fit까지 작성 완료.
### 230920

- `create_model`까지 생성 완료
	1. `tf.linalg.matmul`에서 계산이 정상적으로 되지 않는 문제 수정
	2. `tf.nn.bias_add` 도 계산이 안되는 문제가 있음
		- 위 문제들 모두 수정
		```python
		# tf v1에서 작성된 코드
		output_weights = tf.compat.v1.get_variable(
		  "output_weights", [num_labels, hidden_size],
		  initializer=tf.compat.v1.truncated_normal_initializer(stddev=0.02))
		
		output_bias = tf.compat.v1.get_variable(
		  "output_bias", [num_labels], initializer=tf.compat.v1.zeros_initializer())
		
		# 수정 코드
		output_weights = tf.Variable(
		  initial_value = tf.keras.initializers.TruncatedNormal(stddev = 0.02)(shape = (num_labels, hidden_size)),
		  trainable = True,
		  name = 'output_weights'
		)
		
		output_bias = tf.Variable(
		  initial_value = tf.keras.initializers.Zeros()(shape = (num_labels,)),
		  trainable = True,
		  name = 'output_bias'
		)
		```
		- 기존에 `tf.keras.layers.Dense()`로 2개의 변수를 수정했는데, 더 직접적인 방법으로 `tf.Variable`을 사용하는 것 같다.

- 새로운 이슈
```
loss = tf.reduce_mean(tf.Variable(per_example_loss, trainable = False))

You are passing KerasTensor(type_spec=TensorSpec(shape=(2,), dtype=tf.float32, name=None), name='tf.math.negative_2/Neg:0', description="created by layer 'tf.math.negative_2'"), an intermediate Keras symbolic input/output, to a TF API that does not allow registering custom dispatchers, such as `tf.cond`, `tf.function`, gradient tapes, or `tf.map_fn`
```

- 해결
```python
  # loss = tf.constant(loss) 
  loss = tf.keras.layers.Lambda(lambda x : x)(loss)
```
- loss 자체가 `KerasTensor`로 정의되어 있기 때문에, `tf.constant`로 직접 바꾸는 게 어려울 수 있으며, 이 경우 `tf.keras.layers`를 이용해 간접적으로 변환하는 방법이 있음.
### 230919
- `Loss` 정의하기 - 완료
- `create_model`을 텐서플로우 1 -> 2버전으로 바꾸는 과정에서 구조가 맞지 않는 것 같다. 어디가 문제인지 계속 확인해 봐야 함


### 230918
- 데이터 가공 작업 중 - 일단 krBERT에 있는 것들을 이용해서 가공을 다시 진행해보고, 막히는 곳이 있다면 그걸 수정해나가는 방식으로 작업을 진행하겠음.
- `tf.dataset` 전처리 과정 작업함 - 텐서플로우 버전 2에 맞게 코드 수정
### 230914
- 이슈에 있는 세부사항 - 학습률 값이 텐서에 들어가지 않는 현상 수정 시작
```
- `eval_results.txt`에서는 약 78% 정도의 정확도를 보이는데, 기대한 것보다 많이 낮은 느낌임. 
- 사실 텐서플로우 2.x 버전에서는 `model`을 쓰는 방법이 훨씬 좋아 보이는데, `krBERT`는 `estimator`를 이용하여 작성되어 있음. 이걸 다르게 가져갈 방법이 없을까 고민 중.

- 어떻게 해결했는지는 [[이슈 수정해나가기]]에 기록해두었다.
	1. 학습률`5e-5`을 적용했음에도 정상적으로 텐서에 들어가지 않는 현상
	2. `warmup_step`이 각 에포크마다 반복되지 않고, 특정 스텝 동안만 적용되게끔 함
```

### 230913
- 원본 코드는 `전체 에포크 * 한 에포크 당 훈련 스텝`을 `estimator`에 넣어서 계산하는데, 이를 에포크 수로 나눠서 넣어서 훈련함
	- 여기서 **`warmup_step` 관련 이슈가 있는데, 에포크 별로 반복할 때 `warmup_step`이 반복되는지 아닌지도 체크 필요함** (에포크 별로 반복되지 않아야 맞음)
- 학습에 따른 결과 수치화 결과 ) 에포크마다 Loss 값이 개선되지 않음

### 230912

- 하고 싶은 것 
1. 학습에 따른 결과를 시각화하고 싶음
2. 하이퍼파라미터(배치 크기, 최대 토큰 갯수)를 만지면서 미세조정을 만져보고 싶음


### 230911
- 세부사항 수정 시작
- 일단 `tf.compat.v1`으로 되어 있지 않은 부분들 수정하고, 다시 실행해서 확인함
	- 텐서플로우 자체에 `tf_upgrade_v2 --intree . --outtree ./krbert_v2 --copyotherfiles False` 라는 기능이 있다. 텐서플로우 1로 만든 코드를 2로 자동으로 업데이트하는 코드임. 직접 볼 곳이 있다면 `report.txt`에 자동으로 저장해준다.
### 230908

- **학습 시작했다!! (일단 파이프라인 자체는 완성한 셈)**
	- 훈련 데이터 학습은 1시간 조금 넘게 걸림
- 코드는 돌아가는 듯 한데, 결과를 시각화하는 방법을 찾아봐야 할 것 같다.
#### 이슈
- 같은 이슈 진행중 (GCS 버킷 접근 권한 설정 : `storage.buckets.list` 권한 부여하기)
	- [요 링크](https://colab.research.google.com/drive/1lYBYtaXqt9S733OXdXvrvC09ysKFN30W#scrollTo=aLjsqWr5upAz)의 가장 아랫쪽에서 TPU 에러 발생시 해당 버킷에 모든 유저 R/W 권한 부여가 필요하다고 하고 있다. 
	- GCS 콘솔에서 해당 버킷의 `공개 액세스 방지` 를 취소시킴
		- 이걸 눌러도 `공개 액세스`가 `공개 아님`으로 표시되는 건 동일한데, `allUsers`로부터의 접근 제한이 없어지는 차이점이 있는 듯. 이대로 실험해봄. 

- 아래 명령어로 해결한 듯? 오류 메시지가 달라짐 :
```
!gcloud storage buckets add-iam-policy-binding gs://steam-project-bucket --member=allUsers --role=roles/storage.objectViewer
```

- 다음 에러도 `storage.buckets.create` 관련 에러가 떴다.
- 관련 정보를 찾던 중, [클라우드 스토리지에 대한 IAM 역할](https://cloud.google.com/storage/docs/access-control/iam-roles?hl=ko)이 무슨 의미인가 궁금했는데 이제 이해를 좀 할 것 같음
	- `역할` 내에 `권한`이 포함되는 거임.
		- `역할`은 `objectViewer`, `objectCreator`, `objectUser` 등이 있는 거고
		- `권한`은 `storage.buckets.list`, `storage.object.get` 등이 들어감
	- 따라서, 위에서 `objectViewer`를 `allUsers`에게 권한을 줬는데, 다른 권한도 포함해줘야 하므로 `storage.objectUser`권한을 주고 테스트하겠음


### 230907
- 코랩에서 TPU로 학습하려면 GCS 관련 설정도 필요한 듯 하다. 그거 진행 중..
- GCS 버킷 접근 권한 설정 : `storage.buckets.list` 권한 부여하기
	- `[service-495559152420@cloud-tpu.iam.gserviceaccount.com](mailto:service-495559152420@cloud-tpu.iam.gserviceaccount.com) does not have storage.objects.list access to the Google Cloud Storage bucket.` 인걸로 봐서는 TPU에서 버킷에 접근할 권한을 부여해야 하는 것 같다.

### 230906
#### 새로운 이슈
- `UNIMPLEMENTED: File system scheme '[local]' not implemented (file: './models/char_ranked/model.ckpt-2000000')` 에러
	- GCS Bucket을 써라 + TPU가 해당 GCS Bucket에 접근할 수 있어야 한다
		- GCS 체험판(3달간 300$ 무료)을 사용해봄
	- 혹은, 캐글의 TPU를 사용하는 방법도 있다.
	- GCS 버킷 세팅 & 파일 업로드 완료, 코랩 파일에도 연동 완료.

### 230904 
- 이슈 해결 : 상속받는 optimzer를 `keras`가 아니라 `tf.compat.v1`에서 받게끔 변경


### 230901
- 멀고도 험하다. `krBERT`를 `Tensorflow 2.X` 버전에서 실행하기..

#### 이슈
1. `learning_rate`를 `optimization.create_optimizer`에서 인식하지 못하는 이슈

- 이해가 안 가는 상황이다.
```python
# main.py
import tensorflow as tf

lr = 5e-5
print(lr) # 5e-05

tensor = tf.compat.v1.constant(value = lr, shape = [], dtype = tf.compat.v1.float32)
print(tensor) # tf.Tensor(5e-05, shape=(), dtype=float32)
```

- 위 상황에서, lr 값을 `optimization.py`에 전달한 다음 아래 함수에 전달하는 상황임.
```python
# optimization.py
import tensorflow.compat.v1 as tf
import tensorflow
from tensorflow import keras

def create_optimizer(loss, init_lr, num_train_steps, num_warmup_steps, use_tpu):
  """Creates an optimizer training op."""
  global_step = tf.train.get_or_create_global_step()

  print("init_lr : ", init_lr)
  learning_rate = tf.constant(value=init_lr, shape=[], dtype=tf.float32)

  print("tensor : ", learning_rate)

  # Implements linear decay of the learning rate.
  learning_rate = tf.train.polynomial_decay(
      learning_rate,
      global_step,
      num_train_steps,
      end_learning_rate=0.0,
      power=1.0,
      cycle=False)
    
  print("polynomial_decay : ", learning_rate)

# ....
```

- 근데 여기서 `main.py`에서 `init_lr` 값을 `5e-5`로 전달했을 때,
```python
  print("init_lr : ", init_lr) # 5e-05
  print("tensor : ", learning_rate) # Tensor("Const:0", shape=(), dtype=float32)
  print("polynomial_decay : ", learning_rate) # Tensor("PolynomialDecay:0", shape=(), dtype=float32)

```
- 왜 이렇게 나오는지 모르겠음.

