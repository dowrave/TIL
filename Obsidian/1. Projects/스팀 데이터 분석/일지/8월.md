### 전체 진행 상황

1. `korBERT` 대신 `krBERT`나 `koBERT` 사용 시도
	- `korBERT`의 정확한 사용법을 잘 모르겠음. 체크포인트 파일을 인식하지 못하는 문제도 있어서, 다른 모델을 사용하려 한다.
## 220828

- 다른 모델 사용 ㄱㄱ
	- `koBERT(SKT)`나 `krBERT(SNU)`가 있다.
	- `krBERT`의 경우 `character, biWP` 모델을 사용함
		- 네이버 리뷰 감성분석에서 가장 높은 정확도를 보였기 때문
	- ![[Pasted image 20230828172841.png]]

- `HuggingFace`에서 개인적인 dict 등을 쓰고 싶다면 `token`이 필요하다. 구글 드라이브의 `Now/huggingface_key.py`에 저장해뒀음.
### 이슈
1. `bert_model` 불러오기 & 체크포인트 씌우기

2. 임의로 `tokenization.py`를 수정했는데, 이게 맞는 건가?
	- 모델 내에 이를 보정하는 층이 있을 것 같아서, 일단 원래 토크나이저를 쓰겠음. 어찌됐든 모델이 어떻게 생겼나를 봐야 하는데, 그게 안되는 상황에선 함부로 못 움직임;


3. `korBERT`를 어떻게 쓰는 건지 모르겠다.`koBERT`로 돌리는 게 맞나?
	- 성능은 `OpenAPI`에서 제공하는 `Morphology`를 이용하는 게 성능이 더 좋은데, 하루에 5000개, 1회에 10000자로 제한된 **형태소 분석기 API**를 이용해야만 가능하다. 대충 40일을 잡아야 하니까 이 방법은 쓸 수 없음.


## 230823

### 발생한 이슈
1. `bert_model.load_weights(tf.train.latest_checkpoint(checkpoint_path))` -> `AttributeError: 'NoneType' object has no attribute 'endswith'` 
	- 파일은 인식하는 데, 체크포인트인 걸 인식 못하는 것 같다.
	- 답을 못 찾겠으니까 다른 것부터 하자
2. BERT 모델의 경우, 텍스트 데이터의 시작에 `[CLS]`, 각 문장의 끝에 `[SEP]`가 있어야 한다는데 받은 코드에는 그러한 기능이 없다. 어떻게 해야 할까?
	- `cls_sep` 항목을 토크나이저에 추가하고, True일 때만 작동하게 해보자.

### 해결된 이슈
1. 클래스 불균형
	- 긍정 vs 부정의 비율이 3:1 정도 된다.
	- 오버샘플링이나 언더샘플링보다는, 클래스의 비율에 따라 가중치를 보정하는 방법을 이용하겠음.
2. 인풋 데이터 가공하기
	- 내가 가진 `tokenization.py`에는 패딩을 넣는 기능이 없네? 만들었음
	- 텍스트 데이터의 시작에 `[CLS]`, 각 문장의 끝에 `[SEP]`가 있어야 한다는데 받은 코드에는 그러한 기능이 없다. 어떻게 해야 할까?

## 230822
- korBERT에 설명서가 있긴 한데, **그것만 갖고는 못써먹는다. 직접 하나하나 찾아가면서 작업 중.**

### 이슈
- BERT 모델 파이프라인 작성 중...
1. `tokenization.py`를 어떻게 import하지?
	- [[코랩 파이썬 파일 마운트 후 import]] 
2. `tokenization.py`를 어떻게 이용하지?
	- 해당 파일의 `FullTokenizer`를 이용하며, 전달할 사전은 `dict`로 만들어서 줘도 무방하다. 
	- 코드 보는 재미가 있다.
	1) `tokenizer.py`에서 `tf.gfile.Gfile`이 없다는 에러가 있음 -> `tf.io.gfile.Gfile`로 수정
	2) `WordPieceTokenizer`와 달리 `FullTokenizer`은 `tf.io.gfile.Gfile`이 들어가는데, 텐서플로우에서 파일을 읽는 방식임. 따라서 **파일의 경로**를 전달해야 함. 그러면 코드는 훨씬 간결해짐.
3. 데이터 입력 시 `[h1]` 같은 HTML 태그, 체크박스 등을 남겨놓는 게 좋을까?
	- 일단 그냥 ㄱㄱ. 내가 임의로 뭘 없애고 넣고 해도, 모든 데이터를 볼 수 없기 떄문에 마음에 들지 않는 뭔가가 남을 거임. 그래서 원본 데이터로 진행하는 게 맞는 것 같음.
4. 체크포인트를 불러오는 과정에서 : `bert_model.load_weights(tf.train.latest_checkpoint(checkpoint_path))` 넣었을 때 아무것도 없다고 뜨는 상황. 

## 230821
- 일단 한국어로만 진행해봄
- 모델을 뭘 쓸까가 고민임 - 한국어 특화 모델을 쓰는 게 낫겠다
	- KoBERT : `Pytorch`로만 쓰는 방법이 나와 있어서, 지금은 사용 못할 것 같다.
	- KorBERT : `ETRI`에서 4개의 파일을 제공하는, 설명서를 보면 아래와 같음.
```
  2. BERT 모델 유형
    - 001_bert_morp_pytorch, 002_bert_morp_tensorflow
	  . 형태소분석 결과 기반 BERT 학습 모델
	  . 입력예: ETRI/SL 에서/JKB 한국어/NNP BERT/SL 언어/NNG 모델/NNG 을/JKO 배포/NNG 하/XSV 었/EP 다/EF ./SF
    - 003_bert_eojeol_pytorch, 004_bert_eojeol_tensorflow
	  . 어절 기반 BERT 학습 모델 (형태소분석 미수행)
	  . 입력예: ETRI에서 한국어 BERT 언어 모델을 배포하였다.
```
- 따라서 **ETRI의 4번 버전 모델**을 쓰면 될 듯. (입력예 때문에)

### 한국어 리뷰인데 영어가 나오면 어떡함?
- 그 외에도 이모지 등 특수문자가 나올 수도 있겠다.
- 다국어 모델이 나으려나? 일단 **KorBERT로 시도해본 다음에 판단**하자.


## 230818

 - 리뷰 분석 모델을 만들기 전에, 수집한 리뷰 데이터부터 살펴본다.
1. 데이터 숫자
	- 전체 1452852개
	- 한국어 194263개
		- 긍정 152236개
		- 부정 42027개
	- 일본어 110140개(의외로 적다)
		- 긍정 87767개
		- 부정 22373개
	- 영어 1148449개
		- 긍정 908754개
		- 부정 239695개

2. 특정 게임에 한정되는 것이 아니라, 사용된 모든 데이터에 대해 리뷰가 만들어져야 한다. 지금 걱정인 건 학습 시간이 너무 길까봐 ( + 그래서 코랩이나 캐글이 허용하는 학습 시간을 초과할까봐) 가 있는데, 이건 모델을 만들어봐야 알겠다.

### 프로젝트 계획 짜면서의 의문점과 ChatGPT의 답변

1. BERT 다국어 모델을 가져와서 파인튜닝을 하려고 할 떄, 파인튜닝에서 학습될 언어는 **사전학습된 언어의 종류보다 적다(한국어, 영어, 일본어)**. 이 상황에서 지금의 3가지 언어를 이용하는 것만으로 충분한 효과를 얻을 수 있는가?
> 크게 상관 없다. 애초에 BERT 자체가 몇 개의 언어로 학습되었는지 공개하지 않기도 하고(104개 언어라는 정보가 있으나 오피셜은 아님),  감성 분석의 경우 언어 별 차이가 크지 않기도 하다.
> 대신, 아래의 조건을 고려해야 한다.
> 1. 언어 간 데이터 균형 : 각 언어에 대한 데이터가 충분히 포함되어야 하며, 언어 간 데이터 균형이 중요하다. (샘플 숫자 맞추라는 뜻)
> 2. 언어 특성 : 각 언어마다 `토큰화`, `단어 임베딩`, `문장 구조 차이`, `레이블의 언어 의존성` 등 그 특성이 다양하다. 모델이 이러한 특성을 잘 이해하는지도 평가가 필요하다.

2. Label(긍정, 부정)의 비율도 중요한 걸로 알고 있다. 사용할 수 있는 방법들이 있을까?
> 1. `리샘플링` : `과소표집`은 더 많은 데이터를 쳐내는 방법이고, `과대표집`은 더 적은 데이터를 복붙해서 늘리는 방식이다. 한 쪽은 데이터가 줄고, 다른 쪽은 오버피팅될 수 있기 때문에 주의가 필요하다.
> 2. 레이블이 적은 클래스에 더 높은 가중치를 부여할 수 있다. 아래는 클래스 가중치를 보정하는 간단한 파이썬 코드.
```python
from sklearn.utils.class_weight import compute_class_weight
import numpy as np

# 각 클래스의 데이터 개수를 계산
class_counts = [1000, 500, 200]  # 각 클래스별 데이터 개수 예시

# 클래스 가중치 계산
class_weights = compute_class_weight('balanced', np.unique(y_train), y_train)
class_weights = dict(enumerate(class_weights))

# 모델 정의 및 컴파일
model = ...  # 모델 정의
model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'], class_weight=class_weights)
```
- `balanced` : 클래스 가중치를 자동으로 계산함. 직접 조정할 수 있으며, 뭐가 어떤 영향을 주는지는 실험을 통해 보자.
- 

## 230816

- 결론 : **감성 분석을 해보자!**
- `BERT`를 가져와서 쓰는 방식이 될 거 같은데, 감성 분석을 하려면 어떻게 구축해야 할까?
1. 일단 긍정 / 부정 Label이 있기 떄문에, BERT를 가져온 다음 이진분류하는 모델을 만들어보자.
	- 다른 게 감성 분석이 아니라 이게 감성 분석임 - 어떤 글이 있을 때 **이를 긍정 / 부정 / 중립으로 나누는 게 감성 분석**임
	- 그리고 외부 모델을 가져온 다음, 주어진 데이터로 재학습하는 과정 자체가 `Fine-Tuning`인 거고!

### 모은 데이터로 어떤 걸 할 수 있을까?
1. **리뷰 감성 분석 모델 생성** : 긍정 / 부정 여부만 생각했을 때 가지고 있는 데이터를 사용하면 만들 수 있지 않을까?
	- 새로 리뷰 데이터를 수집하는 건 너무 오래 걸리니까 이미 갖고 있는 데이터로만 진행해야 할 것 같음. 특정 게임 리뷰가 필요하다면 해당 게임에 대해서만 수행하는 방식으로.
	- 이게 긍정/부정 이진 분류인지, 아니면 문장 속 감정을 끄집어 내는 것인지는 더 생각해봐야 할듯. 후자일 경우 추가적으로 레이블링이 더 필요하지 않을까?
2. **리뷰 워드 클라우드** : 특정 게임에 대한 긍정/부정 리뷰에서, 어떤 단어가 많이 나타났는가?
	- 참고) `미니맵` 사이트의 각 게임 탭
	- 가볍게 보고 지나가긴 좋을 듯 : 최종 목표가 되기는 힘들어 보인다. 단어만 갖고 

3. **시간에 따른 추이 단순 시각화**
	- 데이터가 3달치가 쌓였기 때문에, 거의 대부분의 게임에 대해 하루 최대 동접자 추이를 볼 수 있다. `CCU`는 사실상 `DAU`로도 볼 수 있는 개념이니까.
	- 이외에도 리뷰 추이와 연관지어서 볼 수도 있을 듯.
- 게임의 국가를 추적하는 방식은 그냥 gg(딱히 방법이 없어 보임)