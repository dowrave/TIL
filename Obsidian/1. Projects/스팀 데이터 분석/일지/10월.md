## 일자별 정리

- 훈련 / 검증 / 테스트 데이터 : 8 : 1 : 1
<<<<<<< HEAD
=======

### 231026
- 토큰 512개에 대해서도 하이퍼파라미터 튜닝을 돌리려고 하는데, 갑자기 안 뜨던 오류가 뜬다. 
```
Traceback (most recent call last):
  File "/usr/local/lib/python3.10/dist-packages/keras_tuner/src/engine/base_tuner.py", line 273, in _try_run_and_update_trial
    self._run_and_update_trial(trial, *fit_args, **fit_kwargs)
  File "/usr/local/lib/python3.10/dist-packages/keras_tuner/src/engine/base_tuner.py", line 238, in _run_and_update_trial
    results = self.run_trial(trial, *fit_args, **fit_kwargs)
  File "/usr/local/lib/python3.10/dist-packages/keras_tuner/src/engine/tuner.py", line 314, in run_trial
    obj_value = self._build_and_fit_model(trial, *args, **copied_kwargs)
  File "/usr/local/lib/python3.10/dist-packages/keras_tuner/src/engine/tuner.py", line 233, in _build_and_fit_model
    results = self.hypermodel.fit(hp, model, *args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/keras_tuner/src/engine/hypermodel.py", line 149, in fit
    return model.fit(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/keras/src/utils/traceback_utils.py", line 70, in error_handler
    raise e.with_traceback(filtered_tb) from None
  File "/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/context.py", line 742, in sync_executors
    pywrap_tfe.TFE_ContextSyncExecutors(self._context_handle)
tensorflow.python.framework.errors_impl.ResourceExhaustedError: received trailing metadata size exceeds limit
```
--> 이전에 케라스 튜너를 쓰기 전에는 512에서도 잘 작동했는데, 이번에는 잘 작동하지 않음(동일한 코드로 256개의 토큰은 잘 작동한다.): 최대로 쓸 수 있는 토큰 개수를 찾아보자.
- 조사해보니까 이 문제는 데이터셋 올리는 양의 이슈 같음(`데이터 자체의 크기 x 배치 크기`로 정해지는 듯?)

### 231020
- 갑자기 이런 에러가 뜬다 : `InvalidArgumentError: stream is uninitialized or in an error state`
	- `런타임 다시 시작`이 아니라 `런타임 연결 해제/삭제`까지 한 다음에 다시 실행해야 정상적으로 돌아간다. (즉 코랩에 라이브러리 설치부터 다시 진행해야 함)
### 231019
- `batch_Size` 관련.. [배치 크기와 반복 횟수의 상충관계는 뭔가요?](https://stats.stackexchange.com/questions/164876/what-is-the-trade-off-between-batch-size-and-number-of-iterations-to-train-a-neu)
```
1. 일반적으로 경사하강법의 배치 크기는 전체 훈련 데이터 대비 작은 크기를 갖는다 : 일반적으로 "32 ~ 512"를 지정한다.
	- 이것보다 큰 배치사이즈는 "일반화" 성능으로 대표되는 모델의 성능의 열화를 가져온다.
	- 정확한 이유는 모름(2017년 논문 기준)
	- "large batch size"는 훈련 데이터의 10% 정도를 의미하며, "small batch"는 256개의 샘플을 의미함 : 경우에 따라 이것도 충분히 큰 값일 수 있다.
```

>>>>>>> 6617745468b0f0b175b5c0b441862225564e0460
### 231013
- 하이퍼파라미터 튜닝 (`BayesianOptimization`)
	- 학습률 : `1e-6`은 너무 느리고(그로 인해 더 많은 에포크가 필요해진다 = 학습 시간의 증가), `5e-4`는 학습이 되지 않는다. `5e-6` ~ `1e-4`의 범위에서 구해봄.
	- `warmup_step` : `warmup_step` 자체의 숫자 자체는 크게 중요하지 않아 보이는데,  500, 1000, 1500, 2000까지 줘 본다.
	- `freeze_body` : 모델 자체의 가중치를 갱신하지 않게 하는 파라미터인데, 미세 조정 시에는 `False`로 주라고 알고 있다. 근데 `True`로 줬을 때의 값이 더 높게 나오는 경우도 있음
	- `dropout_rate` : 모델의 마지막 층에서 적용하는 `dropout` 비율. `0`부터 `0.5`까지 지정했다.
- 그 외 : `batch_size` - 바꿔가면서 테스트를 해보고 있다. 하이퍼파라미터 튜닝에 들어가지 않는 값이라서 따로 실행해야 할 것 같음.
### 231011
- `tfrecord` 파일이 이미 있으면 원본 csv 파일에서 훈련 / 검증 / 테스트 데이터 분리를 더 이상 하지 않도록 막음
	- `batch_size`를 조정하면서 `val_loss` 값이 가장 낮은 지점을 찾아볼 계획이다. 여태까지 실행 이전에 시간이 좀 걸렸던게 이게 계속 반복되면서 수행되었기 떄문;
	- 배치 크기는 텐서플로우 데이터셋에서 입력할 때 조절하는 값이므로 케라스 튜너에 넣고 돌리긴 힘들 듯.
	- 특정 데이터에만 맞는 배치 크기가 나오지 않을까? 싶긴 한데, 케라스 튜너에 들어가지 않는 값인 이상 오버피팅을 체크할 방법이 없음. 미니 배치로 구성하는 방법 자체가 수렴 성능 / 속도 사이에서 타협을 본 방법이니까 같은 데이터들로 배치 크기만 다르게 구성해도 크게 상관 없지 않을까?
### 231010 

#### 1. `max_seq_length = 256`으로 결정
![[Pasted image 20231010141600.png]]
- 표준편차는 테스트마다 0.002 ~ 0.02까지 나타났음(7회 테스트)
- 총 4회의 테스트를 했으며, `128`과 `256`은 비슷한 편이되 `256`의 `val_loss` 값이 더 낮게 나타났음
	- `512`는 시간도 오래 걸리고, `loss` 값도 다른 두 값에 비해 높게 나타났음

#### 2. 하이퍼파라미터 튜닝
- `keras_tuner - hyperband`를 이용한다. [[HyperBand]] 설명 참고.
- 모델 구조는 일정할 수밖에 없어서, 그 외의 값들을 조정함
- 작업 중 에러 : `tensorflow.python.framework.errors_impl.UnimplementedError: File system scheme '[local]' not implemented (file: './steam_krbert_HPO_bi_classification/trial_0002/checkpoint_temp/part-00000-of-00001')` 
	- 구글 코랩에서 TPU로 작업 시, GCS로 연결해야 하는 이슈가 있었던 걸로 기억한다. TPU 자체는 `local`을 지원하지 않는 듯하며, 클라우드 버킷에 위치를 지정해봄.

### ~ 231009
- 인풋 시퀀스 길이 계속 정하는 중
	- 마지막 테스트로, 인풋 토큰 길이 128, 256, 512 에 대해 7번씩 테스트(훈련 데이터는 모든 모델에 대해 다름), 최종적으로 가장 낮은 `val_loss` 값을 갖는 모델을 선택하겠음
	- 256이나 512 중 하나 선택할 듯?
		- 512 : 한 에포크에 11~12분 정도 찍힘. 256 대비 성능 향상 폭도 눈에 띄는 편은 아니라서.


### 231004
- 하이퍼파라미터 튜닝 : `max_seq_length`(모델에 사용할 인풋 토큰 갯수) 조절
	- `val_loss` 값 기준으로 판단함
	- `64, 128, 256, 512`에 대해, 각각 3번씩(InputExample부터 완전히 다시 만들어서) 실행해서 평균값이 가장 좋은 걸 선정하겠음