
### 230620
- 일본어 공부하느라 바쁘다 ㅠㅠ
- 같은 날에 사이트가 수정돼서 데이터를 중복 수집하는 문제가 발생했음 : 사이트가 수정된 원인은 모르겠지만, 롤백된 것으로 보임
- 지금 데이터 수집 조건을 사이트에 있는 CS:GO의 5가지 피쳐가 내 DB의 5가지 피쳐와 같은지를 체크하고 있는데, 이런 상황이 발생하면 데이터를 중복해서 수집할 우려가 있음. 따라서 **글옵을 점검하는 조건을 취소하고, `time_value`에 오늘 날짜가 있는지만 점검**하겠음
	- `time_value`는 raw 데이터를 불러온 뒤, 파이썬에서 다시 `pd.to_sql()`로 한꺼번에 저장되므로 맞는 방법이라고 생각함. 문제가 생기면 그 때 고치면 되지~

### 230614
- 디테일 수집 : 날짜가 늘어남에 따라 디테일 갯수도 증가해야 하는데 6월 8일 전후로 그 갯수가 줄어들었음
- 체크 : 들여쓰기 상태는 상관 없고, **마지막 수집 이후에 `to_sql`이 빠져 있어서 그럼 : 수정 완료**
- 그 외에도 `get_detail`에서 저장하는 과정 함수로 따로 뺐음


### 230612
- 로컬에선 `lang_genre`, `tag`가 잘 저장되는데 컨테이너에선 저장되지 않았음 : 왜?


### 230609
- 어제 체크포인트를 저장하는 과정에서, `get_details` 함수가 반환하는 값은 `raw_detail`에 수집된 전체 데이터여야 하는데 마지막 체크포인트 부분만 반환되는 문제가 있다. 이를 수정함.
- `get_details`의 마지막 부분에서 `raw_detail`의 어제 날짜 부분만 긁어서 데이터프레임으로 받아오면 될 듯?

> 수집이 완료되면 `RAW_DETAIL`에 오늘 수집된 데이터들을 모두 불러오는 쿼리를 날려서 데이터를 가져오게끔 바꿈

#### 다시 스크립트가 왜 실행되지 않는가를 해결해야 함
- 스크립트 자체에 문제가 있을 확률이 높음
- chatGPT에 물어봐도 계속 질문 내용이 반영이 안된 대답을 한다. 돌겠다 ㅋㅋ


#### 답을 찾은 듯 하다?
`python script.py` -> `python -u script.py`
- `-u`를 추가해주면 의도한 로그들이 잘 뜸
- 그러면 `-u`가 무엇이며, 왜 위 스크립트랑 달리 아래 스크립트는 `-u`를 추가해야 `print()`문이 보이는 걸까? -> 블로그에 정리해둠
- `-u`는 출력을 바로바로 내보내는 역할을 함 : **`터미널`에서 실행한 경우는 상관 없는데, 중간에 다른 매개체를 끼면 `-u`를 지정해야 한다. 그렇지 않으면 버퍼가 쌓이기 전에는 로그가 보이지 않음.**



### 230608
- 서버가 이상한 상태인 경우나 점검 중인 상태에서 실행되면 `get_detail()`은 약 5000개의 데이터를 `appid`에 대한 쿼리 1개씩을 보내서 처리되게 됨.
- 현재는 5000개 전체를 받은 다음 저장 과정을 실행하는데, 중간에 인터넷 이슈 등이 발생할 경우 다시 처음부터 수집을 진행하게 된다. 1초에 1개의 쿼리를 리퀘스트하므로, 작업이 더 늘어지게 됨
- 따라서 중간에 집계된 것들을 저장하는 과정을 `get_detail` 내에 넣어야 할 것 같다.
- 데이터 수집 결과 자체는 `raw DB`에 저장하고 있기 때문에 한꺼번에 저장하는 스크립트 대신 중간중간에 저장하는 방식으로 구현하면 될 것 같음
- 근데 steamspy 리퀘스트 관련 오류는 내가 얻고 싶을 때 얻을 수 있는 게 아니라서, 오류가 발생한 날에 시도해야 할 것 같다. 일단은 함수만 만들어놓자.
- 이거에 영향을 받는 함수가 `check_today_raw_data`이다. 예전엔 데이터를 모두 수집한 다음 넣었기 때문에 이 함수가 유효했지만, 지금은 중단됨 -> 근데 중간에 날짜는 넣었네? -> 저 함수대로 실행하면 오늘은 더이상 실행할 필요가 없지만 실제로는 더 수집해야 하는 상황이 발생할 수도 있다. 

#### 컨테이너 켜놓고 냅두니까 작동했는데?
- 근데 다시 건드리니까 또 안됨. 미치겠다.
- 일단 내가 작성한 스크립트가 문제인지부터 확인하기 위해 컨테이너 내부 시간을 반환하는 파일을 하나 넣어놓고 그 파일도 컨테이너 시작 시 작동하도록 설정함
- 얘는 작동한다 : 그러면 스크립트가 문제라는 뜻.



### 230607
- 왜 실패로그도 안 뜨는 걸까?
- 테스트 : 잘못된 파이썬 이름 전달하기
	- 파일 이름 뒤에 `1`을 붙였음 
	- 로그 조회
	- `python: can't open file '/app/container_collect_data1.py': [Errno 2] No such file or directory` - 엔트리포인트는 작업 경로 잘 인식하는 거 확인 가능
- 테스트2 : 호스트 이름 이상하게 전달하기 (`steamspy-mysql111`)
	- **로그 반환 안함!**
- 테스트3 :
```dockerfile
ENTRYPOINT ["python", "container_collect_data.py"]

CMD ["--db-host", "steamspy-mysql"]
```
- `--db-host` 위치 변경
- `container_collect_data.py [-h] [--db-host MYSQL_HOST] container_collect_data.py: error: unrecognized arguments: steamspy-mysql` 에러 반환
- 테스트4:
```dockerfile
RUN chmod +x /app/container_collect_data.py

ENTRYPOINT ["python", "container_collect_data.py", "--db-host"]

CMD ["steamspy-mysql"]
```
- 권한도 솔직히 별 의미는 없는 듯

#### 진전
- restart: always를 collector에 안하고 mysql에 했었다 ㅋㅋㅋㅋㅋ collector에 하니까 2003 오류가 여러 번 뜨다가 1049오류로 바뀜
- **생각했던 것처럼 네트워크가 구축되기 전에 통신을 시도했던 것 같음.** 
- 스크립트에 `time.sleep(5)`을 넣고 2003번 예외는 함수를 바로 끝내게 구성했음
- 근데 또 컨테이너는 떠 있는데 아무런 로그가 없다. 미치겠네 ㄹㅇㅋㅋ

### 230606

- 컨테이너에 접근해서 동일한 커맨드로 스크립트를 실행하면 잘 작동한단 말임?
- 뭐가 문제인지 모르겠다 ㅠㅠ 오늘 거의 6시간 넘게 박았는데

#### 1. create_engine 및 connect 수 줄이기
- `SQLAlchemy`의 `create_engine`은 1번만 수행해도 되며, DB에 연결하고 끊는 과정은 `connect` 객체를 만들고, `close`로 닫으면 된다.
- 또한, 일반적으로 1번의 `connect`객체 내에서 많은 작업을 수행하는 것이 좋다고 한다. 네트워크 연결 설정, 인증, 설정 초기화 등의 작업이 반복되기 떄문이다.
- `df.to_sql(if_exists = 'append')`항에서 문제가 발생함 : 저렇게 지정했는데 새로운 데이터를 추가하는 게 아니라 새로 테이블을 만들려고 시도함 : 왜???
	- **엔진을 DB에 접근하지 않은 채로 만들었는데, 이 상태에서 `use db` 이후 `df.read_sql()`을 쓰는 것과 DB에 접근하는 엔진 자체를 이용하는 방법에는 차이가 있는 것으로 보인다.**
	- 따라서 엔진을 2개로 만들겠음 : RAW DB에 접근하는 엔진과 가공 데이터에 접근하는 엔진
	- 수정해보니까 엔진 이슈가 맞음! `df.to_sql()`을 이용할 때는 `db`까지 접근해 있는 엔진을 이용해야 함
- 위 과정을 아나콘다에서 했음 : 컨테이너까지 옮겨봄

#### 2. 그래도 안되는데 좀 다른 이유로 안됨
- 대충 `conn`에서 이슈가 생긴 것 같은데, 컨테이너에 올렸을 때 에러가 발생함
- `2003` 에러 : 연결할 수 없는 경우 발생한다고 하며, 해당 컨테이너에 `mysql-client`를 설치해봄
	- `default-mysql-client` 설치해도 안됨
			- 호스트명 : `steamspy-mysql root 0000 33060` (X, 에러 내용 동일)
			- 호스트명 : `localhost root 0000 33060` (X, 에러 내용 동일)

##### 다른 컨테이너를 만들어서 mysql 컨테이너에 연결을 시도해 봄(mysqladmin)
- 33060 포트는 인식하지 못했고, 3306 포트는 인식했음 : 즉, **3306 포트를 넣어야 맞는 것 같음**
- 그래도 안된다. 2003 에러.




### 230605
- **데이터를 수집하는 컨테이너를 올리고, 가능하다면 `docker-compose`까지 작성**해서 다 띄워놔보자.


### 230603~04
- 6월 3일은 해외 직구 환불 이슈로 하루를 날렸다. 그래도 매일 컴퓨터는 켜서 데이터 수집은 했음..
- 일단 호스트 OS에서 `MySQL` 컨테이너를 올린 다음, `localhost`와 `33060` 포트를 이용해 데이터를 수집하고 저장하는 과정은 구현했다.

### 230602
- 제일 먼저 할 일
1. 현재 Container에 저장하는 파일의 호스트 명을 `localhost`로 지정한 상태임 : Host OS에서 컨테이너에 저장하라면 `localhost:33060`으로 설정하면 된댔음 -> (사이트가 멀쩡하다면) 이걸 호스트 OS에서 실행해서 MySQL에 데이터가 저장되는지 확인할 수 있을 거임
	- 이게 잘 작동하면, 호스트 이름만 바꿔서 컨테이너에 파일 올리면 됨


### 230601
- 데이터를 수집하는 조건에 대해 생각해본다
	- 지금은 **수동으로**, 하루 중 특정 시간(오후 2시 전후)에 스크립트를 실행시켜서 데이터를 받고 있다.
	- 일부러 시간을 유지하고 있는데, `steamspy` 사이트가 하루에 1번 갱신되지만 그 정확한 시간이 나와있지 않기 때문이다.
	- 따라서, 지금까지는 사람이 신경을 써가면서 데이터를 일정한 시간에 받았다.
- 그런데 `appdetails`으로 특정한 id만 계속 추적한다면, 굳이 날짜를 신경쓸 필요가 없을 수 있다.
	- 사이트가 이상해지더라도 `appid`를 이용한 추적은 여전히 유효하다
	- 특정 appid의 데이터를 받음 -> 이미 있는 최근 날짜의 raw 데이터와 비교 -> 날짜에 따라 변하는 값들이 있는데, 이 값들이 동일하다면 이미 수집된 적이 있는 것이고, 값이 달라졌다면 데이터가 갱신되었다는 의미가 된다!
- 이걸 하는 이유는 예를 들면 이런 상황이 있기 때문임
> 한국 날짜로 6월 2일이 됐다고 하자. 
> 근데 steamspypi가 갱신되는 시간을 모른다. 
> 지금 방식으로는 6월 2일이 되면 스크립트를 또 실행할 수 있다. 날짜만 비교하기 때문.
> 그런데 사이트가 갱신되지 않았다면, 중복된 데이터를 받아오게 되는 것이다.

- 코드는 대충 이런 식으로 작성됨
```python
conn = get_connection(db = MYSQL_DB)

q = f"""SELECT * FROM {TABLE_TIME_VALUE} WHERE appid = 730 ORDER BY date DESC LIMIT 1"""
df = pd.read_sql(q, conn)
df = df.drop('date', axis = 1)
df

def check_same(data_sql, data_site: dict):
    if (data_sql.positive[0] == data_site['positive'] and
        data_sql.negative[0] == data_site['negative'] and
        data_sql.ccu[0] == data_site['ccu'] and
        data_sql.average_2weeks[0] == data_site['average_2weeks'] and
        data_sql.median_2weeks[0] == data_site['median_2weeks']):
        return True
    else:
        return False
```
- `appid = 730`은 글옵으로, 스팀에서 가장 피크 동접자가 많은 게임이므로 같은 날짜가 아닌 이상 모든 기간에 거쳐 정보가 다를 것이라 선정했음.
- 이거를 `check_today_executed`에 넣으면 되지 않을까?
