- 이전 : [[7-6. Sink Connector]]

- 8장 실습은 `8.Streaming` 폴더에서 진행함

- 패키지 
```sh
pip install kafka-python requests psycopg2-binary
```
>`kafka-python`
>- Kafka를 SDK 형태로 파이썬에서 쓸 수 있게 도와주는 Kafka Python Client 패키지
>- 여기선 Consumer 구현 시 사용함
>`requests`
>- 파이썬에서 `HTTP` 통신이 필요한 프로그램 작성 시 사용
>- API 호출 시 사용


![[Pasted image 20230105145211.png]]
- 7장의 `Sink Connector` 대신 직접 `Kafka Python SDK`를 이용해 `Consumer`를 구현해서 사용한다.

- 7장과 비교
> 7장 : Sink Connector가 해당 토픽에 있는 데이터를 읽어서 **자동으로** 타겟 데이터베이스에 전달했음
> 8장 : API 서버에 Request를 보내고 Response를 받는다.
>> 이 과정이 수동으로 이뤄지기 때문에 Sink Connector를 사용할 수 없다.

- Data Subscriber의 절차
>1. `psycopg2` : Target DB에 접근해 테이블 생성
>2. `kafka-python` : 브로커의 토픽에 있는 데이터를 읽는 Consumer 생성
>3. `requests` : 받은 데이터를 6장에서 띄운 API 서버에 보내고 예측값을 받음
>4. `psycopg2`로 받은 예측값을 Target DB에 삽입함

1. 예측값 포함한 테이블 생성 `data_subscriber.py`
```python
import psycopg2  
  
def create_table(db_connect):  
	create_table_query = """  
		CREATE TABLE IF NOT EXISTS iris_prediction (  
		id SERIAL PRIMARY KEY,  
		timestamp timestamp,  
		iris_class int  
		);"""  
	print(create_table_query)  
	with db_connect.cursor() as cur:  
		cur.execute(create_table_query)  
		db_connect.commit()  
  
if __name__ == "__main__":  
	db_connect = psycopg2.connect(  
		user="targetuser",  
		password="targetpassword",  
		host="target-postgres-server",  
		port=5432,  
		database="targetdatabase",  
		)  
	create_table(db_connect)
```

2. `Consumer` 생성 : `data_subscriber.py`의 일부
```python
from json import loads  
from kafka import KafkaConsumer  
  
consumer = KafkaConsumer(  
	"postgres-source-iris_data",  
	bootstrap_servers="broker:29092",  
	auto_offset_reset="earliest",  
	group_id="iris-data-consumer-group",  
	value_deserializer=lambda x: loads(x),  
)
```
- 파라미터 설명
> `topic` : 데이터를 읽고 싶은 토픽 지정
>> - 주의 : `topic = 'postgres..'로 입력하면 안됨`
> `bootstrap_servers` 
>> - Bootstrap으로 띄워진 `브로커 이름 : 브로커 포트`
> `auto_offset_reset` 
>> - 어떤 `offset` 값부터 가져올까를 결정
>> - `earliest`, `latest`가 있음
>>> `offset` : `consumer`에서 메시지를 어디까지 읽었는지 저장하는 값
> `value_deserializer`
>> - `Source Connector` 혹은 `Producer`에서 `Serialization`된Value 값을 `Deserialization`할 때 사용하는 `Desirializer`를 결정함
>> - 여기선 `Json Deserializer`를 이용하기 때문에 `lambda function`을 같이 씀

```python
for msg in consumer:  
	print(  
		f"Topic : {msg.topic}\n" ,
		f"Partition : {msg.partition}\n" ,
		f"Offset : {msg.offset}\n",
		f"Key : {msg.key}\n" ,
		f"Value : {msg.value}\n",  
	)
```
- `consumer` 인스턴스는 반복문으로 실시간으로 계속 데이터를 가져올 수 있다.
- 출력된 값 중 사용할 건 `Value`의 `payload` 값이다. 
```
'payload': {'id': 134, 'timestamp': '2022-12-15 04:49:41.21', 'sepal_length': 6.1, 'sepal_width': 2.8, 'petal_length': 4.0, 'petal_width': 1.3, 'target': 1}
```


### API 호출
- 6장에서 띄운 `schemas.py`의 내용
```python
from pydantic import BaseModel  
  
class PredictIn(BaseModel):  
	sepal_length: float  
	sepal_width: float  
	petal_length: float  
	petal_width: float  
  
class PredictOut(BaseModel):  
	iris_class: int
```
- 보낼 값은 4개의 칼럼, 받을 값은 1개의 칼럼이다

- 이 중 필요없는 칼럼들은 아래처럼 삭제할 수 있다
`data_subscriber.py`의 일부
```python
msg.value["payload"].pop("id")  
msg.value["payload"].pop("target")  
ts = msg.value["payload"].pop("timestamp")
```
- `timestamp`는 Source DB의 timestamp를 Target DB에 넣기 때문에 삭제는 하면서도 변수로 할당한다.

- 이후 payload 값을 보내고 response를 받는다.
```python
# data_subscriber.py
response = requests.post(  
	url="http://api-with-model:8000/predict",  
	json=msg.value["payload"],  
	headers={"Content-Type": "application/json"},  
).json()  
response["timestamp"] = ts
```
- 파라미터 설명
> `url` 
>> - API 서버의 엔드포인트를 설정함
>`json`
>> - `request`로 보낼 인자값들을 명시함
>`headers`
>> - `request` 전송 시 부가적인 정보 설정
>마지막으로 위에서 뗴놓은 `timestamp` 정보인 `ts`를 `response`에 넣어준다.

- 예측값 삽입
`data_subscriber.py`
```python
def insert_data(db_connect, data):  
	insert_row_query = f"""  
		INSERT INTO iris_prediction  
		(timestamp, iris_class)  
		VALUES (  
		'{data["timestamp"]}',  
		{data["iris_class"]}  
	);"""  
	print(insert_row_query)  
	with db_connect.cursor() as cur:  
		cur.execute(insert_row_query)  
		db_connect.commit()  
  
insert_data(db_connect, response)
```


- 전체 코드 `data_subscriber.py`
```python
from json import loads  
  
import psycopg2  
import requests  
from kafka import KafkaConsumer  
  
  
def create_table(db_connect):  
	create_table_query = """  
		CREATE TABLE IF NOT EXISTS iris_prediction (  
		id SERIAL PRIMARY KEY,  
		timestamp timestamp,  
		iris_class int  
		);"""  
	print(create_table_query)  
	with db_connect.cursor() as cur:  
		cur.execute(create_table_query)  
		db_connect.commit()  
  
  
def insert_data(db_connect, data):  
	insert_row_query = f"""  
		INSERT INTO iris_prediction  
		(timestamp, iris_class)  
		VALUES (  
		'{data["timestamp"]}',  
		{data["iris_class"]}  
	);"""  
	print(insert_row_query)  
	with db_connect.cursor() as cur:  
		cur.execute(insert_row_query)  
		db_connect.commit()  
  
  
def subscribe_data(db_connect, consumer):  
	for msg in consumer:  
		print(  
		f"Topic : {msg.topic}\n"  
		f"Partition : {msg.partition}\n"  
		f"Offset : {msg.offset}\n"  
		f"Key : {msg.key}\n"  
		f"Value : {msg.value}\n",  
		)  
  
	msg.value["payload"].pop("id")  
	msg.value["payload"].pop("target")  
	ts = msg.value["payload"].pop("timestamp")  

	# 모델을 띄운 서버에 request / response
	response = requests.post(  
		url="http://api-with-model:8000/predict",  
		json=msg.value["payload"],  
		headers={"Content-Type": "application/json"},  
		).json()  
	response["timestamp"] = ts  
	insert_data(db_connect, response)  
  
  
if __name__ == "__main__":  
	db_connect = psycopg2.connect(  
	user="targetuser",  
	password="targetpassword",  
	host="target-postgres-server",  
	port=5432,  
	database="targetdatabase",  
	)  
	create_table(db_connect)  
	  
	consumer = KafkaConsumer(  
		"postgres-source-iris_data",  
		bootstrap_servers="broker:29092",  
		auto_offset_reset="earliest",  
		group_id="iris-data-consumer-group",  
		value_deserializer=lambda x: loads(x),  
	)  
	subscribe_data(db_connect, consumer)
```

- `Dockerfile` 생성
```dockerfile
FROM amd64/python:3.9-slim  
  
WORKDIR /usr/app  
  
RUN pip install -U pip &&\  
pip install psycopg2-binary kafka-python requests  
  
COPY data_subscriber.py data_subscriber.py  
  
ENTRYPOINT ["python", "data_subscriber.py"]
```

- `stream-docker-compose.yaml` 파일 생성
```yaml
# stream-docker-compose.yaml  
version: "3"  
  
services:  
  data-subscriber:  
	build:  
	  context: .  
	  dockerfile: Dockerfile  
	container_name: data-subscriber  
  
networks:  
  default:  
    name: mlops-network  
    external: true
```

- 띄우기
```sh
docker compose -p part8-stream -f stream-docker-compose.yaml up -d
```

- 데이터 확인
```sh
PGPASSWORD=targetpassword psql -h localhost -p 5433 -U targetuser -d targetdatabase

# db 진입
SELECT * FROM iris_prediction LIMIT 100;
```

- 트러블 슈팅
> 1. 테이블은 만들어짐 / 데이터가 안들어옴
>> - 로그 조회해도 나오는 거 없음
>> - 7-6에서 카프카에 넣은 싱크 커넥터는 `토픽 -> SQL 서버의 테이블`로 데이터를 넣는 거였음
>> 이번 장에서 만든 `subscriber`는  `kafkaconsumer` 객체를 만들어서 `서버-그룹-토픽` 으로 접근해서 메시지를 하나씩 가져오는 방식이었음
>해결 : 토픽 이름이 `postgres-source-iris_data`인데 `iris-data`로 들어가 있었다

- 다음 :[[8-2. Grafana Dashboard]]