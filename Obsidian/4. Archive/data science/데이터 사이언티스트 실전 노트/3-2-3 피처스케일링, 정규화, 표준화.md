#독서 #데이터사이언티스트실전노트

- `예측 변수(Predictor Variable) = 피쳐`
- `레이블 = 타겟`
- `지도 학습, 비지도학습`


##### 분류
- 출력되는 데이터가 2개, 혹은 2개 이상의 그룹이며 셀 수 있는 경우
- 이진 분류
- 다중 분류

#### 분류 알고리즘
- **로지스틱 회귀** : 일반적 선형 모형. 데이터가 어떤 범주에 속하는지 시그모이드 함수로 확률을 계산한 후 임계값에 따라 범주로 분류
- **분류 트리** : DT죠?
- **SVM** : 데이터가 분류된 각 범주에서 최대한 멀리 떨어진 비선형 결정 경계(Decision Boundary)를 정의
- **k-NN**

#### 회귀 알고리즘
- **선형 회귀** : 종속변수 Y에 1개 이상의 독립변수 X와의 선형 상관관계
- **회귀 트리** : 얘도 DT임
- **SVR** : SVM의 종속변수가 연속형

### 수치형 : 데이터값을 그대로 쓸까 변환해야 할까?

#### 1. 규칙을 이용한 알고리즘
- **트리 기반 알고리즘**을 사용하면 **이상치가 있거나 비대칭 분포라도 주어진 데이터 값 그대로를 사용할 수 있다.**


#### 2. 데이터값 변환(피쳐 스케일링)
- 데이터 열마다 단위가 다를 경우, 큰 범위를 가진 데이터에 가중치가 생길 수 있다. 이럴 때는 단위를 통일해줘야 함.

##### 1. 거리 기반 알고리즘
- (지도학습) k-NN, SVM
- (비지도학습) K-Means 

##### 2. 경사하강법 기반 알고리즘
> 예시 ) 단순선형회귀
- 목적) 오차의 합을 최소화하는 직선 찾기 : 여기에 들어가는 절편과 기울기 값을 가중치라고 하며, 그 가중치를 찾는 게 목표.
$$
h_\theta(x) = \theta_0 + \theta_1x_1
$$
- 머신러닝에서 h는 가설을 의미하며, $\theta$는 가중치를 의미한다.

- 이를 최소화하기 위해 **비용 함수(cost function)** 를 정의하는데, 여기선 아래처럼 나타냄.
$$
cost = \frac{1}{n}\Sigma^n_{i=1}[y^{(i)} - h(x^{i})]^2
$$
- 잔차의 합의 평균값을 나타내며, 이 $cost$함수의 가장 작은 값을 구하는 게 목표이다.
- **피쳐 스케일링이 필요한 이유는 낮은 cost로 향하는 이동의 수를 줄이기 위함이다**
	- 피쳐 스케일이 다르다면, 지그재그로 움직인다
	- 피쳐 스케일을 했다면 낮은 cost로 곧바로 움직인다

- 최소점이 아니라 극소점에 도달할 수 있으며, 이를 Perturbed GD, SGD 등으로 극복할 수 있다.

## 피쳐 스케일링 선택 기준
- 크게 정규화, 표준화가 있다.

|               | 1. 정규화                              | 2. 표준화                                                                              |
|:-------------:| -------------------------------------- | -------------------------------------------------------------------------------------- |
|     목적      | **피쳐 범위를 변환**                       | **평균 0, 표준편차 1이 되도록 변환** / 2개 이상의 피처 범위를 같은 기준으로 비교할 수 있음 |
|   표현&용어   | Normalization                          | Standardization                                                                        |
| 파이썬 패키지 | MinMaxScaler, MaxAbsScaler(최대절댓값) | StandardScaler, RobustScaler(중앙값, IQR 사용)                                         |
|   주요 사항   | 분포 변화 X                            | 기존 변수가 정규분포일 때 더 적합                                                      |
|               | 이상치에 민감                          | 평균 0 , 표준편차 1이 되지만 표준 정규분포가 꼭 되는 건 아님                           |
|               | 변환 후 표준편차가 줄어들 수 있음      |                                                                                        |

### 표준화

#### 1. StandardScaler : 평균 0 , 표준편차 1로 표준화
- 표준편차 = 1일때를 단위분산이라고 한다.
- 거리, 경사하강법, 단위가 다른 피처 값을 비교할 때 단위를 맞춰준다.
$$
\frac{x_i - mean(x)}{std(x)}
$$
- 중요한 건 **표준화 작업 != 표준정규분포화**이라는 것이다. 
- **정규분포는 표준화를 하면 표준정규분포가 되지만, 정규분포가 아닌 데이터는 표준화를 해도 원래 분포를 갖는다.** (왜도, 첨도 값 등은 유지된다!)

#### 2. RobustScaler : 중앙값, IQR로 표준화
- 평균은 Robust하지 않다 : 이상치에 따라 값이 크게 달라지기 때문이다
- **중간값, 사분위수는 Robust하다** 
$$
\frac{x_i - Q_2}{Q_3 - Q_1}
$$
- 스케일링 후에 **이상치가 없어지거나 비대칭도가 줄어들지 않는다.**


### 정규화

#### 1. MinMaxScaler : 최솟값 0, 최댓값 1로 범위 변환
$$\frac{x_i - X_{min}}{X_{max} - X_{min}}$$
- **왜도, 첨도, 이상치 여부는 원래 값과 동일**하며, **이상치 값이 최솟값이나 최댓값으로 설정되기 때문에 스케일링 전에 정리해야 할 이상치를 확인**해야 함

#### 2. MaxAbsScaler : 최대절댓값으로 범위 변환
$$
\frac{x_i}{abs(X_{max})}
$$
- 스케일링된 피쳐 범위는 **-1 ~ 1**로 설정된다.
- $X_{min}$을 빼지 않기 때문에 $0$ 데이터는 $0$으로 유지된다 : 이는 희소 데이터의 희소성을 유지할 수 있는 장점이 있다.

### 피쳐 스케일링에서 기억해야 할 것

#### 1. 스케일링은 분포를 바꾸지 않는다.
- 왜도, 첨도, 밀도 분포 등 스케일링 전후의 값이 비슷하다.
- **피쳐 스케일링은 데이터 범위만을 바꾼다.**
- 단 정규분포에 StandardScaler를 적용하면 표준정규분포가 되긴 한다.

#### 2. 이상치 제거 작업이 아니다.
- 이상치를 제거하지 않고 스케일링을 하면 그 결과에 이상치가 반영되므로 **이상치는 피쳐 스케일링 이전에 제거되어야 한다.**
- `RobustScaler`는 반영되지 않긴 하지만, 이상치가 남은 건 마찬가지이다.

#### 3. 꼭 필요한 작업인가? 어느 피쳐에 어떤 스케일러를 써야 하는가?
- 일부 피처에 k-NN이나 PCA를 할 경우 이 작업에 해당하는 피쳐만 스케일링하면 된다.
- **특성마다 다른 스케일러를 쓸 수 있다.**

#### 4. 스케일링 함수는 정확히 사용해야 한다.
- `.fit()` : 스케일링할 피쳐의 평균, 표준편차가 저장됨
- `.transform()` : `.fit()`에 저장된 값을 바탕으로 피쳐를 변환함
- 훈련 데이터에서 일부 피쳐에만 `fit()`을 하고, 테스트 데이터에선 그 피쳐들에 `transform()`을 하라는 뜻.

#### 5. 스케일러를 저장하자.
- **`fit()` 설정 값은 `pickle`로 저장할 수 있다.** 나중에 사용할 때 써먹을 수 있겠죠?
- 스케일러를 저장하면 스케일링된 피쳐를 원래 값으로 복원할 때도 쓸 수 있다.
	- 스케일링을 하면 피쳐값의 해석 능력이 떨어질 수밖에 없다.