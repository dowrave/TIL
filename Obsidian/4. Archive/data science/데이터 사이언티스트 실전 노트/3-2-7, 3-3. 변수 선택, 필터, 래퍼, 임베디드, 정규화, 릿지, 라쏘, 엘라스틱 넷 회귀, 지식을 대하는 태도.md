- `이전 과정까지는 변수 추출`로, `전체 변수에서 이를 잘 나타낼 수 있는 새로운 변수를 만드는 과정`이었다.
- 변수 선택 방법은 필터, 래퍼, 임베디드 등이 있음
- 일부 변수로 예측 모델을 만들시 장점
	- 독립변수 - 종속변수 간의 관계를 더 잘 이해할 수 있음
	- 오버피팅을 피함
	- 예측 모델의 해석, 설명력(Interpretation)을 높인다.


## 1. 필터(Filter)

### 과정
- 예측 모델을 만들기 전, **독립변수 - 종속변수 간의 연관성을 측정**해 연**관성이 떨어지는 변수를 제외**한다.

### 장단점
- 장점 ) 고차원일 경우 다른 방법보다 필터 방법이 적합함
- 단점 ) 독립변수 끼리의 연관성은 고려하지 않음 : 다중공선성 문제가 생길 수 있다.

### 예시

1. **0에 가까운 분산(Near Zero Variance)**
- 관측치 대부분이 같은 값일 경우 분산이 0에 가까워 필요한 정보가 충분히 없을 수 있음

2. **Pearson's Correlation**
- X, y 모두 연속형

3. **ANOVA Correlation**
- X : 범주형, y : 연속형 or X : 연속형, y : 범주형

4. **카이 제곱 검정 / 정보 이득 (상호 정보량)**
- X : 범주형, y : 범주형

5. **비모수적 모델 사용**
- `랜덤 포레스트, DT 등`
- 상관관계를 계산할 때 변수 간의 선형관계 조건이 있는데, 이게 충족되지 않은 경우 사용함

## 2. 래퍼(Wrapper)

### 과정
일부 변수만을 예측모델에 사용해 성능 측정 작업을 반복한 뒤, 가장 성능이 높았던 조합을 이상적인 변수로 선택함

### 장단점
- 장점) 다른 변수를 추가/제거하는 과정에서 예측모델 성능 변화의 추이를 지켜볼 수 있음
- 단점)  오래 걸리며, 과적합 문제 발생 가능

### 예시
1. **전진선택(Forward Selection)**
- 변수가 없는 상태에서 시작, 1개씩 선택해 모델 성능을 측정 후 가장 좋은 성능 선택
- 1개의 가장 좋은 성능이 선택된 상태에서 추가로 새로운 변수를 넣고 다시 성능 체크, 가장 좋은 변수 선택
- 위 과정을 모델 성능 향상이 없을 때까지 반복함

2. **후진제거(Backward Elimination)**
- 모든 변수를 갖고 시작, 1개씩 제거하면서 성능에 가장 영향이 적은 변수를 제거함
- 원하는 변수 개수 or 정해진 성능 기준까지 반복

3. **단계선택(Stepwise Selection)**
- `전진선택 + 후진제거`

## 3. 임베디드(Embedded)

### 과정
- 필터 + 래퍼의 장점을 혼합
- 모델 자체에 변수 선택 기능이 포함되어 학습 및 생성 과정에서 최적의 변수를 선택함

### 장단점
- 래퍼 과정에서 과적합 문제가 발생할 경우 대안으로 쓸 수 있음

### 예시
1. **라쏘 회귀(LASSO Regression)**
- `Least Absoulute Shrinkage and Selection Operator Regression`
- L1 정규화를 사용함

2. **릿지 회귀(Ridge Regression)**
- L2 정규화를 사용함

3. **엘라스틱 넷 회귀(Elastic Net Regression)**
- `릿지 + 라쏘`의 선형 결합으로 두 정규화방법의 장단점을 보완함


## 정규화(Regularization)

> 예를 들면 이거임 
> 10개의 점이 있고, 이들을 잘 나타내는 회귀 모델의 계수를 구하고 싶음
> 이 **데이터들을 가장 잘 나타내는 선은, 이들 모두를 지나는 선**일 거임. 그리고 그 선은 복잡할 수밖에 없음
> 이 복잡한 선에 새로운 데이터가 들어왔을 때, 예측 성능은 저하될 수밖에 없음
> 이 때 새로운 데이터들과 회귀선의 차이의 합을 **잔차제곱합(RSS)** 이라고 하며, 작을수록 좋은 회귀 모델임

- 회귀 모델을 구하는 과정에서 모델에 사용되는 회귀 계수 값을 줄이거나 없애는 제약조건을 넣을 수 있다. **회귀선의 복잡함을 줄이는 효과**가 있음.

- 과적합을 줄이는 방법으로 **릿지 회귀와 라쏘 회귀**가 있다.

#### L1 정규화(LASSO)
$$
RSS + \lambda\Sigma^p_{j=1}|\beta_j|
$$
- **라쏘회귀는 위 식을 최소화하며 회귀 계수를 구한다.**
- 오른쪽의 항이 제약 조건으로, **회귀계수 절댓값의 합에서 튜닝 파라미터$\lambda$를 곱한 값**이다. 튜닝 파라미터는 **사용자가 지정**한다.
- **덜 주요한 변수의 회귀 계수**가 0이 아닐 때, 0이 아닌 패널티 값을 더해서 **회귀 계수를 0으로 만들 수 있다.** 여기서 **변수 선택 효과**가 나타난다.


#### L2 정규화(Ridge)
$$
RSS + \lambda\Sigma^p_{j=1}\beta_j^2
$$
- 릿지회귀는 위 식을 최소화하며 회귀 계수를 구한다.
- 회귀 계수를 0에 가까워지게 만들 뿐  0으로 만들지는 못하기 때문에 이를 보완한 게 라쏘회귀이다.
- **독립 변수가 많다**면 릿지 회귀를 쓸 때 회귀 계수를 0에 가깝게 만들 수는 있지만, 여전히 모든 독립변수가 살아있기 떄문에 여전히 예측 모델이 복잡하게 된다. 이를 보완한 게 Lasso.

> 책에는 많은 변수로 다중공선성 문제가 발생하면 라쏘회귀 성능이 떨어지므로 릿지를 쓰는게 낫다...라고 언급돼있는데 라쏘는 특정 피쳐를 사용하지 않는 특성 선택효과가 있기 때문에 더 낫다고 한 거 같은데?.. 뚜렷하게 나오는 뭔가가 없어서 일단 이대로..

#### 엘라스틱 회귀
- 라쏘 + 릿지
$$
\lambda\left(\frac{1-\alpha}{2}\Sigma^p_{j=1}\beta_j^2 + \alpha\Sigma^p_{j=1}|\beta_j|\right)
$$
- 여기서 **알파값은 0 ~ 1**이며 
	- 1에 가까울수록 라쏘가 커지고 
	- 0에 가까울수록 릿지가 커짐

- 차원을 선택한다 = 변수를 줄인다이므로 정보를 줄임으로써 얻을 수 있는 이득과 정보 손실을 꼭 따져봐야 하며, 여러 번의 실험 과정이 꼭 필요하다.

---
## 지식을 대하는 태도
- 계속 새로운 기술이 나오고 알고리즘이 보완되고 있기 때문에, 지식의 양 자체로 더 나은 사람을 구분하는 건 무의미하다.

- 지식의 양보다는 지식을 대하는 태도나 질이 더 중요하다.

1. 왜 이걸 알아야 할까?
2. 어떻게 실무에 올바르게 적용할 수 있을까?
3. 파악하지 못한 부분은 뭘까?
4. 더 나은 방법이 있을까?

- 태도는 쉽게 바뀌지 않기 때문에, 위의 4개를 생각하면서 새로운 것을 학습할 것.

