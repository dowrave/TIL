
- [[#1. 신경망과의 첫 만남|1. 신경망과의 첫 만남]]
	- [[#1. 신경망과의 첫 만남#MNIST 데이터셋 이미지 분류|MNIST 데이터셋 이미지 분류]]
- [[#2. 신경망을 위한 데이터 표현|2. 신경망을 위한 데이터 표현]]
	- [[#2. 신경망을 위한 데이터 표현#텐서(Tensor)|텐서(Tensor)]]
		- [[#텐서(Tensor)#텐서의 핵심 속성|텐서의 핵심 속성]]
	- [[#2. 신경망을 위한 데이터 표현#배치 데이터와 배치 축|배치 데이터와 배치 축]]
	- [[#2. 신경망을 위한 데이터 표현#텐서 예시|텐서 예시]]
	- [[#2. 신경망을 위한 데이터 표현#이미지 데이터|이미지 데이터]]
	- [[#2. 신경망을 위한 데이터 표현#비디오 데이터|비디오 데이터]]
- [[#3. 텐서 연산|3. 텐서 연산]]
	- [[#3. 텐서 연산#1. 원소별 연산|1. 원소별 연산]]
	- [[#3. 텐서 연산#2. 브로드캐스팅|2. 브로드캐스팅]]
	- [[#3. 텐서 연산#3. 텐서 곱셈`dot`|3. 텐서 곱셈`dot`]]
	- [[#3. 텐서 연산#4. 텐서 크기 변환`reshape`|4. 텐서 크기 변환`reshape`]]
	- [[#3. 텐서 연산#5. 텐서 연산 기하학적 해석|5. 텐서 연산 기하학적 해석]]
	- [[#3. 텐서 연산#6. 딥러닝의 기하학적 해석|6. 딥러닝의 기하학적 해석]]
- [[#4. 신경망 엔진 : 그래디언트 기반 최적화|4. 신경망 엔진 : 그래디언트 기반 최적화]]
	- [[#4. 신경망 엔진 : 그래디언트 기반 최적화#확률적 경사 하강법 MGD(Mini-batch SGD)|확률적 경사 하강법 MGD(Mini-batch SGD)]]
	- [[#4. 신경망 엔진 : 그래디언트 기반 최적화#4. 도함수 연결 : 역전파 알고리즘|4. 도함수 연결 : 역전파 알고리즘]]
		- [[#4. 도함수 연결 : 역전파 알고리즘#GradientTape|GradientTape]]
- [[#5. 텐서플로우 API로 MNIST 예제 구현하기|5. 텐서플로우 API로 MNIST 예제 구현하기]]
	- [[#5. 텐서플로우 API로 MNIST 예제 구현하기#1. Dense Class|1. Dense Class]]
	- [[#5. 텐서플로우 API로 MNIST 예제 구현하기#2. Sequential Class|2. Sequential Class]]
	- [[#5. 텐서플로우 API로 MNIST 예제 구현하기#3. 모델 만들기|3. 모델 만들기]]
	- [[#5. 텐서플로우 API로 MNIST 예제 구현하기#4. MiniBatch 생성기|4. MiniBatch 생성기]]
	- [[#5. 텐서플로우 API로 MNIST 예제 구현하기#5. 훈련 스텝 실행하기|5. 훈련 스텝 실행하기]]
	- [[#5. 텐서플로우 API로 MNIST 예제 구현하기#6. 전체 훈련 루프|6. 전체 훈련 루프]]

> 실습은 코랩에서 진행함

## 1. 신경망과의 첫 만남

### MNIST 데이터셋 이미지 분류
- 몇 번 했을 거라서 짚을 부분만 빠르게 짚겠음

1. mnist 이미지는 0부터 255까지 값이 있는데, 이 값들을 **정규화**해준다. 왜냐하면 **신경망은 입력 데이터의 스케일에 민감**하기 때문이다.

2. 케라스 모델의 전체적인 코드
```python
(train_images, train_labels), (test_images, test_labels) = keras.datasets.mnist.load_data()


model = keras.Sequential([
						  
])

model.compile(optimizer = 'rmsprop',
			 loss = 'sparse_categorical_crossentropy',
			 metrics = ['accuracy'])

# 훈련
model.fit(train_x, train_y, epochs= , batch_size = )

# 예측
predict = model.predict(test_images) # 리스트로 10개의 데이터 반환
```

## 2. 신경망을 위한 데이터 표현

### 텐서(Tensor)
- 데이터를 위한 컨테이너이며 일반적으로 숫자를 담는다.
- **임의의 차원 개수를 갖는 행렬의 일반화**된 모습이다. `차원 = 축`
- `랭크Rank` : 축의 개수로 봐도 무방함
	- 랭크-0 : `스칼라`
	- 랭크-1 : `벡터`
	- 랭크-2 : `행렬`
	- 랭크-3부터는 딱히 부르는 이름이 없다
- **딥러닝에서는 랭크-0부터 랭크-4까지 일반적으로 다룬다. 동영상인 경우 랭크-5까지 다룸.**

#### 텐서의 핵심 속성
- `랭크` : 축의 갯수. `.ndim`으로 조회 가능
- `크기` : 텐서의 각 랭크에 얼마나 많은 차원이 있는지 나타내는 튜플. `.shape`로 조회
- `데이터 타입` : `float16, float32, float64, uint8, string(텐서플로)` 등.

- 임의의 이미지 1개 조회
```python
img = train_images[4]
plt.imshow(img, cmap = plt.cm.binary)
plt.show()
```

- 슬라이싱
- MNIST의 경우 (데이터 갯수, 픽셀 수, 픽셀 수)
- 넘파이의 슬라이싱은 `[][][]`이 아니라 `[:, :, :]`이다.

### 배치 데이터와 배치 축
- **딥러닝에서 사용하는 모든 텐서의 1번째 축은 샘플 축**이다. (데이터셋으로 치면 **샘플 갯수**)
- 왜 샘플 축이 필요하냐 : 딥러닝은 데이터셋들을 한꺼번에 모두 처리하지 않고, 배치 단위로 나눠서 처리하는데 이들 각각을 표시할 인덱스가 필요하기 때문이다.

### 텐서 예시
- 일반적으로 **생각하는 차원에 1차원 올리면 맞음**
	- 벡터  : `samples, features` 
	- 시계열, 시퀀스  : `samples, timesteps, features`
		- 각 샘플은 특성 벡터의, 길이를 `timesteps`으로 갖는 시퀀스이다.
	- 이미지 : `samples, height, width, channel` or `sample, channel, height, width`
		- 각 샘플은 픽셀의 2D 격자, 각 픽셀은 수치값의 벡터
	- 동영상 : `sample, frame, height, width, channel` or `sample, frame, channel, height, width`
		- 각 샘플은 이미지의 시퀀스이다. 길이가 `frame`임.


### 이미지 데이터
- 흑백 : `샘플 수, 높이 픽셀 수, 너비 픽셀 수, 1`
- 컬러 : `샘플 수, 높이 픽셀 수, 너비 픽셀 수, 3`

> 참고 : **이미지는 보통 너비 * 높이로 일상에서 말하지만, 행렬에서는 행이 앞으로 나오니까 높이 * 너비로 표현**됨  

- `채널 마지막Channel-last` : 채널이 텐서의 가장 뒤에 오는 것
	- 텐서플로우
- `채널 우선Channel-First` : 채널이 샘플 다음으로 오는 것
	- 파이토치

### 비디오 데이터
-  동영상은 일반적으로 `float32`로 저장되지 않지만, 그렇게 저장한다면 144 x 256 크기의 60초짜리 데이터는 405mb이다.
> 실생활에서 보는 비디오는 MPEG 포맷 같은, 높은 압축률로 압축되어 있어 훨씬 용량이 적다.

## 3. 텐서 연산
```python
keras.layers.Dense(512, activation = 'relu')
```

- 위 함수는 실제로 이렇게 작동한다.
```python
output = relu(dot(W, input) + b)
```
- 입력 텐서와 W의 내적
- 내적으로 만들어진 행렬과 벡터 `b`의 덧셈
- `relu` 함수 : `max(x, 0)`

### 1. 원소별 연산
- `relu` 함수와 `+`은 원소별 연산`element-wise operation`이다. 텐서의 각 원소에 독립적으로 적용된다.
- `numpy`는 이런 원소별 연산을 매우 빠르게 처리할 수 있다. 
	- `BLAS : Basic Linear Algebra Subprogram` 구현에 복잡한 일들을 `numpy`가 위임한다.
		- `BLAS`는 고도로 병렬화되고 효율적인 저수준의 텐서 조작 루틴으로, 포트란이나 C로 구현됨.
	- 파이썬으로 연산을 직접 구현하는 것 대비 예제 기준 100배(2.45초 vs 0.02초) 이상 차이남.

### 2. 브로드캐스팅
- 크기(차원)가 다른 두 텐서가 더해진다면, **작은 텐서가 큰 텐서의 크기에 맞춰 브로드캐스팅**된다.
- 예를 들어 `(32, 10)`인 행렬과 `(10,)`인 벡터가 더해진다면, 후자인 벡터는 전자에 32번 반복되어 적용됨. 즉 동일한 행이라면 같은 값이 더해진다.

### 3. 텐서 곱셈`dot`
- 행렬에서 `내적`이라고 부른걸 `텐서 곱셈, 점곱 Dot Product`이라고 부른다. 
- 점곱 성립 조건 : `x \dot y`일 때 `x.shape[1] == y.shape[0]`이어야 함
- **벡터-벡터 내적은 교환법칙이 성립**하지만 
- **행렬-벡터 내적, 행렬-행렬 내적은 교환 법칙이 성립하지 않는다.**

### 4. 텐서 크기 변환`reshape`

### 5. 텐서 연산 기하학적 해석
- 모든 텐서의 연산은 기하학적 해석이 가능하다. 
- `텐서 덧셈` : 객체를 특정 방향으로 특정 양만큼, 객체를 왜곡시키지 않고 이동함
	- `이동` : 어떤 점에 벡터를 더하기
	- `회전` : `[[cos -sin][sin cos]] \dot [x y]`으로 얻을 수 있음
	- `크기 변경` : `[[horizontal_factor, 0][0, vertical_factor]] \dot [x y]`
	- `선형 변환` : 임의의 행렬과 내적
	- `아핀 변환affine transform` : `선형 변환`과 `이동`의 조합. $y = W \cdot x + b$ 과 동일.
	- `relu 사용하는 Dense층`
		- 아핀 변환의 중요한 성질 중 하나는, **여러 아핀 변환을 반복해서 적용해도 결국 하나의 아핀 변환이 된다** 
		- **활성화 함수를 적용하지 않으면 아무리 깊은 층을 형성해도 1개의 층으로 요약 가능하다와 같은 말**

### 6. 딥러닝의 기하학적 해석
- 신경망 = 텐서 연산의 연결
- 텐서 연산 = 입력 데이터의 간단한 기하학적 변환
- **기초적인 연산을 길게 연결해서 복잡한 기하학적 변환을 조금씩 분해**하는 방식인 것

## 4. 신경망 엔진 : 그래디언트 기반 최적화
```python
output = relu(dot(W, input) + b)
```
- `W` : 가중치 (커널)
- `b` : 훈련되는 파라미터(편향)

> 커널은 의미가 다양함 : `커널 분석`, `CNN의 필터` 모두 커널로 쓰지만 다 다른 말임


- `무작위 초기화Random Initialization` : 초기에는 가중치 행렬이 작은 난수로 초기화됨
- `훈련Training` : 피드백 신호에 의해 가중치는 점진적으로 조정됨

- 훈련을 계속 반복하는데, 그 루프 속에서 이런 단계가 반복된다.
	1. 훈련 샘플 x와 y_true의 배치 추출
	2. `forward pass` : x를 사용해 모델을 실행, y_pred 구함
	3. y_pred와 y_true의 차이를 측정해 모델의 손실 계산
	4. 모델의 모든 가중치를 업데이트함

- 위 서순에서 `1`은 그냥 코드, `2, 3`은 앞에서 다룬 행렬 연산이다.
- `4`는 **어떻게, 얼마나 가중치를 업데이트하냐**는 이슈가 있다.
	- 방법 1. 관심 있는 원소만 다른 값을 적용하고, 나머지 값은 고정
		- 모든 원소에 이걸 반복한다면 비효율적임 : 비용이 너무 큼(수백만개의 가중치)
	- **방법 2. 경사 하강법** 
		- `z = x + y`에서, 어떤 값을 수정했을 때 z값의 변경 방향을 알 수 있다. 이를 **미분 가능**하다고 함.
		- 가중치 - 손실 매핑 함수에도 적용 가능하며, **그래디언트**라는 연산이 사용된다.

> `도함수derivative`
> `그래디언트gradient` : 텐서 연산의 도함수.


### 확률적 경사 하강법 MGD(Mini-batch SGD)

> SGD는 일반적으로 Mini-Batch SGD를 의미함

- 도함수가 0인 지점을 모두 찾고, 그 중에서 최솟값인 지점을 찾는게 목표임
- 위의 4단계를 조금 더 나누면
	4-1. `backward pass` : 모델의 파라미터에 대한 손실 함수의 그래디언트를 계산함 
	4-2. 그래디언트의 **반대 방향**으로 파라미터를 조금 이동시킴
$$
W = - LearningRate \times gradient
$$
- 위 과정을 `미니배치 확률적 경사 하강법 Mini-batch stochastic GD`이라고 한다.
	- `확률적` : **각 배치 데이터가 무작위로 선택**된다

- SGD는 아래 두 알고리즘의 중간점이다.
	- true SGD(반복마다 1개의 샘플과 타깃 뽑기) 
	- BGD(Batch GD : 모든 데이터를 사용한 반복) : 더 정확하지만 더 많은 비용

- 경사 하강법은 실제로는 매우 고차원에서 진행된다. 
- 최솟값을 찾는 이슈는 실제로 매우 어려움 : 1000개의 파라미터에서 극솟값이 최솟값일 확률은 $2^{-1000}$이다. 

- 가중치 계산 시 **현재 그래디언트 값과 더불어 이전에 업데이트된 가중치를 고려**하는 방식들도 있다.
	- 모멘텀 사용 : `SGD, Adagrad, RMSProp`
	- **모멘텀은 중요한데, SGD에 있는 2개의 문제점인 수렴 속도와 지역 최솟값을 해결**해준다.

```python
# 네스테로프 모멘텀 : 모멘텀을 2번 반복하는 단순 예제
past_velocity = 0.
momentum = 0.1
while loss > 0.01:
	w, loss, gradient = get_current_parameters()
	velocity = momentum * past_velocity - learning_rate * gradient
	# 기본 모멘텀 : w += velocity
	w += momentum * velocity - learning_rate * gradient
	past_velocity = velocity
	update_parameter(w)
```

- SGD에서 모멘텀을 쓰고 싶다면
```python
sgd = optimizers.SGD(lr = 0.01, momentum = 0.9, nesterov = True)
```
같은 방식으로 객체를 직접 생성한뒤 `.compile(optimzer = sgd)` 처럼 전달해야 한다.

> 모멘텀은 일반적으로 0.9 정도를 많이 사용한다.


### 4. 도함수 연결 : 역전파 알고리즘
- 역전파는 어떻게 계산되는가

- 손실 함수 : `loss_value = loss(y_true, softmax(dot(relu(dot(inputs, W1) + b1), W2) + b2))`

- `연쇄 법칙Chain Rule`
```python
def fg(x):
	x1 = g(x)
	y = f(x1)
	return y
```
- 연속된 함수의 미분 법칙은 기억나지 않음? : 겉에꺼 미분한거 + 속에꺼 미분한거

- 연쇄법칙의 핵심은 **노드의 경로를 따라 각 엣지의 도함수를 곱하면, 어떤 노드에 대한 다른 노드의 도함수를 얻을 수 있다**는 것이다.
	- 예시로, `grad(loss_val, w) = grad(loss_val, x2) * grad(x2, x1) * grad(x1, w)`로 얻을 수 있음.
	- `dy/dx = dy/dz * dz/dx` 이런 개념 기억나면 그거임
- 이 법칙을 계산 그래프에 적용한 것 뿐이다.
- `자동 미분`은 텐서플로우와 같은 프레임워크에서 쓰여서 신경망을 구현한다.

#### GradientTape
- 텐서플로우의 자동 미분 API이다. 
```python
import tensorflow as tf

x = tf.Variable(0.) # 초기값 : 스칼라 변수 생성
with tf.GradientTape() as tape: 
	y = 2 * x + 3 # 변수에 텐서 연산 적용
grad_of_y_wrt_x = tape.gradient(y, x) # tape를 사용해 변수 x에 대한 y의 그래디언트 계산

# 다차원 텐서
x = tf.Varaible(tf.zeros(2, 2))
with tf.GradientTape() as tape:
	y = 2 * x + 3
grad_of_y_wrt_x = tape.gradient(y, x) # (2, 2) 크기의 텐서
```
- 이런 식으로 `tape` 내부에 어떤 `shape`가 오든 이용 가능함
```python
with tf.GradientTape() as tape:
	y = tf.matmul(x, W) + b # matmul은 내적임
grad_of_y_wrt_W_and_b = tape.gradient(y, [W, b])
```

## 5. 텐서플로우 API로 MNIST 예제 구현하기

> 실제로 케라스가 어떤 식으로 작동하는지, 텐서플로우에서 low level 코드를 살펴보자

### 1. Dense Class
```python
import tensorflow as tf

# 1. Dense Class
class NaiveDense:
	def __init__(self, input_size, output_size, activation):
		self.activation = activation
		
		# 랜덤값 초기화된 w_shape 크기 행렬 만들기
		w_shape = (input_size, output_size) 
		w_initial_value = tf.random.uniform(w_shape, minval = 0, maxval = 1e-1)
		self.W = tf.Varaiable(w_initial_value)
		
		# b는 0으로 초기화
		b_shape = (output_size,) 
		b_initial_value = tf.zeros(b_shape)
		self_b = tf.Variable(b_initial_value)
	
	def __call__(self, inputs): # 정방향 패스 수행
		return self.activation(tf.matmul(inputs, self.W) + self.b)
	
	@property
	def weights(self):
		return [self.W, self.b]
```

### 2. Sequential Class
```python
class NaiveSequential:
	def __init__(self, layers):
		self.layers = layers
		
	def __call__(self, inputs):
		x = inputs
		for layer in self.layers:
			x = layer(x)
		return x
	
	@property
	def weights(self):
		weights = []
		for layer in self.layers:
			weights += layer.weights
		return weights
```

### 3. 모델 만들기
```python
model = NaiveSequential([
			NaiveDense(input_size = 28 * 28, output_size = 512, activation = tf.nn.relu),
			NaiveDense(input_size = 512, output_size = 10, activation = tf.nn.softmax)
])
assert len(model.weights) == 4
```

### 4. MiniBatch 생성기
```python
import math

class BatchGenerator:
	def __init__(self, images, labels, batch_size = 128):
		assert len(images) == len(labels)
		self.index = 0
		self.images = images
		self.labels = labels
		self.batch_size = batch_size
		self.num_batches = math.ceil(len(images) / batch_size)
	
	def next(self):
		images = self.images[self.index : self.index + self.batch_size]
		labels = self.labels[self.index : self.index + self.batch_size]
		self.index += self.batch_size
		return images, labels
```


### 5. 훈련 스텝 실행하기
- **1개의 배치 데이터에서 모델을 실행하고, 가중치를 업데이트**한다.
> 1. 배치 이미지에 대해 모델 예측 계산
> 2. 실제 레이블로 예측의 손실 값 계산
> 3. 모델 가중치에 대한 손실의 그래디언트 계산
> 4. 그래디언트의 반대 방향으로 가중치 이동  

```python
def one_training_step(model, images_batch, labels_batch):
	with tf.GradientTape() as tape: # 정방향 패스 실행
		predictions = model(images_batch)
		per_sample_losses = tf.keras.losses.sparse_categorical_crossentropy(labels_batch, predictions)
		average_loss = tf.reduce_mean(per_sample_losses)
	gradients =tape.gradient(average_loss, model.weights) # 가중치 손실 계산
	update_weights(gradients, model.weights) # 가중치 업데이트
	return average_loss
```
- 가중치 손실 계산은 `model.weights` 리스트에 있는 가중치에 매칭된다.

- 가중치 업데이트 함수
```python
def update_weights(gradients, weights):
	for g, w in zip(gradients, weights):
		w.assign_sub(g * learning_rate) # assign_sub = `-=`

# 실제로는 직접 구현하는 것보단 이렇게 사용함
from tensorflow.keras import optimizers

optimizer = optimizers.SGD(learning_rate = 1e-3)

def update_weights(gradients, weights):
	optimizer.apply_gradients(zip(gradients, weights))
```

### 6. 전체 훈련 루프
```python
def fit(model, images, labels, epochs, batch_size = 128):
	for epoch_counter in range(epochs):
		print(epoch_counter)
		batch_generator = BatchGenerator(images, labels)
		for batch_counter in range(batch_generator.num_batches):
			images_batch, labels_batch = batch_generator.next()
			loss = one_training_step(model, images_batch, labels_batch)
			if batch_counter % 100 == 0:
				print(f"{batch_counter}번째 배치 손실 : {loss:.2f}")
```

- 함수 테스트
```python
from tensorflow.keras.datasets import mnist
(train_images, train_labels), (test_images, test_labels) = mnist.load_data()

train_images = train_images.reshape((60000, 28 * 28))
train_images = train_images.astype("float32") / 255

test_images = test_images.reshape((10000, 28 * 28))
test_images = test_images.astype('float32') / 255

fit(model, train_images, train_labels, epochs = 10, batch_size = 128)
```

- 모델 평가
```python
predictions = model(test_images)
predictions = predictions.numpy() # tensor -> array
predicted_labels = np.argmax(predictions, axis = 1)
matches = predicted_labels == test_labels

print(f"정확도 : {matches.mean():.2f}")
```
