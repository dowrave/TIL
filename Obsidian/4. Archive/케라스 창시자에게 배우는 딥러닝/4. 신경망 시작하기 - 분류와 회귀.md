1. [[#1. 이진 분류 : 영화 리뷰 분류하기|1. 이진 분류 : 영화 리뷰 분류하기]]
	1. [[#1. 이진 분류 : 영화 리뷰 분류하기#1. IMDB 데이터셋|1. IMDB 데이터셋]]
	2. [[#1. 이진 분류 : 영화 리뷰 분류하기#2. 데이터 준비|2. 데이터 준비]]
	3. [[#1. 이진 분류 : 영화 리뷰 분류하기#3. 신경망 모델 만들기|3. 신경망 모델 만들기]]
	4. [[#1. 이진 분류 : 영화 리뷰 분류하기#4. 검증 데이터 및 훈련|4. 검증 데이터 및 훈련]]
2. [[#2. 뉴스 기사 분류 : 다중 분류|2. 뉴스 기사 분류 : 다중 분류]]
	1. [[#4. 검증 데이터 및 훈련#손실함수 : `categorical_crossentropy`와 `sparse_categorical_crossentropy`의 차이점|손실함수 : `categorical_crossentropy`와 `sparse_categorical_crossentropy`의 차이점]]
	2. [[#4. 검증 데이터 및 훈련#중간층의 유닛은 충분히 커야 한다.|중간층의 유닛은 충분히 커야 한다.]]
3. [[#3. 주택 가격 예측 : 회귀|3. 주택 가격 예측 : 회귀]]
	1. [[#3. 주택 가격 예측 : 회귀#1. 표준화|1. 표준화]]
	1. [[#3. 주택 가격 예측 : 회귀#2. 회귀 모델 구성|2. 회귀 모델 구성]]
	1. [[#3. 주택 가격 예측 : 회귀#3. K-fold Cross-Validation|3. K-fold Cross-Validation]]




## 1. 이진 분류 : 영화 리뷰 분류하기

### 1. IMDB 데이터셋
- 양극단의 리뷰 5만개로 이뤄짐
- 훈련 25000개, 테스트 25000개, 긍정 부정 절반씩

```python
from tensorflow.keras.datasets import imdb

(train_data, train_labels), (test_data, test_labels) = imdb.load_data(num_words = 10000) # 가장 자주 나타나는 단어 1만개만 포함

print(train_data[0]) # 각 리뷰는 단어 인덱스의 리스트
print(train_labels[0]) # 부정 : 0, 긍정 : 1
```

### 2. 데이터 준비
- 신경망에 숫자 리스트를 바로 주입할 수 없다 : 숫자 리스트의 길이가 모두 다르기 때문( = 문장의 길이가 모두 다르기 때문)
- 신경망은 항상 동일한 크기의 배치를 인풋으로 받아야 한다.

- 리스트 -> 텐서 바꾸기
	1. 같은 길이가 되도록 패딩 추가, `(sample, max_length)` 크기의 정수 텐서로 변환
	2. `멀티 - 핫 인코딩`해서 리스트를 0과 1의 벡터로 변환한다.

- 여기선 2번째 방식을 사용함
```python
import numpy as np

def vectorize_sequences(sequences, dimension = 10000):
  results = np.zeros((len(sequences), dimension)) # (리뷰 갯수, 단어 갯수)의 크기를 가지면서 모든 값 0
  for i, sequence in enumerate(sequences):
    for j in sequence:
      results[i, j] = 1. # 특정 인덱스의 위치만 1로 만듦

x_train = vectorize_sequences(train_data)
x_test = vectorize_sequences(test_data)

y_train = np.asarray(train_labels).astype('float32')
y_test = np.asarray(test_labels).astype('float32')
```
- `멀티 - 핫 인코딩`이 뭐냐?
	- 기존 리스트는 숫자들의 연속임
	- 각 리뷰에 대해, 해당 숫자를 가지면 1, 없으면 0을 갖는 배열을 만드는 거임
	- `len(sequences)`는 리뷰 갯수
	- `dimension`은 최대 단어 개수(최초에 10000으로 설정)

### 3. 신경망 모델 만들기
- `Dense`층을 쌓을 때 중요한 구조상의 결정
	1. 얼마나 많은 층을 쓸 것인가
	2. 각 층에 얼마나 많은 유닛을 쓸 것인가

> **층 결정 원리는 5장에서 배운다.**   
> 여기서는 16개 유닛, 2개의 중간층 & 2개로 분류하는 마지막층(유닛 1개)으로 구성

```python
from tensorflow import keras
from tensorflow.keras import layers

model = keras.Sequential([
    layers.Dense(16, activation = 'relu'),
    layers.Dense(16, activation = 'relu'),
    layers.Dense(1, activation = 'sigmoid')
])

model.compile(optimizer = 'rmsprop', loss = 'binary_crossentropy', metrics=['accuracy'])

```
1. 16개의 유닛이 있다 = 가중치 행렬의 크기가 `input_dimension, 16`이라는 뜻.
	- 즉 입력 데이터와 W를 내적하면 입력 데이터가 16차원으로 투영됨.
2. 마지막 층의 `sigmoid` : 0과 1 사이의 점수로, 출력 값을 확률처럼 해석할 수 있다.
3. `rmsprop` : 거의 모든 문제에서 기본 선택으로 좋음
4. 손실 함수 : 이진 분류라 `binary_crossentropy`를 이용했으며 `mean_sqaured_error`도 회귀에 많이 쓰지만 여기서도 사용은 가능하다. 그러나 **확률 문제는 `crossentropy`가 좋음.**

### 4. 검증 데이터 및 훈련
```python
x_val = x_train[:10000]
partial_x_train = x_train[10000:]
y_val = y_train[:10000]
partial_y_train = y_train[10000:]

# 훈련
history = model.fit(partial_x_train, partial_y_train, epochs = 20, batch_size = 512, validation_data = (x_val, y_val))
```

- `history` 객체 확인
```python
# history 속성 확인
history_dict = history.history
history_dict.keys()
```

- 훈련과 검증 손실 그리기
```python
import matplotlib.pyplot as plt
history_dict = history.history
loss_values = history_dict['loss']
val_loss_values = history_dict['val_loss']
epochs = range(1, len(loss_values) + 1)
plt.plot(epochs, loss_values, "bo", label = "Training Loss")
plt.plot(epochs, val_loss_values, "b", label = "Validation Loss")
plt.legend()
plt.show()


# 훈련과 검증 정확도 그리기

acc = history_dict['accuracy']
val_acc = history_dict['val_accuracy']
plt.plot(epochs, acc, "ro", label = "Training accuracy")
plt.plot(epochs, val_acc, "r", label = "Validation accuracy")
plt.legend()
plt.show()
```
![[Pasted image 20230720234103.png]]
- 검증 손실, 검증 정확도가 4번째 이후부턴 오히려 감소하는 현상을 보이는데, 이는 과대적합되었다고 할 수 있다.
- 과대적합을 막는 여러 기술이 있는데 이건 5장에서 다룸.
- 에포크를 4회로 줄여서 다시 실행하고 `model.predict(x_test)`를 돌려본다.


## 2. 뉴스 기사 분류 : 다중 분류
- 레이블이 1개이고, 다중 분류 문제인 `단일 레이블 다중 분류` 모델이 된다.
- 차이점만 서술해놓겠음. 코드는 `readme.md`의 링크에 있는 [코드(4-2)](https://github.com/dowrave/TIL/tree/main/NotObsidian/%EC%BC%80%EB%9D%BC%EC%8A%A4%20%EC%B0%BD%EC%8B%9C%EC%9E%90%EC%97%90%EA%B2%8C%20%EB%B0%B0%EC%9A%B0%EB%8A%94%20%EB%94%A5%EB%9F%AC%EB%8B%9D)를 참고하자.

#### 손실함수 : `categorical_crossentropy`와 `sparse_categorical_crossentropy`의 차이점  
>  - **`label(y)`이 어떤 형태로 오느냐의 차이다.** 적용 알고리즘은 동일함. 인풋만 다름.
> `categorical_crossentropy` : 각 범주가 원핫인코딩된 텐서다. 3개중 3번째 범주면 `[0 0 1]`임.
> `sparse_categorical_crossentropy` : 각 범주는 그냥 숫잣값을 갖는다. 3번째 범주는 `2`

#### 중간층의 유닛은 충분히 커야 한다.
- 마지막 층이 46개인데, 중간층의 유닛이 4 같이 작은 수면 안됨
- 많은 정보를 저차원(64 -> 4)으로 압축하면서 중요한 정보는 압축이 되지만, 손실되는 정보도 커질 수 밖에 없다.




## 3. 주택 가격 예측 : 회귀
- 실습 코드는 역시 [여기의 4-3 참고](https://github.com/dowrave/TIL/tree/main/NotObsidian/%EC%BC%80%EB%9D%BC%EC%8A%A4%20%EC%B0%BD%EC%8B%9C%EC%9E%90%EC%97%90%EA%B2%8C%20%EB%B0%B0%EC%9A%B0%EB%8A%94%20%EB%94%A5%EB%9F%AC%EB%8B%9D)
```python
from tensorflow.keras.datasets import boston_housing

(train_data, train_targets), (test_data, test_targets) = (boston_housing.load_data())

print(train_data.shape)
print(test_data.shape) 

# 타겟의 단위는 1000$임
```


### 1. 표준화
- 정규화라고 써져 있지만 표준화 작업이다. **신경망은 스케일에 예민**한 것을 주의하자
- 넘파이로 쉽게 구현 가능.
```python
mean = train_data.mean(axis = 0) # 각 세로축의 평균 값을 구함
train_data -= mean
std = train_data.std(axis = 0) # 각 세로축의 표준편차를 구함
train_data /= std

test_data -= mean
test_data /= std
```

### 2. 회귀 모델 구성
```python
def build_model():
  model = keras.Sequential([
      layers.Dense(64, activation = 'relu'),
      layers.Dense(64, activation = 'relu'),
      layers.Dense(1),

  ])

  model.compile(optimizer = 'rmsprop', loss = 'mse', metrics = ['mae'])
  return model
```
- 분류 모델과의 2가지 차이점이 있음
1. 마지막에 **활성화 함수 없이 `1`개의 유닛만 있음. 스칼라 회귀 예측** 과정임.
2. 컴파일 : 평균제곱오차와 평균절대오차로 모니터링함
	- 타겟이 1000$ 단위 이므로, mae가 0.5면 평균적으로 예측이 500$ 정도 차이가 난다는 뜻

### 3. K-fold Cross-Validation

- k-fold cv 직접 구현하기
```python
# k-fold cv
# 사실 k-fold cv를 하려면, 데이터세트를 훈련/검증으로 나눔 -> 훈련셋의 평균, 표준편차를 구해서 표준화함 -> 검증셋에도 훈련셋의 평균,표준편차로 표준화함
# 이어야 하며, 이는 사이킷런의 `Pipeline` 클래스로 쉽게 구현할 수 있다.

k = 4
num_val_samples = len(train_data) // k
num_epochs = 100
all_scores = []
for i in range(k):
  print("fold : ", i)

  # 이번 검증 데이터 : k = i
  val_data = train_data[i * num_val_samples : (i + 1) * num_val_samples]
  val_targets = train_targets[i * num_val_samples : (i + 1) * num_val_samples]

  # 원래 데이터에서 일부를 검증으로 뺐으니까 나머지 데이터를 합쳐서 훈련 데이터라고 함
  partial_train_data = np.concatenate(
        [train_data[:i * num_val_samples], train_data[(i+1)* num_val_samples:]], axis = 0
  )
  partial_train_targets = np.concatenate(
        [train_targets[:i * num_val_samples], train_targets[(i+1) * num_val_samples:]], axis = 0
  )

  # k번째 fold 마다 모델을 새로 만듦 : 당연한거임 가중치 초기화해야 하니까
  model = build_model()
  model.fit(partial_train_data, partial_train_targets, epochs = num_epochs, batch_size = 16, verbose = 0) # 출력 X
  val_mse, val_mae = model.evaluate(val_data, val_targets, verbose = 0)
  all_scores.append(val_mae)

# 이들을 평균낸 게 스코어
np.mean(all_scores) # 2.409
```

-