
1. [[#모델의 최대 성능 이끌어내기|모델의 최대 성능 이끌어내기]]
	1. [[#모델의 최대 성능 이끌어내기#하이퍼파라미터 최적화|하이퍼파라미터 최적화]]
		1. [[#하이퍼파라미터 최적화#KerasTuner 사용하기|KerasTuner 사용하기]]
		2. [[#하이퍼파라미터 최적화#올바른 검색 공간을 만드는 기술|올바른 검색 공간을 만드는 기술]]
		3. [[#하이퍼파라미터 최적화#HPO의 미래 : 자동화된 머신 러닝|HPO의 미래 : 자동화된 머신 러닝]]
	2. [[#모델의 최대 성능 이끌어내기#모델 앙상블|모델 앙상블]]
2. [[#대규모 모델 훈련하기|대규모 모델 훈련하기]]
	1. [[#대규모 모델 훈련하기#혼합 정밀도로 GPU 훈련 속도 높이기|혼합 정밀도로 GPU 훈련 속도 높이기]]
		2. [[#혼합 정밀도로 GPU 훈련 속도 높이기#부동 소수점 정밀도 이해하기|부동 소수점 정밀도 이해하기]]
		3. [[#혼합 정밀도로 GPU 훈련 속도 높이기#혼합 정밀도 활성화하기|혼합 정밀도 활성화하기]]
	2. [[#대규모 모델 훈련하기#다중 GPU 훈련|다중 GPU 훈련]]
		1. [[#다중 GPU 훈련#2개 이상의 GPU 활용하기|2개 이상의 GPU 활용하기]]
		2. [[#다중 GPU 훈련#단일 호스트, 다중 장치 동기 훈련|단일 호스트, 다중 장치 동기 훈련]]
	3. [[#대규모 모델 훈련하기#TPU 훈련|TPU 훈련]]
		1. [[#TPU 훈련#코랩에서 TPU 사용하기|코랩에서 TPU 사용하기]]
		2. [[#TPU 훈련#스텝 융합을 활용하여 TPU 활용도 높이기|스텝 융합을 활용하여 TPU 활용도 높이기]]

- 지금까지 훈련한 모델은 작은 규모다 : 작은 데이터셋, 1개의 GPU를 사용한다.
- 최상의 결과를 얻기 위해선, 더 나은 모델 성능을 얻을 필요가 있다.
	- `하이퍼파라미터 튜닝(HyperParameter Tuning)`
	- `모델 앙상블(Model Ensemble)`
	- 다중 GPU, TPU 훈련
	- 혼합 정밀도
	- 클라우드 컴퓨팅 자원 활용

## 모델의 최대 성능 이끌어내기


### 하이퍼파라미터 최적화
- 모델을 만들 때 무작위로 보이는 결정(층 갯수, 유닛 갯수, 활성화 함수, 정규화 층 배치 등)이 있다.
- 일반 `파라미터`는 역전파로 훈련된다면, `하이퍼파라미터Hyperparameter`는 이와 구분된다.
- 하이퍼파라미터에 대한 공식적인 규칙은 없다. 
	- 1번째 선택은 최적치가 아닐 확률이 훨씬 높기에, 옵션을 수정하고 모델을 반복적으로 훈련해야 한다. 
	- 이를 기계에 위임할 수 있다.

- 결정 공간을 자동적, 조직적, 규칙적으로 찾아야 하며, 실제 가장 높은 성능을 내는 구조를 찾아야 한다.

> - 하이퍼파라미터 최적화 과정
> 1. 일련의 하이퍼파라미터를 자동으로 선택
> 2. 선택된 하이퍼파라미터로 모델 생성
> 3. 훈련 데이터 학습, 검증 데이터 성능 측정
> 4. 다음 시도 하이퍼파라미터 자동으로 선택
> 5. 1~4를 반복
> 6. 테스트 데이터에서 성능 측정

- 다음에 시도할 하이퍼파라미터를 선택하는 알고리즘이 핵심이다.
	- `베이즈 최적화Bayesian Optimization`
	- `유전 알고리즘Genetic Algorithm`
	- `랜덤 탐색Random Search`

- 손실 함수를 이용하는 가중치 훈련과 달리, 하이퍼파라미터 업데이트는 다음 요소들을 생각해야 한다.
	- `분리되어 있는 결정Disrete Decision`들로 채워져 있다. 미분 가능하지 않기 때문에, 경사하강법보다 비효율적인 방법을 사용해야 한다.
	- 최적화 과정의 피드백 신호를 계산하는 비용이 많이 든다. 모델을 아예 재훈련해야 하기 때문이다.
	- 피드백 신호는 잡음이 많을 수 있다. 
		- 성능이 0.2% 정도 더 좋다면, 더 좋은 모델이기 때문일까? 아니면 초기화가 우연히 잘 됐기 때문일까?

- 케라스에서는 `KerasTuner`를 지원한다.

#### KerasTuner 사용하기
```
!pip install keras-tuner -q
```

- 케라스 튜너를 사용하면 하이퍼파라미터 값을 아래처럼 바꿀 수 있다.
```python
units = 32 # 기존
Int(name = 'units', min_value = 16, max_value = 64, step = 16) # 선택 범위
```
- 이러한 선택 집합을 하이퍼파라미터 튜닝 과정의 `탐색 공간Search Space`라고 부른다.

- 모델 구축
```python
# 모델 구축 함수 정의
from tensorflow import keras
from tensorflow.keras import layers

def build_model(hp):
  units = hp.Int(name = 'units', min_value = 16, max_value = 64, step = 16)
  model = keras.Sequential([
      layers.Dense(units, activation = 'relu'),
      layers.Dense(10, activation = 'softmax')
  ])

  optimizer = hp.Choice(name = 'optimizer', values = ['rmsprop', 'adam'])
  model.compile(optimizer = optimizer, 
                loss = 'sparse_categorical_crossentropy', 
                metrics = ['accuracy'])
  return model
```

- 모델 구축 : 모듈화 & 설정하기 쉽게 만들기 : `HyperModel` 상속
```python
# 더 쉽게 만들기  : HyperModel 클래스 상속

import keras_tuner as kt

class SimpleMLP(kt.HyperModel):
  def __init__(self, num_classes):
    self.num_classes = num_classes

  def build(self, hp):
    units = hp.Int(name = 'units', min_value = 16, max_value = 64, step = 16)
    model = keras.Sequential([
        layers.Dense(units, activation = 'relu'),
        layers.Dense(self.num_classes, activation = 'softmax')
    ])
    optimizer = hp.Choice(name = 'optimizer', values = ['rmsprop', 'adam'])
    model.compile(optimizer = optimizer, 
                  loss = 'sparse_categorical_crossentropy', 
                  metrics = ['accuracy'])
    return model

hypermodel = SimpleMLP(num_classes = 10)
```

- 튜너 정의
- 튜너는 아래 과정을 반복하는 `for` 루프로 생각할 수 있다.
	- 하이퍼파라미터 값 선택
	- 값으로 모델 구축 함수를 호출, 모델 생성
	- 모델 훈련 및 결과 기록
- `RandomSearch, BayesianOptimization, HyperBand`가 있다. 여기선 BO를 씀.
```python
# 튜너 정의하기
tuner = kt.BayesianOptimization(
    hypermodel, # 모델 구축 함수 or HyperModel 클래스 객체
    objective = 'val_accuracy', # 튜너가 최적화할 지표 : 항상 검증 지표를 지정하자!
    max_trials = 100, # 하이퍼파라미터 탐색 공간에서 선택하는 횟수
    executions_per_trial = 2, # 같은 하이퍼파라미터로 훈련하는 횟수. 검증은 평균값을 이용함.
    directory = 'mnist_kt_test', # 탐색 로그 저장 위치

    overwrite = True  # 디렉토리 데이터를 덮어쓸 것인가 
    # 모델 구축 함수를 수정했다면 True, 같은 모델 구축 함수로 탐색을 잇는다면 False
)

# tuner.search_space_summary() 로 탐색 공간의 요약 저
```

- `objective`의 경우, `accuracy` 같은 내장 지표라면 `KerasTuner`가 최대화, 최소화의 방향을 추정한다.
	- 사용자 정의 지표는 직접 지정해야 한다.
```python
objective = kt.Objective(
						 name = 'val_accuracy',
						 direction = 'max' # 지표의 최적화 방향
)
tuner
```

- 탐색 시작
	- 검증 데이터를 전달한다.
	- **테스트 데이터는 하이퍼파라미터 튜닝까지 끝난 결과 모델에 적용해야 함!**
```python
# 탐색 시작
# 검증 데이터를 전달한다. 테스트 데이터는 하이퍼파라미터 튜닝 후에 적용한다.

(x_train, y_train), (x_test, y_test) = keras.datasets.mnist.load_data()
x_train = x_train.reshape((-1, 28 * 28)).astype('float32') / 255
x_test = x_test.reshape((-1, 28 * 28)).astype('float32') / 255

# 나중에 쓰기 위해 따로 보관함
x_train_full = x_train[:]
y_train_full = y_train[:]

num_val_samples = 10000
x_train, x_val= x_train[:-num_val_samples], x_train[-num_val_samples:]
y_train, y_val= y_train[:-num_val_samples], y_train[-num_val_samples:]

# 모델마다 얼마나 많은 에포크가 필요한지 모르기 때문에
# 에포크는 크게 지정하고, 과대적합 시작 전에 훈련을 멈춘다.
callbacks = [
    keras.callbacks.EarlyStopping(monitor = 'val_loss', patience = 5)
]

# fit과 동일한 매개변수를 사용한다.
# 모델의 fit() 메서드마다 이 매개변수들이 전달된다.
tuner.search(x_train,
             y_train,
             batch_size = 128,
             epochs = 100,
             validation_data = (x_val, y_val),
             callbacks = callbacks,
             verbose = 2)
```

- 최상의 하이퍼 파라미터 설정 확인
```python
# 최상의 하이퍼파라미터 설정 확인하기
top_n = 4

best_hps = tuner.get_best_hyperparameters(top_n) 
```
- 튜너의 `get_best_hyperparameters()` 에서 반환된 `HyperParameters` 클래스 객체의 `values` 속성에 튜너가 찾은 최상의 매개변수가 딕셔너리로 저장되어 있다.

- 가장 좋은 하이퍼파라미터를 얻었다면, **검증 세트도 훈련 세트에 포함시켜서 마지막으로 모델을 훈련**시키면 된다.
- 이에 앞서서, `최적의 훈련 에포크 횟수`를 결정해야 한다. 
	- 탐색 과정보다 모델을 더 오래 훈련시킨다.
	- 이를 위해 `EarlyStopping` 콜백 함수의 `patience`의 값을 높게 지정한다.
```python
# 최상의 에포크 찾기
def get_best_epoch(hp):
  model = build_model(hp)
  callbacks = [
      keras.callbacks.EarlyStopping(
          monitor = 'val_loss',
          mode = 'min',
          patience = 10
      )
  ]
  history = model.fit(x_train, y_train,
                      validation_data = (x_val, y_val),
                      epochs = 100,
                      batch_size = 128,
                      callbacks = callbacks)
  val_loss_per_epoch = history.history['val_loss']
  best_epoch = val_loss_per_epoch.index(min(val_loss_per_epoch)) + 1
  print(f"최상의 에포크 : {best_epoch}")
  return best_epoch
```

- 마지막으로 더 많은 데이터에서 훈련한다.
```python
# 최적 모델과 에포크를 얻고, 더 많은 데이터에서 조금 더 오래 훈련한다.
def get_best_trained_model(hp):

  best_epoch = get_best_epoch(hp)
  model = build_model(hp) # 책에는 이게 빠진 듯?

  model.fit(x_train_full, y_train_full,
            batch_size = 128, epochs = int(best_epoch * 1.2))
  return model
```
```python
# 실행 부분
best_models = []
for hp in best_hps:
  model = get_best_trained_model(hp)
  model.evaluate(x_test, y_test)
  best_models.append(model)
```

- 조금 낮은 성능을 걱정하지 않는다면, 튜너로 얻은 최상의 가중치 모델을 바로 로드할 수 있다.
```python
best_models = tuner.get_best_models(top_n)
```

> 비교) 
> 1. 전체 코드 : 최상의 **하이퍼파라미터**를 얻음 -> `hp`를 인풋으로 받는 함수나 `HyperModel` 객체에 이를 넣고 **모델을 만듦** -> 최상의 **에포크 수를 얻음** -> **훈련 + 검증 데이터 모두를 훈련 세트**로 이용하고, **에포크 수도 조금 더 늘려서** **최종 훈련을 진행**함  
> 2. `get_best_models` : 최상의 하이퍼파라미터로 모델을 만들고 끝남.

- 아래 방법은 상대적으로 적은 데이터, 적은 에포크 수로 모델을 만든다.
- 위 방법은 더 많은 데이터, 더 많은 에포크 수로 모델을 만든다. 데이터 수가 성능에 비례하는 편이라고 생각하면 위 방법이 정석인 것 같으니 참고하자.

> 참고) 하이퍼파라미터 대규모 수행 시 중요한 것 중 하나는 **검증 세트 과적합**이다. 동일한 검증 세트로 하이퍼파라미터를 업데이트하므로, 검증 데이터에 대한 훈련이 되는 셈이다.  
> `Dacon` 때 `Public` 5위인데 `Private` 26위로 떨어진 걸 생각해보자. 

#### 올바른 검색 공간을 만드는 기술
- 하이퍼파라미터는 최고의 모델을 얻거나 경연 대회에서 우승하기 위한 강력한 도구이다.
- 그러나 하이퍼파라미터 튜닝으로 **모델 아키텍처의 모범 사례를 대체할 수는 없다.**
	- HPO에 모든 걸 맡기기엔 비용이 너무 크기 때문이다.
	- HPO의 목적은 자동화이지 마법이 아니다.
	- **잠재적인 실험 설정은 직접 골라야 한다.**
- 하이퍼파라미터 튜닝을 사용해서 설정에 대한 결정을 `미시적`에서 `거시적`으로 바꿀 수 있다.
	- `미시적` : 층의 개수 몇 개? 유닛 개수 몇 개?
	- `거시적` : 이 모델에 잔차 연결을 해야 하는가?
- `미시적` 결정은 특정 모델이나 데이터셋마다 다르지만, `거시적` 결정은 여러 작업과 데이터셋에 걸쳐 일반화가 더 잘된다.
	- 모든 이미지 분류 문제는 같은 종류의 탐색 공간 템플릿으로 풀 수 있다.

- 위 논리를 따라 `KerasTuner` 는 넓은 범위를 가진 문제에 관련된 `사전 정의 탐색 공간Premade Search Space`을 제공한다.
	- `kt.applications.HyperXception`
	- `kt.applications.HyperResNet`
	- `kt.applications.HyperEfficientNet`
	- `kt.applications.HyperImageAugment`

#### HPO의 미래 : 자동화된 머신 러닝
- 현재 딥러닝 엔지니어로서의 대부분의 일은
	- 파이썬 스크립트로 데이터 정리
	- DNN 아키텍처와 하이퍼파라미터를 오래 튜닝 -> 작동하는 모델 만들기
위 과정은 최선이 아니다. 자동화가 도움이 될 수 있는데, 이는 하이퍼 파라미터 튜닝에 그치지 않는다.

- 강화학습이나 유전 알고리즘으로, 사전에 거는 제약 없이 모델 아키텍처를 생성할 수 있다. 
- 미래에는 엔드-투-엔드 머신 러닝 파이프라인이 자동으로 생성될 것이다.
- 이를 `AutoML`이라고 한다.
	- 이미 `AutoKeras` 같은 라이브러리를 활용하면 수동 작업을 거의 거치지 않고 기초적인 머신러닝 문제를 풀 수 있다.
	- `AutoML`은 아직 초창기이고 대규모 문제에 적용하기 어렵다. 그러나 성숙해지면 머신 러닝 엔지니어의 직업이 사라지는 게 아니라 `데이터 큐레이션, 비즈니스 목표를 반영한 복잡한 손실 함수 생성, 모델이 배포 생태계에 미치는 영향 이해하기` 등에 더 많은 노력을 기울일 수 있다.

### 모델 앙상블
- `모델 앙상블Model Ensemble`은 가장 좋은 결과를 얻을 수 있는 또다른 강력한 기법이다.
	- 여러 모델의 예측을 합쳐서 더 좋은 예측을 만든다.
	- **아주 뛰어난 단일 모델보다 성능이 좋다.**

- 가정) 독립적으로 훈련된 다른 종류의 모델이, 각기 다른 장점을 가지고 있다.
	- 각 모델은 예측을 위해 서로 다른 특징을 바라본다.
	- 이 특징들을 모으면 데이터를 훨씬 정확하게 묘사할 수 있다는 아이디어.

```python
preds_a = model_a.predict(x_val)
preds_b = model_b.predict(x_val)
preds_c = model_c.predict(x_val)
preds_d = model_d.predict(x_val)
final_preds = 0.25 * (preds_a + preds_b + preds_c + preds_d) # 초기 예측보다 정확해야 함
```

- 단, 앙상블은 **분류기들이 비슷하게 좋을 때 잘 작동**한다. 한 모델이 다른 것보다 성능이 월등히 나쁘면 최종 예측은 앙상블 내의 가장 좋은 분류기보다 못할 수 있다.

- 그래서 더 나은 방법은 **검증 데이터의 가중치를 사용해 가중 평균**하는 것이다.
	- 좋은 분류기에 높은 가중치를 주자는 아이디어!
	- 좋은 가중치를 찾기 위해 `랜덤 서치`나 `넬더-미드Nelder-Mead 알고리즘`같은 간단한 최적화 알고리즘을 적용할 수 있다. 
```python
preds_a = model_a.predict(x_val)
preds_b = model_b.predict(x_val)
preds_c = model_c.predict(x_val)
preds_d = model_d.predict(x_val)
final_preds = 0.5 * preds_a + 0.25 * preds_b + 0.1 * preds_c + 0.15 *preds_d
```

- 앙상블이 잘 작동하는 힘은, **분류기의 다양성**이다. 
- 따라서 최대한 다르면서 좋은 모델을 앙상블해야 한다.
	- 랜덤 초기화만 다르게 한 같은 네트워크를 앙상블하는 건 가치가 없다.
- 실전에서 잘 동작하는 방법은, `트리 기반 모델 : RF나 Gradient Boosting Tree`이나 `심층 신경망`을 앙상블하는 것이다. 

## 대규모 모델 훈련하기
- 발전의 반복 루프 : `아이디어 -> (케라스) -> 실험 -> (GPU, TPU) -> 결과 -> (텐서보드) -> 아이디어 -> (...)`
	1. 케라스 API에 대한 전문 지식이 쌓이면 실험을 위한 코딩 속도가 사이클의 병목이 되지 않을 수 있다. 
	2. 다음 병목은 모델 훈련 속도인데, `빠른 훈련 인프라`는 10~15분 내에 결과를 얻을 수 있다.

- 이번 장에서는 **빠르게 훈련할 수 있는 3가지 방법**을 배운다.
	1. 1개의 GPU를 쓸 수 있는 `혼합 정밀도Mixed-Precision` 훈련.
	2. 여러 개의 GPU를 사용한 훈련
	3. TPU를 사용한 훈련

### 혼합 정밀도로 GPU 훈련 속도 높이기
- 공짜로 거의 모든 모델의 훈련 속도를 3배까지 높일 수 있다.
- `혼합 정밀도 훈련Mixed-Precision Training`이 그것인데, 우선 컴퓨터 과학의 `정밀도` 개념을 보자.

#### 부동 소수점 정밀도 이해하기
- `정밀도`란 숫자의 해상도에 해당한다.
	- `uint8`에서 `00000000` = 0 , `11111111` = 255이다.
	- 255 이상의 수를 나타내려면 더 많은 `비트`가 필요하다.

- 부동 소수점 숫자를 보자. 실수는 연속적인데, 두 숫자 사이에 무한한 포인트가 있다.
- 그러나 컴퓨터에서는 무한한 포인트를 구현할 수 없다. 즉, **두 숫자 사이에 유한한 포인트**가 있는데, 이 `유한한 포인트의 갯수`는 숫자를 저장하는 데 사용하는 `비트 개수`에 따라 다르다.
	- `float16` : 숫자를 16비트에 저장하는 `반정밀도half precision`
	- `float32` : 숫자를 32비트에 저장하는 `단정밀도single precision`
	- `float64` : 숫자를 64비트에 저장하는 `배정밀도double precision`

---
> 참고) 부동 소수점에서 표현할 수 있는 숫자의 개수
>**모든 $N$에 대해, $2^N$과 $2^{N+1}$ 사이에서 표현할 수 있는 숫자의 개수는, 1과 2 사이에서 표현할 수 있는 숫자의 개수와 동일하다.**  
>이는 부동 소수점이 `부호, 유효값(가수Mantissa), 지수` 3가지 부분으로 인코딩 되어있기 때문인데, **$2^N$ 과 $2^{N+1}$ 사이 값의 개수는 `가수` 부분의 비트 수로 결정되기 때문이다.**

- 원주율 파이의 근삿값을 `float32`로 인코딩하면 아래와 같다.
```python
0  \  10000000  \  10010010000111111011011
부호(1)  지수(8)          가수(23)
# 1 * (2 ** (128 - 127)) * (1.5707963705062866) = 3.141592...
```
- `지수` : $2^N$에서 $N$ 값이 지수. 지수 부분을 정수라고 표현할 수 있겠다.
- `가수` : 소수점 밑 부분을 표현함.

> 그리고 이렇게 얘기하면 `Unsigned`가 왜 2배의 값을 표현할 수 있는지도 명확해진다. 부호 비트를 다른 쪽으로 돌리면 되니까.
---
- 부동 소수점의 `해상도` 개념을, 안전하게 처리할 수 있는 두 숫자 사이의 `최소 거리`라는 관점으로 생각할 수 있다. 
	- 단정밀도는 `1e-7`
	- 배정밀도는 `1e-16`
	- 반정밀도는 `1e-3`이다.
- 이 책의 모든 모델은 단정밀도 : `float32`를 사용한다. 
	- 정보를 잃지 않고, 정방향 패스와 역방향 패스를 실행하는 데 충분하다.
	- 특히, 그래디언트 업데이트가 작으면 더 두드러진다.
		- 일반적인 학습률 크기는 `1e-3`, 가중치 업데이트는 `1e-6` 크기가 일반적이다.
- `float64`는 낭비이다. 비용은 많이 들지만, 큰 이점은 없다.
- 그렇지만 `float16`은 같은 작업을 수행할 수 없다. 경사하강법 과정이 부드럽게 진행되지 못한다.
	- `1e-3`은 `1e-6`을 반영하지 못하기 때문에

- 이 때 하이브리드 방식인 `혼합 정밀도`를 선택할 수 있다.
	- 중요하지 않은 곳에 `float16`을 쓰고
	- 중요한 곳, 수치적인 안정성이 필요한 경우에 `float32`을 쓴다.
- 최신 GPU와 TPU는 `float16`을 더 빠르게 수행하고 메모리를 덜 사용하는 특수 하드웨어를 장착하고 있다. 가능할 때마다 낮은 정밀도 연산을 사용해서 큰 폭으로 훈련 속도를 높일 수 있다.
	- 모델이 정밀도에 민감하다면, `float32`만 써서 품질을 유지할 수 있다.


- **`float32`는 케라스, 텐서플로우의 기본 부동 소수점 타입이지만, 넘파이 배열의 기본값은 `float64`임에 주의하자**
```python
# 배열 -> 텐서 변환 시 float64 텐서가 만들어지니 주의하자
import tensorflow as tf
import numpy as np
np_array = np.zeros((2, 2))
tf_tensor = tf.convert_to_tensor(np_array)
tf_tensor.dtype # float64

# 따라서 넘파이 배열 변환 시 명시적으로 데이터 타입을 지정해준다.
np_array = np.zeros((2, 2))
tf_tensor = tf.convert_to_tensor(np_array, dtype = 'float32')
tf_tensor.dtype # tf.float32

# 참고) 넘파이 배열로 fit 메서드 호출 시 자동 변환되긴 함
```

#### 혼합 정밀도 활성화하기
```python
from tensorflow import keras
keras.mixed_precision.set_global_policy("mixed_float16")
```
- 정방향 패스는 대부분 `float16`으로 수행된다.
	- `softmax`, `crossentropy` 등은 수치적으로 불안정해서 `float32`
- 가중치는 `float32`로 저장되고 업데이트 된다.

- 케라스 층에는 `variable_dtype`과 `compute_dtype`이 있다. 
	- 기본적으로 `float32`로 설정되어 있는데, 혼합 정밀도를 활성화하면 `compute_dtype` 속성이 `float16`으로 바뀐다.
	- 한편 `variable_dtype`은 `float32`로 유지되는 것을 확인할 수 있다.

- 활성화 후, 특정 층에서 혼합 정밀도를 사용하지 않으려면 층의 생성자에 `dtype = 'float32'`를 넣는다.

### 다중 GPU 훈련
- 딥러닝 모델은 갈수록 커지고 더 많은 계산 자원을 필요로 한다. 이 때, `다중 GPU 분산 훈련`을 할 수 있다.

- 연산 분산 방법은 크게 2가지이다.
	- `데이터 병렬화data parallelization`
	- `모델 병렬화model parallelization`

- `데이터 병렬화` : 1개의 모델이 여러 장치, 머신에 복제되며, 모델의 복제본은 각각 다른 데이터 배치를 처리한 뒤 결과를 합친다.
- `모델 병렬화` : 1개의 모델의 각기 다른 부분이 여러 장치에서 실행되며, 동시에 1개의 데이터 배치를 처리한다.
	- 태생적으로 병렬 구조를 지원하는 모델에 맞다. 여러 `브랜치`가 있는 모델처럼.
	- 1개의 장치에서 실행하기에는 너무 큰 모델에서만 실행한다.

- 여기선 대부분 쓰게 될 `데이터 병렬화`를 다룬다.

#### 2개 이상의 GPU 활용하기
- 아래 방법 중 하나를 쓸 수 있어야 한다.
	- (최선 아님)2~4개의 GPU를 한 컴퓨터에 설치하고, CUDA 드라이버 & cuDNN을 설치한다.
	- 구글 클라우드, Azure, AWS에서 다중 GPU 가상 머신을 임대한다. 연중무휴로 모델을 훈련하지 않는 경우 가장 좋은 방법.
		- 이들을 다루는 방법은 자주 바뀌고 온라인에서 쉽게 찾을 수 있어서 다루지 않음.

- VM 인스턴스를 직접 관리하고 싶지 않다면, [텐서플로우 클라우드](https://github.com/tensorflow/cloud)를 이용할 수 있다. 코랩 노트북 첫 부분에 코드를 추가, 다중 GPU에서 훈련을 시작할 수 있다.

#### 단일 호스트, 다중 장치 동기 훈련
```python
strategy = tf.distribute.MirroredStrategy() # 분산 전략 객체 : 디폴트값임
print(f"장치 개수 : {strategy.num_replicas_in_sync}")

with strategy.scope(): # 모든 변수는 이 코드블록 안에서 만들어진다. 모델 구축과 compile까지.
	model = get_compiled_model()
model.fit(train_dataset,
		 epochs=100,
		 validation_data = val_dataset,
		 callbacks = callbacks)
```
- `단일 호스트 다중 장치 동기 훈련Single-host Multi-device Synchronous Training`이며, `미러링된 분산 전략Mirrored Distribution Strategy`라고도 한다. 
- `단일 호스트` : 여러 GPU가 한 머신에 설치되어 있다는 뜻
	- <->`클러스터` : 각자 GPU를 가진 머신들이 네트워크로 통신하는 구조
- `동기 훈련` : GPU에 복제된 모델들의 상태가 모두 동일하다는 뜻. 

- `with` 문 내에서 모델을 만들 때, `Mirrored_Strategy` 객체가 가능한 GPU마다 복제 모델을 1개씩 만든다. 이후 훈련의 각 스텝은 아래처럼 전개된다.

1. 데이터셋에서 `글로벌 배치Global Batch`가 추출된다.
2. 이를 4개의 `서브 배치(로컬 배치)Local Batch`로 분할한다.
	- 글로벌 배치가 512개라면, 서브 배치는 128개씩 4개를 갖는다.
	- 글로벌 배치는 일반적으로 매우 커야 한다.
3. 4개의 복제 모델 각각은 1개의 로컬 배치를 독립적으로 처리한다. 각 복제 모델은 이전 가중치의 그래디언트가 주어졌을 때, 모델의 가중치 변수 업데이트 정도를 나타내는 가중치 `델타delta`를 만든다.
4. `델타`는 4개의 복제모델로부터 수집되어 `글로벌 델타`를 만든다. 이 `글로벌 델타`가 모든 모델에 적용되는데, 매 스텝의 끝에서 수행되므로 복제 모델들은 항상 동기화된 상태다. 즉 가중치가 항상 같다.

- 이상적으론 N개의 GPU에서 N배 빨라져야 하지만, 분산 처리에는 약간의 `오버헤드Overhead`가 있다. 특히 가중치 `델타`를 합치는 과정에서 그렇다.
	- 2개는 2배
	- 4개는 3.8배
	- 8개는 7.3배 빨라짐

>`tf.data`성능 팁
> 1. 분산 훈련 수행 시 항상 `tf.data.Dataset` 객체로 데이터를 전달하자. 넘파이 배열로 전달해도 `fit()`에서 `Dataset` 객체로 변환되므로 괜찮다.  
> 2. `데이터 프리페칭Prefetching`을 활용하자. `fit()`에 데이터셋을 전달하기 전, `dataset.prefetch(buffer_size)`를 호출할 수 있다. 버퍼 사이즈를 못 고르겠다면 자동으로 골라주는 `dataset.prefetch(tf.data.AUTOTUNE)`을 이용할 수 있다.

### TPU 훈련
- `TPU` : 딥러닝 워크플로우를 위해 특별히 설계된 하드웨어. 구글 크라우드와 코랩에서 사용 가능.
	- `NVIDIA P100 GPU`보다 15배 빠르며, 평균적으로도 `GPU`보다 3배 이상 비용 효율적이다.

#### 코랩에서 TPU 사용하기
- 코랩에서는 `8 코어 TPU`를 무료로 사용할 수 있다. 
- `코랩 런타임 메뉴`의 `런타임 유형 변경`에 `TPU`를 선택할 수 있게 되어 있는데, **TPU는 모델을 만들기 전에 추가적인 단계가 필요하다.**

```PYTHON
import tensorflow as tf

tpu = tf.distribute.cluster_resolver.TPUClusterResolver.connect()
print("장치 : ", tpu.master())
```
- 그냥 TPU에 연결하는 거니까 무지성복붙하면 된다. 

- 또한, TPU를 사용하려면 분산 전략`TPUStrategy with` 문으로 블록을 감싸야 한다.
- 위에서 다룬 `MirroredStrategy`와 동일한 템플릿을 따른다 : 코어마다 모델이 복제되고, 모든 복제 모델은 동기화를 유지한다.

```python
import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import layers

tpu = tf.distribute.cluster_resolver.TPUClusterResolver.connect()
print("장치 : ", tpu.master())

strategy = tf.distribute.TPUStrategy(tpu)
print("복제 모델 개수 : ", strategy.num_replicas_in_sync)

def build_model(input_size):
  inputs = keras.Input((input_size, input_size, 3))
  x = keras.applications.resnet.preprocess_input(inputs)
  x = keras.applications.resnet.ResNet50(weights = None, include_top = False,
                                         pooling = 'max')(x)
  outputs = layers.Dense(10, activation = 'softmax')(x)
  model = keras.Model(inputs, outputs)
  model.compile(optimizer = 'rmsprop',
                loss = 'sparse_categorical_crossentropy',
                metrics = ['accuracy']
                )
  return model

with strategy.scope():
  model = build_model(input_size = 32)
```

- 코랩의 TPU에는 VM이 2개이다.
- 노트북 런타임을 호스팅하는 VM은, TPU가 있는 VM과 다르다. 
- 따라서, **로컬 디스크(노트북이 호스팅된 VM에 링크된 디스크)에 저장된 파일에서 훈련할 수 없다.** TPU 런타임이 디스크를 읽을 수 없기 떄문인데, 데이터를 로딩하는 2가지 방법이 있다.
	1. **VM의 메모리에 있는 데이터로 훈련**한다. 데이터가 넘파이 배열이라면 이미 하고 있는 것.
	2. **데이터를 GCS 버킷에 저장하고, 로컬로 내려받지 않고 바로 버킷에서 읽어들이는 데이터셋을 만든다.**
		- `TPU` 런타임은 GCS에서 데이터를 읽을수 있다. 데이터를 모두 로드하기에는 너무 크다면, 이게 유일한 방법이다.

- 아래 코드는 `1.` 방법으로 `CIFAR10` 데이터셋을 훈련한다.
```python
(x_train, y_train), (x_test, y_test) = keras.datasets.cifar10.load_data()
model.fit(x_train, y_train, batch_size = 1024) # TPU 훈련 시 다중 GPU 훈련처럼 배치 크기가 충분히 커야 한다.
```
- 1번째 에포크 시작에 시간이 걸린다. TPU가 실행할 수 있는 형태로 모델을 컴파일하기 때문이며, 이게 끝나면 엄청 빨라짐.

> 주의) I/O 병목 주의  
> TPU가 배치 데이터를 매우 빠르게 처리할 수 있기 때문에, GCS에서의 읽는 속도가 병목이 될 수 있다.  
> 1. 데이터셋이 충분히 작다면 VM 메모리에 적재한다. `dataset.cache()`를 이용하면 GCS에서 1번만 데이터를 읽는다.
> 2. 너무 크다면, 빠르게 로드할 수 있는 바이너리 저장 포맷인 `TFRecord` 파일로 저장한다. [keras.io](https://keras.io/examples/keras_recipes/creating_tfrecords)에 예제가 있다. 

#### 스텝 융합을 활용하여 TPU 활용도 높이기
- TPU 코어를 잘 활용하려면 매우 큰 배치로 훈련해야 한다. 작은 모델은 배치 크기가 너무 커질 수 있다.
- 배치 크기가 너무 커진다면, **옵티마이저의 학습률을 높여야 한다.** 가중치의 업데이트가 더 적어지지만 정확해지기도 한다.
- TPU를 최대한 활용하며 합리적인 크기의 배치를 유지하는 트릭으로, `스텝 융합Step Fusing`이 있다. 
	- TPU 실행 스텝마다 여러 훈련 스텝을 실행한다. 
	- 이를 위해서 `compile(steps_per_execution = 8)`을 지정하면 된다.
		- 위 예제의 경우 TPU 실행마다 훈련 스텝을 8번 실행한다.
- TPU를 **최대로 활용하지 못하는 작은 모델이라면, 위처럼 지정하면 실행 속도를 극적으로 향상**시킬 수 있다.