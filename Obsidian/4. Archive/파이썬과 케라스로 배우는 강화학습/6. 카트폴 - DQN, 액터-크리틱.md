1. [[#알고리즘 1. DQN|알고리즘 1. DQN]]
	1. [[#알고리즘 1. DQN#카트폴 예제|카트폴 예제]]
	2. [[#알고리즘 1. DQN#DQN 이론|DQN 이론]]
	3. [[#알고리즘 1. DQN#DQN 코드 설명|DQN 코드 설명]]
2. [[#알고리즘 2 : 액터-크리틱|알고리즘 2 : 액터-크리틱]]
	1. [[#알고리즘 2 : 액터-크리틱#코드로 구현|코드로 구현]]
	2. [[#알고리즘 2 : 액터-크리틱#실행 및 결과|실행 및 결과]]


`OpenAI`는 `짐`이라는 환경을 제공, 강화학습을 적용할 수 있는 여러 환경을 제공한다. 그 중 가장 기본적인 예제인 `카트폴`에 여러 강화 학습 알고리즘을 적용하려 한다. 
딥마인드 논문에서 소개된 `DQN 알고리즘`은 딥살사와 달리 큐러닝의 `큐함수 업데이트` 방법을 사용하며, 이를 가능하게 하기 위해 `경험 리플레이`를 사용한다.
`REINFORCE 알고리즘`의 발전된 형태인 `액터-크리틱`은 REINFORCE 알고리즘을 에피소드가 아닌 타임스텝 단위로 학습할 수 있게 해준다.

## 알고리즘 1. DQN

### 카트폴 예제
`AG Barto`의 논문에 소개되어 있다. 
검은 사각형이 `카트Cart`, 황색 막대가 `폴Pole`이다. 카트는 수평선으로 자유롭게 오가며, 폴은 카트에 핀으로 연결되어 있고 핀을 축으로 자유롭게 회전할 수 있다.
에이전트는 카트에 **왼쪽이나 오른쪽으로 일정한 크기의 힘**을 가할 수 있다. 예제는 힘의 크기가 정해져 있고, 에이전트가 할 일은 **폴이 쓰러지지 않도록 카트를 움직이는 것**이다.
`OpenAI GYM`에서는 폴을 5초 동안 세우는 것이 목표로, 폴이 일정 각도 이상으로 떨어지거나 화면을 벗어나면 에피소드가 종료된다. 따라서 일정 각도 이상으로 떨어지지 않게 하고, 화면에서도 벗어나지 않게 해야 한다.  

---
- 에이전트가 이용할 수 있는 정보는 4가지이다. 이 4가지가 에이전트의 상태를 구성한다. 
	- 카트의 수평선의 위치 $x$
	- 속도 $\dot x$
	- 폴의 수직선으로부터 기운 각도 $\theta$
	- 폴의 각속도 $\dot \theta$
- 모두 `float` 자료형을 쓰며, 실질적으로 **상태는 테이블로 만들어 학습할 수 없다.**
- 가치 기반 강화학습으로 학습시키려 한다면, 큐함수를 근사하는 함수를 사용해야 한다. 이 때, 큐함수를 인공신경망으로 근사할 수 있다. 

### DQN 이론
그리드월드 예제에서는 `딥살사`를 온폴리시 알고리즘인 `살사`를 이용하여 학습했다. 오프폴리시 알고리즘인 `큐러닝`과 `인공신경망`을 함께 사용하려면, 딥살사와는 다른 장치가 필요한데, `경험 리플레이Experience Replay`가 그것이다.
`경험 리플레이`라는 아이디어는 에이전트가 환경에서 탐험하며 얻는 샘플 $(s, a, r, s')$ 을 메모리에 저장한다는 것이다. 이 때 저장되는 메모리를 `리플레이 메모리Replay Memory`라고 한다. 에이전트가 학습 시, 리플레이 메모리에서 여러 샘플을 **무작위**로 뽑아서 뽑은 샘플에 대해 인공신경망을 업데이트한다. 이 과정을 매 타임스텝마다 반복한다.
메모리의 크기가 정해져 있기 때문에, 메모리가 꽉 차면 맨 처음에 들어온 샘플부터 삭제한다. 에이전트의 학습이 진행되면서 더 좋은 샘플들이 리플레이 메모리에 저장된다.
경험 리플레이를 이용하면 샘플 간의 상관관계를 없앨 수 있다. `딥살사` 같은 `온폴리시 알고리즘`의 단점이 있는데, 안 좋은 상황에 빠지면 그 상황에 맞게만 학습한다는 것이다. 매 타임스텝마다 신경망이 업데이트된다면 안 좋은 상황이 지속되는 경우 인공신경망이 그 방향으로 계속 업데이트되고, 좋은 상황을 학습하지 못할 수 있다. 그러나 `리플레이 메모리`는 **학습에 사용되는 샘플의 시간적인 상관관계가 없기 때문에** 그러한 일이 발생하지 않는다. 
또한 **여러 개의 샘플**로 인공신경망을 업데이트하므로 학습이 **안정적**이다. 여러 데이터에서 그래디언트를 구하면 1개의 데이터에서보다 그래디언트 값 자체의 변화가 줄어 인공신경망이 더 안정적으로 업데이트 될 수 있다. 
경험 리플레이 자체가 지금 에이전트가 경험하고 있는 상황(=현 정책으로부터 발생한 상황)이 아니라 다양한 과거의 상황(=이전의 정책으로 발생한 상황)으로 학습하기 떄문에, 오프폴리시 알고리즘이 적합하다. 따라서 오프폴리시 알고리즘인 `큐러닝`을 `리플레이 메모리`와 함께 사용하는 것이다. 
DQN의 또다른 특징으로 **`타깃 신경망Target Network`을 사용**하는 것이 있다. 에이전트는 매 타임스텝마다 리플레이 메모리에서 샘플을 `Batch`로 추출해서 학습해 사용한다. 

- 큐러닝에서 큐함수를 업데이트하는 식은 아래와 같다.
$$
Q(S_t, A_t) \leftarrow  Q(S_t, A_t) + \alpha(R_{t+1} + \gamma \, \underset{a'}{max} \, Q(S_{t+1}, a') - Q(S_t, A_t))
$$

- 딥살사에서처럼 `DQN`도 오류함수로 `MSE`를 사용한다. 수식은 아래와 같다.
$$
MSE = (정답 - 예측)^2 = (R_{t+1} + \gamma \, \underset{a'}{max}Q(S', a', \theta) - Q(s, a, \theta))^2
$$

부트스트랩의 문제점은 업데이트의 목표가 되는 정답이 계속 변한다는 것이다. 인공신경망 자체도 업데이트되면 부트스트랩의 문제가 더 심해지는데, 이를 위해 **정답을 만들어내는 인공신경망을 일정 시간 동안 유지**한다. 타깃신경망을 따로 만들어서 타깃신경망에서 정답에 해당하는 갑슬 구한다. 구한 정답으로 다른 인공신경망을 계속 학습시키고, 타깃 신경망은 일정 시간 간격마다 그 인공신경망으로 업데이트한다. 

- 오류함수 수식에서는 이를 구분하게 위해 타깃 신경망은 $\theta^-$를 매개변수로 갖는 것으로 표현하고, 인공신경망은 $\theta$를 매개변수로 갖는 것으로 표현한다.
$$
MSE = (정답 - 예측)^2 = (R_{t+1} + \gamma \, \underset{a'}{max}Q(S_{t+1}, a', \theta^-) - Q(s_t, A_t, \theta))^2
$$

논문에서 소개된 DQN 알고리즘은 아타리 게임에 적용된 것으로, 화면을 입력으로 받아야 한다. 따라서 `CNN`을 사용하는데, 카트폴은 화면으로 학습하지 않기 때문에 `딥살사`처럼 간단한 인공신경망을 사용한다. 

### DQN 코드 설명

```python
import env

# main 함수

	# 1. 환경 가져오기(OpenAI의 GYM이라는 라이브러리)
    env = gym.make('CartPole-v1')
    
    # 2. 상태와 행동의 크기를 환경 객체에서 가져옴
    state_size = env.observation_space.shape[0]
    action_size = env.action_space.n

    # 3. 에이전트 객체를 생성
    agent = DQNAgent(state_size, action_size)

```

> 에이전트가 환경과 어떻게 상호작용하는가 정리
> 1. 상태에 따른 행동 선택
> 2. 선택한 행동으로 한 타임스텝 진행
> 3. 환경으로부터 다음 상태와 보상을 받음
> 4. $(s, a, r, s')$ 샘플을 리플레이 메모리에 저장
> 5. 리플레이 메모리에서 무작위 추출한 샘플로 학습
> 6. 에피소드마다 타깃 모델 업데이트 

> 1. 상태에 따른 행동 선택
**상태를 입력으로 받고, 큐함수를 출력**으로 내보내는 모델이 필요하다. 
```python
    # 상태가 입력, 큐함수가 출력인 인공신경망 생성
    def build_model(self):
        model = Sequential()
        model.add(Dense(24, input_dim=self.state_size, activation='relu',
                        kernel_initializer='he_uniform'))
        model.add(Dense(24, activation='relu',
                        kernel_initializer='he_uniform'))
        model.add(Dense(self.action_size, activation='linear',
                        kernel_initializer='he_uniform'))
        model.summary()
        model.compile(loss='mse', optimizer=Adam(lr=self.learning_rate))
        return model
```
- 여기서 살펴볼 것은 3가지이다. 
	1. `state_size`를 입력으로 받고, `action_size`를 출력으로 넣는다.
	2. `kernel_initializer` : 가중치 초기화 옵션으로, DQN에서는 `he_uniform` 옵션을 사용한다. 
		- he_uniform이란 평균이 0이고 분산이 2 / (입력 뉴런 수) 인 균등 분포를 따르는 초기화 옵션이다. 특히 `ReLU`를 활성화 함수로 사용하는 층에서 효과적이라고 알려져 있다.
	3. `model.summary()` : 행동의 개수는 `좌, 우` 2개의 노드가 있다. 학습 가능한 파라미터 수는 770개이다.
- `DQN`이 타깃 신경망을 사용하기 떄문에 `build_model`은 2번 호출된다. 그러나 가중치 초기화는 분포를 따르는 무작위 값이기 때문에 두 모델의 초기화 가중치가 반드시 같다고 할 수 없는데, 따라서 학습 시작 전 두 모델의 가중치 값을 통일해야 한다. 이를 실해하는 함수가 `update_target_model` 함수이다.
```python
	# 생성자 부분
        # 모델과 타깃 모델 생성
        self.model = self.build_model()
        self.target_model = self.build_model()

        # 타깃 모델 초기화
        self.update_target_model()

	# ...
	
    # 타깃 모델을 모델의 가중치로 업데이트
    def update_target_model(self):
        self.target_model.set_weights(self.model.get_weights())
```
- `update_target_model`은 학습 도중에도 `target_model`을 `model`로 업데이트하는 역할을 한다. 

위처럼 생성한 모델로 에이전트는 행동을 선택할 수 있다. 행동은 `입실론-탐욕 정책`으로 선택하며, `get_action` 함수가 그 역할을 한다.
```python
    # 입실론 탐욕 정책으로 행동 선택
    def get_action(self, state):
        if np.random.rand() <= self.epsilon:
            return random.randrange(self.action_size)
        else:
            q_value = self.model.predict(state)
            return np.argmax(q_value[0])
```
- 특이사항으로, $\varepsilon$ 값은 처음에는 1을 가진다. 즉 에이전트는 무조건 무작위 선택을 한다. 그러다가 학습이 진행되면서 모델 예측에 따라 행동을 선택하게 하기 위해 `epsilon`을 줄인다. 너무 작은 값이 되지 않게 하기 위해 최솟값인 `epsilon_min`이 정의되어 있으며, 이는 `train_model`에 구현되어 있다.
```python
    def train_model(self):
        if self.epsilon > self.epsilon_min:
            self.epsilon *= self.epsilon_decay
```

> 4. 샘플을 리플레이 메모리에 저장
`get_action`에서 한 타임스텝 진행하면 에이전트는 1개의 샘플을 얻는다. 이를 메모리에 저장하기 위해, `DQNAgent` 초기화 단계에서 아래의 코드로 설정한다. `deque`를 사용해 일정 크기를 갖는 메모리를 생성한다. 샘플은 `append_sample` 함수로 저장된다.
```python
	# __init__
        self.memory = deque(maxlen=2000)

	# ....
	
    # 샘플 <s, a, r, s'>을 리플레이 메모리에 저장
    def append_sample(self, state, action, reward, next_state, done):
        self.memory.append((state, action, reward, next_state, done))
```
- `append_sample`은 매 타임스텝마다 경험한 것을 샘플로 메모리에 저장하고, 이 메모리를 다시 매 타임스텝마다 학습에 사용한다. `DQN`에서는 배치로 사용하며, `train_model` 함수는 `batch_size`만큼의 샘플을 무작위로 뽑아서 학습한다. 

메인함수에서 `train_model` 함수는 메모리에 데이터가 여러 `batch_size`로 쪼개서 넣을 수 있을 만큼 많아질 때까지 기다렸다가 실행된다. 
```python
            # 매 타임스텝마다 학습
            if len(agent.memory) >= agent.train_start:
                agent.train_model()
```
- 참고로 `train_start`는 1000, `batch_size`는 64로 설정되어 있다

`train_model` 함수는 아래와 같다. 
```python
    # 리플레이 메모리에서 무작위로 추출한 배치로 모델 학습
    def train_model(self):
        if self.epsilon > self.epsilon_min:
            self.epsilon *= self.epsilon_decay

        # 메모리에서 배치 크기만큼 무작위로 샘플 추출
        mini_batch = random.sample(self.memory, self.batch_size)

        states = np.zeros((self.batch_size, self.state_size))
        next_states = np.zeros((self.batch_size, self.state_size))
        actions, rewards, dones = [], [], []

        for i in range(self.batch_size):
            states[i] = mini_batch[i][0]
            actions.append(mini_batch[i][1])
            rewards.append(mini_batch[i][2])
            next_states[i] = mini_batch[i][3]
            dones.append(mini_batch[i][4])

        # 현재 상태에 대한 모델의 큐함수
        # 다음 상태에 대한 타깃 모델의 큐함수
        target = self.model.predict(states)
        target_val = self.target_model.predict(next_states)

        # 벨만 최적 방정식을 이용한 업데이트 타깃
        for i in range(self.batch_size):
            if dones[i]:
                target[i][actions[i]] = rewards[i]
            else:
                target[i][actions[i]] = rewards[i] + self.discount_factor * (
                    np.amax(target_val[i]))

        self.model.fit(states, target, batch_size=self.batch_size,
                       epochs=1, verbose=0)
```
- `model.fit`에 들어가는 `states`는 배치 상태로 들어가야 한다. 따라서 `np.zeros()`로 배치 형태로 지정한다. 다음 사태의 배치도 마찬가지.
- `targets`은 큐함수의 배치이므로, `target[i]`는 행동의 갯수만큼 원소를 가진다. 모든 원소가 업데이트의 목표가 되지 않고, `i`번째 샘플의 행동 `action[i]`에 해당하는 원소만 업데이트 목표가 된다. 
- 모델은 MSE 오류함수로 업데이트 되므로, 목표함수가 되는 `target[i]`의 원소를 제외하면 모두 예측값과 동일하게 만들어야 한다. 
$$
MSE = (정답 - 예측)^2 = (R_{t+1} + \gamma \, \underset{a'}{max}Q(S', a', \theta) - Q(s, a, \theta))^2
$$

따라서 `target`을 예측 값으로 일단 구한 뒤
```python
        # 현재 상태에 대한 모델의 큐함수
        # 다음 상태에 대한 타깃 모델의 큐함수
        target = self.model.predict(states)
        target_val = self.target_model.predict(next_states)
```

업데이트의 목표가 되는 원소만 $R_{t+1} + \gamma \, \underset{a'}{max}Q(S', a', \theta)$ 으로 바꾸는 식으로 코드가 구성되어 있다.
```python
        # 벨만 최적 방정식을 이용한 업데이트 타깃
        for i in range(self.batch_size):
            if dones[i]:
                target[i][actions[i]] = rewards[i]
            else:
                target[i][actions[i]] = rewards[i] + self.discount_factor * (
                    np.amax(target_val[i]))
```

DQN에서 중요한 부분 중 하나가 업데이트 타깃을 구하는 부분이다. 정답에 해당하는 업데이트 타깃을 구할 때 다음 상태의 큐함수 중 최댓값을 이용한다. 다음 상태의 큐함수는 타깃 모델을 통해 나온 값을 사용하므로, `target_model.predict(next_state)`를 통해 $Q(S_{t+1}, a', \theta^-)$를 구한다. 

`states`와 `target`을 구하면 두 배치로 모델을 업데이트할 수 있다. 강화학습에서 `epochs = 1`로 설정한다.

위 과정까지가 `1 ~ 6`번 과정이며 메인 루프는 아래처럼 구성되어 있다.
```python
if __name__ == "__main__":
    # CartPole-v1 환경, 최대 타임스텝 수가 500
    env = gym.make('CartPole-v1')
    state_size = env.observation_space.shape[0]
    action_size = env.action_space.n

    # DQN 에이전트 생성
    agent = DQNAgent(state_size, action_size)

    scores, episodes = [], []

    for e in range(EPISODES):
        done = False
        score = 0
        # env 초기화
        state = env.reset()
        state = np.reshape(state, [1, state_size])

        while not done:
            if agent.render:
                env.render()

            # 현재 상태로 행동을 선택
            action = agent.get_action(state)
            # 선택한 행동으로 환경에서 한 타임스텝 진행
            next_state, reward, done, info = env.step(action)
            next_state = np.reshape(next_state, [1, state_size])
            # 에피소드가 중간에 끝나면 -100 보상
            reward = reward if not done or score == 499 else -100

            # 리플레이 메모리에 샘플 <s, a, r, s'> 저장
            agent.append_sample(state, action, reward, next_state, done)
            # 매 타임스텝마다 학습
            if len(agent.memory) >= agent.train_start:
                agent.train_model()

            score += reward
            state = next_state

            if done:
                # 각 에피소드마다 타깃 모델을 모델의 가중치로 업데이트
                agent.update_target_model()

                score = score if score == 500 else score + 100
                # 에피소드마다 학습 결과 출력
                scores.append(score)
                episodes.append(e)
                pylab.plot(episodes, scores, 'b')
                pylab.savefig("./save_graph/cartpole_dqn.png")
                print("episode:", e, "  score:", score, "  memory length:",
                      len(agent.memory), "  epsilon:", agent.epsilon)

                # 이전 10개 에피소드의 점수 평균이 490보다 크면 학습 중단
                if np.mean(scores[-min(10, len(scores)):]) > 490:
                    agent.model.save_weights("./save_model/cartpole_dqn.h5")
                    sys.exit()
```
- 카트폴에서는 보상이 플레이 중 매 타임스텝마다 +1 씩 주어진다. 그러나 예제의 핵심은 쓰러지게 하는 행동을 하지 않게 하는 것이다. 따라서 500 타임스텝을 채우지 못하고 에피소드가 끝나면, -100의 보상을 주게 한다.
- 강화학습에서는 학습 종료 시점을 정해놓는 것이 좋다. 이 코드에서는 최근 10개 에피소드 도안의 점수 평균이 490을 넘으면 학습을 중지하고, `sys.exit()`을 통해 코드를 종료한다. 

## 알고리즘 2 : 액터-크리틱

REINFORCE 알고리즘은 에피소드마다만 학습할 수 있었고, 에피소드가 길면 특정 상태에 대한 반환값의 변화가 커지는, 즉 분산이 커지는 경향이 있었다. 위 단점은 학습을 느리게 만든다.  
매 타임스텝마다 학습할 수 있도록 한 것이 `액터-크리틱 알고리즘`이다. `Reinforcement Learning : Introduction` 이라는 책에 소개되어 있다.
액터-크리틱은 REINFORCE 알고리즘의 단점을 해결하고자 DP의 `정책 이터레이션 구조`를 사용했다. 이를 폴리시 그래디언트에 적용하면..

- 정책 이터레이션 : 가치함수에 대한 탐욕 정책 -> 정책 발전
- 폴리시 그래디언트 : 정책 신경망 업데이트 -> 정책 발전

그렇다면 정책 평가는 어떨까? 
- 정책 이터레이션 : DP -> 정책에 대한 가치함수 얻기

폴리시 그래디언트의 정책 신경망 업데이트 식은 아래와 같다.
$$
\theta_{t+1} = \theta_t + \alpha \nabla_\theta J(\theta) \approx \theta_t + \alpha[\nabla_\theta \, log\pi_\theta (a|s) q_\pi(s, a)]
$$

여기서 $q_\pi(s, a)$ 를 반환값 $G_\pi$로 대체한 식이 REINFORCE 알고리즘으로 아래와 같았다.
$$
\theta_{t+1} \approx \theta_t + \alpha[\nabla_\theta \, log\pi_\theta (a|s) G_t]
$$

반환값을 사용하지 않고 큐함수를 근사하는 방법이 있다. `큐함수 또한 근사하는 방법`으로, 정책을 인공신경망으로 근사하듯이 **인공신경망을 하나 더 만들어서 큐함수 또한 근사**할 수 있다. 이를 `가치신경망`이라고 하는데, 정책을 평가한다는 이유로 `크리틱Critic`이라는 이름이 붙었다.

정책 이터레이션과 액터-크리틱을 비교하면 아래와 같다.
-  정책 이터레이션
	- 정책 평가
	- 정책 발전
- 액터-크리틱
	- 크리틱(가치신경망)
	- 정책 신경망의 업데이트

가치신경망의 가중치를 w라고 할때, 액터-크리틱의 업데이트 수식은 아래와 같다.
$$
\theta_{t+1} \approx \theta_t + \alpha[\nabla_\theta \, log\pi_\theta (a|s) Q_\pi(s, a)]
$$

REINFORCE에서 오류함수는 `크로스 엔트로피 X 반환값` 이었다. 딥살사, DQN과 비교하면 그 둘은 MSE 오류함수를 쓰므로 큐함수 -> 오류함수가 아니라 MSE 식을 통해 정답과 예측의 차이가 오류함수의 값으로 쓰였다.
그러나 `액터-크리틱`에서는 오류함수가 아래와 같다.
$$
오류함수 = 정책 \, 신경망 \, 출력의 \, 크로스\, 엔트로피 \, \times \, 큐함수(가치신경망 \, 출력) 
$$
큐함수의 변화를 줄여주기 위해 `베이스라인Baseline`을 사용한다. **특정 상태에서 행동에 따른 값이 변하지 않아서** 액터 크리틱에서 이용한다. 가치함수는 특정 상태에서 하나의 값을 가지기 때문이다. 

가치함수 또한 근사해야 하며, `v`라는 새로운 변수를 도입한다. 가치함수를 베이스라인으로 큐함수에서 빼준 것을 `어드밴티지Advantage`함수라고 하고 아래 수식으로 정의한다.
$$
A(S_t, A_t) = Q_w(S_t, A_t) - V_v(S_t)
$$

위 수식에서는 `큐함수`와 베이스라인인 `가치함수`를 따로 근사하기 때문에 **비효율적**이다. 따라서 큐함수를 가치함수를 사용해 **가치함수의 근사만으로 어드밴티지 함수를 정의**할 수 있다. `시간 차 에러`와 형태가 같아서 $\delta_v$로 정의한다.
$$
\delta_v = R_{t+1} + \gamma \, V_v(S_{t+1} - V_v(S_t))
$$

어드밴티지 함수를 사용한 액터-크리틱의 업데이트 식
$$
\theta_{t+1} \approx \theta_t + \alpha[\nabla_\theta \, log \pi_\theta (a|s) \delta_v]
$$

한편, 가치신경망의 학습은 시간차 오류를 통해 진행한다. 현상태와 다음 상태 및 보상으로 MSE 오류함수를 계산, 이 오류함수를 최소화하는 방향으로 업데이트를 진행한다.
$$
MSE = (정답 - 예측)^2 = (R_{t+1} + \gamma V_v(S_{t+1}) - V_v(S_t))^2
$$

- 요약
액터 크리틱은 2개의 신경망을 가진다. `정책 신경망`은 정책을 근사하며, `가치 신경망`은 가치함수를 근사한다.  
카트폴 문제에서 입력인 사태는 4개의 원소를 가지며, 행동은 2가지가 있기 떄문에 두 신경망의 구조는 아래와 같다. 
> 1. `정책신경망` : 입력(4개의 상태)을 받아 출력(2개의 행동 확률)을 이용해 `크로스엔트로피 오류함수`를 계산함
> 2. `가치신경망` : 입력(4개의 상태)을 받아 출력(1개의 가치함수)을 이용해 `시간차에러`를 구함
> 3. `가치신경망`은 시간차 에러를 이용해 딥살사와 같은 방식으로 신경망을 업데이트 함
> 4. `정책신경망`은 **크로스엔트로피 오류함수와 시간차 에러의 곱**을 이용해 새로운 오류 함수를 정의하고, 이 오류함수로 정책 신경망을 업데이트함.

액터-크리틱이 어드밴티지를 사용하기 때문에 `A2C : Advantage Actor-Critic`이라고도 한다.

### 코드로 구현
- A2C 에이전트와 환경과의 상호작용은
> 1. 정책신경망으로 확률적으로 행동 선택
> 2. 선택한 행동으로 환경에서 한 타임스텝 진행
> 3. 환경으로부터 다음 상태와 보상을 받음
> 4. 샘플 $(s, a, r, s')$ 을 통해 시간차 에러를 구하고 어드밴티지 함수를 구함
> 5. 시간차 에러로 가치 신겨망을, 어드벤티지 함수로 정책신경망을 업데이트.

우선 `정책신경망actor`과 `가치신경망critic`이 필요하다. 2개의 네트워크를 업데이트하므로 정책신경망을 조금 더 간단하게 구성한다.
```python
    # actor: 상태를 받아 각 행동의 확률을 계산
    def build_actor(self):
        actor = Sequential()
        actor.add(Dense(24, input_dim=self.state_size, activation='relu',
                        kernel_initializer='he_uniform'))
        actor.add(Dense(self.action_size, activation='softmax',
                        kernel_initializer='he_uniform'))
        actor.summary()
        return actor

    # critic: 상태를 받아서 상태의 가치를 계산
    def build_critic(self):
        critic = Sequential()
        critic.add(Dense(24, input_dim=self.state_size, activation='relu',
                         kernel_initializer='he_uniform'))
        critic.add(Dense(24, input_dim=self.state_size, activation='relu',
                         kernel_initializer='he_uniform'))
        critic.add(Dense(self.value_size, activation='linear',
                         kernel_initializer='he_uniform'))
        critic.summary()
        return critic
```
- `actor`의 출력은 각 행동을 할 확률이므로 `softmax`, `critic`은 `linear`로 받음에 유의하자.

위 코드를 통해 생성자에서 생성되고, `get_action`으로 특정 상태에 대해 확률적으로 행동을 선택할 수 있다. REINFORCE와 함수 내용이 동일하다.
```python
	# 생성자
        self.actor = self.build_actor()
        self.critic = self.build_critic()
	
	# ...
	
    def get_action(self, state):
        policy = self.actor.predict(state, batch_size=1).flatten()
        return np.random.choice(self.action_size, 1, p=policy)[0]
```
- `action`을 진행하면 에이전트는 환경에서 action으로 한 스텝 진행한다. 환경은 에이전트에게 다음 상태와 보상 정보를 알려주며, 에이전트는 `action`이 좋은지 여부를 알 수 있다. 환경으로부터 얻는 정보로 에이전트는 `actor`와 `critic`을 업데이트한다. 

모델을 업데이트하는 함수는 `train_model`이다. 업데이트 형태를 만드는 함수는 `actor_optimizer`와 `critic_otpimizer`이다. 케라스가 `actor`에서 사용하는 오류 함수를 지원하지 않기 때문에, 직접 만들어서 사용한다.  

`actor_optimizer` 함수는 아래와 같다.
```python
    # 정책신경망을 업데이트하는 함수
    def actor_optimizer(self):
        action = K.placeholder(shape=[None, self.action_size])
        advantage = K.placeholder(shape=[None, ])

        action_prob = K.sum(action * self.actor.output, axis=1)
        cross_entropy = K.log(action_prob) * advantage
        loss = -K.sum(cross_entropy)

        optimizer = Adam(lr=self.actor_lr)
        updates = optimizer.get_updates(self.actor.trainable_weights, [], loss)
        train = K.function([self.actor.input, action, advantage], [],
                           updates=updates)
        return train
```
- 그리드월드의 `reinforce_agent.py`와 다른 점은 `advantage`를 사용한다는 점으로, `train_model` 함수에서 구해진다.

- `actor`의 오류 함수는 크로스 엔트로피와 어드밴티지 함수의 곱이다. actor의 업데이트 식은 다음과 같다.
$$
\begin{matrix}
\theta_{t+1} &\approx& \theta_t + \alpha[\nabla_\theta \, log\pi_\theta(a|s)\delta_v ] \\ &\approx& \theta_t + \alpha \nabla_\theta[log\pi_\theta(a|s) \delta_v]
\end{matrix}
$$
시간차 에러인 $\delta_v$값이 $\theta$의 함수가 아니므로 아래 수식으로 고쳐 쓸 수 있다. 결국 오류 함수는 $log\pi_\theta(a|s) \delta_v$ 이다. 즉, `actor 예측` 값에 로그를 취한 값과, 어드밴티지 함수의 크로스 엔트로피가 `actor`의 오류함수가 된다.  
따라서 다음 코드로 `actor`의 업데이트를 위한 오류 함수를 구할 수 있다.
```python
        cross_entropy = K.log(action_prob) * advantage
        loss = -K.sum(cross_entropy)
```
- `action_prob`는 에이전트가 실제로 취한 행동에 대한 정책의 출력값이다. 

가치신경망을 업데이트하는 `critic_optimizer`는 다음과 같다.
```python
    # 가치신경망을 업데이트하는 함수
    def critic_optimizer(self):
        target = K.placeholder(shape=[None, ])
		
		# MSE 계산
        loss = K.mean(K.square(target - self.critic.output))

        optimizer = Adam(lr=self.critic_lr)
        updates = optimizer.get_updates(self.critic.trainable_weights, [], loss)
        train = K.function([self.critic.input, target], [], updates=updates)

        return train
```
- `target`은 `train_model`에서 구한다. 

위 2개의 `optimizer`를 이용해 업데이트하는 함수는 `train_model`이다. $(s, a, r, s')$을 통해 가치신경망의 업데이트 목표인 target을 계산하고 정책신경망의 업데이트에 사용되는 어드밴티지 함수 `advantage`를 계산한다.
```python

    # 각 타임스텝마다 정책신경망과 가치신경망을 업데이트
    def train_model(self, state, action, reward, next_state, done):
        value = self.critic.predict(state)[0]
        next_value = self.critic.predict(next_state)[0]

        act = np.zeros([1, self.action_size])
        act[0][action] = 1

        # 벨만 기대 방정식를 이용한 어드벤티지와 업데이트 타깃
        if done:
            advantage = reward - value
            target = [reward]
        else:
            advantage = (reward + self.discount_factor * next_value) - value
            target = reward + self.discount_factor * next_value

        self.actor_updater([state, act, advantage])
        self.critic_updater([state, target])

```
- `critic`의 오류함수는 MSE로, 수식으로 표현하면 아래와 같다.
$$
MSE = (정답 - 예측 )^2 = (R_{t+1} + \gamma V_v(S_{t+1} - V_v(S_t)))^2
$$

- `target`과 `advantage`를 구하면 `actor_updater` 및 `critic_updater`의 인수로 넣어서 정책신경망과 가치신경망을 업데이트한다.

---
### 실행 및 결과
- 액터 크리틱은 REINFORCE 대비 매 타임스텝 학습을 할 수 있다는 장점이 있다. 
- 대신, 부트스트랩에 또 다른 근사함수인 크리틱을 사용했기 때문에 **편향이 쉽다.**
- 또한 `온폴리시`학습이기 떄문에 현재 경험 중인 샘플에 초점이 맞춰져서 **에이전트의 현 상화에 따라 업데이트가 계속 달라진다.**
아래 2가지의 장점은 `액터-크리틱` 알고리즘의 학습을 어렵게 한다. 

- 액터 크리틱은`DQN` 대비 좀 더 부드러운 학습 곡선을 그린다. 두 알고리즘 모두 스텝마다 학습되지만, `DQN`은 리플레이 메모리로 배치를 이용하기 때문에 더 빨리 학습된다.

한편, REINFORCE 알고리즘과도 비교해볼 수 있다.
- REINFORCE는 정책 평가 기준으로 반환값을 사용한다. 반환값은 값의 변화가 크기 떄문에 전반적인 학습 그래프의 편차가 큰 경향이 있다.
- 액터-크리틱은 정책 평가 기준으로 `크리틱`을 사용하므로, 값의 변동이 상대적으로 작고, REINFORCE보다 문제를 더 빨리 풀어낸다.


결국 `DQN`의 학습 속도가 가장 빠르지만, 폴리시 그래디언트 알고리즘(`REINFORCE`, `액터 크리틱`)이 더 안정적으로 학습한다. 
- DQN은 무작위 배치로 추출해서 학습하므로, 같은 에피소드 내에 더 많은 학습을 한다고 할 수 있다. 그리고 다양한 시간의 샘플로 학습하므로 치우치거나 제대로 학습하지 못하는 현상이 없다.
- 한편, `액터-크리틱`은 크리틱 업데이트에 살사 방식을 사용하므로 리플레이 메모리처럼 현 정책이 아닌 다른 정책에 의해 생성된 샘플로 학습할 수 없다. 현재로서는 학습 속도와 샘플 사이의 연관성 때문에 학습 성능이 떨어지는데, 이를 극복한 것이 `A3C : Asynchronous Advantage Actor-Critic`이다. 