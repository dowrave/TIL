

`그리드월드`, `카트폴` 예제에서는 상태를 정의하고 이를 입력으로 사용했지만, **상태 정의에는 전문적인 지식이 필요한 경우가 많다.** 하지만 딥러닝은 환경에서 나오는 정보를 그대로 입력으로 집어넣었을 때 알아서 그 정보에서 필요한 내용을 추려낸다. 
즉, 게임 화면으로부터 학습할 수도 있다. 이게 딥마인드의 `DQN`이라는 알고리즘이다. 이번 장에서는 아타리 사의 브레이크아웃이라는 게임에서, DQN과 A3C을 브레이크아웃에 적용, 강화학습 + 딥러닝으로 어떻게 학습이 이뤄지는지를 볼 수 있다. 

## 브레이크아웃 DQN

### 아타리 브레이크아웃
- `아타리Atari` : 1972년 설립, `퐁Pong`을 개발했다. 퐁은 탁구같은 느낌인데, 이를 변형해서 무언가를 부수는 게임이 1976년 만들어진 `브레이크아웃Breakout`이다. 
2015년 딥마인드의 DQN 알고리즘이 네이처에 소개되었고, 딥마인드는 브레이크아웃을 강화학습으로 학습시키는 영상을 유튜브에 공개했다. 이후 2016년 `OpenAI`에서 강화학습 환경인 짐을 공개했다. `짐`에는 `브레이크아웃`이 포함되어 있다.

#### 브레이크아웃의 MDP
브레이크아웃에는 여러 버전이 있는데, 이 책에서는 그 중 `BreakoutDeterministic-v4`를 사용한다. 
브레이크아웃에서 **상태는 게임화면**이다. 에이전트는 화면 4개를 연속으로 입력으로 받으며, 화면 4개가 1개의 상태가 된다. 이 때 화면은 2차원 RGB 데이터이다.
에이전트는 3개의 행동을 한다 : 원래 아타리 게임은 행동이 6개지만, OpenAI gym에서는 행동이 제자리, 왼쪽, 오른쪽 3가지이다. 벽돌이 1개씩 깨질 때마다 에이전트는 + 보상을 받으며, 뒤의 벽돌을 깰수록 더 높은 보상을 받는다. 아무 것도 깨지 못한 평소에는 0의 보상을 받으며, 공을 놓치는 경우는 -1의 보상을 받는다. 
에이전트는 5개의 목숨으로 이뤄져 있고, 모두 잃으면 게임오버된다. 에피소드의 진행 여부는 왼쪽 상단에 표시된다, 좌상단의 점수는 누적 보상을 의미하고, 에이전트는 보상 최대화를 목표로 한다.

### CNN
화면을 인풋으로 받기 때문에 CNN을 이용한다. 기존 카트폴 예제에서는 4개의 수치화된 정보를 이용했지만, 대부분 사람은 감각으로 들어오는 정보에 집중한다. 
만약 게임 화면을 인공 신경망의 입력으로 쓴다면, `가로 픽셀수 x 세로 픽셀 수 x 3`의 크기를 가진다. 이 때, 해상도가 210, 160이므로 은닉층 노드 1개가 받는 가중치 수는 100800개이다.  파라미터 수가 많아지면서 학습이 오래 걸리게 된다.
이를 해결하기 위해, 사람이 실제로 어떻게 시각 정보를 처리하는지 알아야 했다. `수용 영역Receptive Field`과 `추상화Abstraction`이다.

- `수용 영역` : 사람의 시신경은 수용 영역을 가진다. **각 시신경은 일부 입력에만 반응한다.**
이미지의 모든 픽셀에 노드를 연결한다는 것은 모든 시각 입력에 대해 반응한다는 것이다. 즉, 실제로 사람이 일부 입력에만 반응하니까 기계도 그렇게 구성하자는 의미이다. 
이를 구현한 것이 `컨볼루션 필터Filter`이다. 필터는 **이미지의 노이즈를 없애거나 어떤 특징을 강조**한다. 필터는 `컨볼루션 연산`을 통해 그림과 합성된다. **필터는 가지고 있는 값에 따라 주는 효과가 달라지므로, 전혀 다른 정보를 전달하고 얻을 수 있다.** 그러나 필터의 종류가 많고, 각 문제에 어떤 필터를 적용할지 결정하는 것도 많은 시간이 걸린다.

이러한 **필터 결정 문제를 자동**으로 만들어주는 것이 `CNN`이다. 기존 `특징 추출 = 필터 결정` 문제는 머신러닝 기법에서 중요했고, 전문성을 필요로 했지만 `CNN`을 이용하면 특징 추출 단계를 사람이 결정하지 않아도 된다. 입력에서 출력으로 곧바로 연결된단든 특징으로, 이러한 학습 방법을 `End-to-End 학습`이라고도 한다. 

즉 기존 인공신경망은
- 인풋 화면 -> 특징 추출 -> 인공신경망 -> 큐함수의 값

였지만, `CNN`은
- 인풋 화면 -> CNN -> 큐함수의 값

으로 구성된다. 

- 컨볼루션 신경망을 이용해 큐함수를 근사하는 네트워크를 `DQN : Deep Q-Network`라고 한다. 

### 브레이크아웃의 컨볼루션 신경망
```python
    # 상태가 입력, 큐함수가 출력인 인공신경망 생성
    def build_model(self):
        model = Sequential()
        model.add(Conv2D(32, (8, 8), strides=(4, 4), activation='relu',
                         input_shape=self.state_size))
        model.add(Conv2D(64, (4, 4), strides=(2, 2), activation='relu'))
        model.add(Conv2D(64, (3, 3), strides=(1, 1), activation='relu'))
        model.add(Flatten())
        model.add(Dense(512, activation='relu'))
        model.add(Dense(self.action_size))
        model.summary()
        return model
```
- 위 층들의 설정 값은 `Human-Level Control Through Deep Reinforcement Learning`의 설정값을 그대로 따른다. 
- 일반적으로 쓰는 CNN과 다른 특이한 점은, **필터의 크기와 Stride 설정**이다. 그런 설명은 없어서 그냥 특이하다고 짚고 넘어감.
- CNN에 대해 이미 알고 있는 것들은 그냥 넘기겠음
- 위 모델의 파라미터 수는 약 168만개로, 학습에 많은 시간이 걸릴 것으로 예상된다.

### DQN 학습 전 준비사항
- `DQN` 알고리즘은 이전 카트폴에서도 썼지만, 원래는 아타리를 위한 알고리즘이다.
추가로 알아야 할 점들이 있다. 
> 1. 에이전트는 게임 화면의 색상을 알 필요는 없다.
> 2. 사용하지 않는 화면을 잘라낸 다음 비율을 조절할 수 있다. 

- 따라서 3채널을 1채널로, 이미지 크기도 210 x 160 -> 84 x 84로 줄일 수 있다.

위 과정을 `전처리Preprocessing`이라고 한다. 책의 예제보다는 새로운 예제에 강화학습을 적용할 때, 들어오는 입력에 대해 전처리를 해야 할 가능성이 높다. 
한편, 전처리를 한 이미지를 바로 모델의 입력으로 넣으면 에이전트는 학습을 제대로 할 수 없다. 이미지 1개로는 게임 화면의 상황을 정확히 모르기 떄문이다. 즉, **공의 움직이는 방향 정보가 필요하므로, 4개의 연속된 이미지를 입력으로 받아들인다.**
하지만 연속된 이미지를 받아도 공의 움직임은 큰 차이가 없어서, `프레임 스킵Frame Skip`이 필요하다. 

> `프레임 스킵 예시`
> 예를 들어 1번부터 30번의 이미지를 받았다고 치면, `1번~4번, 5번~8번, ... `을 선택해야 한다고 생각한다. 하지만 연속된 프레임이 들어오면 1번 ~ 4번의 이미지는 큰 차이가 나지 않는다.
> 따라서, 이미지를 선택할 때 `1~3번 생략`, `4번 선택`, `5~7번 생략`, `8번 선택`, ... 같은 방식으로 선택한다. 즉, 실제로 선택되는 이미지는 `4, 8, 12, 16, ...`이다.
> 따라서 모델에 들어가는 이미지는 `t=0`일 때 `4, 8, 12, 16`, `t=1`일 때 `8, 12, 16, 20`같은 방시기 반복된다.

- `BreakoutDeterministic-v4` 버전은 프레임스킵을 자동으로 해준다. 
- `히스토리History` : 위 예시에서, 선택된 4개의 연속된 화면을 의미한다. `t=0`일 때를 `히스토리 1`, `t=1`일 때를 `히스토리 2`라고 한다. 각 히스토리는 `[84, 84, 4](가로, 세로, 화면 수)`의 크기를 가진다.

- 이후는 카트폴의 DQN 알고리즘과 동일하다. 
	- 샘플 $(s, a, r, s')$ 사이의 관계성을 깨기 위해 리플레이 메모리를 사용해 미니 배치로 모델을 업데이트한다. 
	- 또한, 타깃 모델을 형성해서 업데이트의 타깃에 해당하는 값은 타깃 모델의 출력을 사용한다. 
	- 타깃 모델을 일정 시간마다 업데이트하여 학습의 안정성을 높인다.

딥마인드 논문에 나온 브레이크 아웃 사용 변수는 아래와 같다.

| 변수                    | 값                                |
| ----------------------- | --------------------------------- |
| 미니 배치 크기          | 32                                |
| 리플레이 메모리 크기    | 40만                              |
| 히스토리 길이           | 4프레임                           |
| 타깃 모델 업데이트 주기 | 1만 스텝 당 1번                   |
| 감가율                  | 0.99                              |
| 프레임 스킵             | 4화면 중 1화면 사용               |
| 학습 속도(lr)           | 0.00025                           |
| $\varepsilon$           | 1부터 0.1까지 100만스텝 동안 감소 |
| 학습 시작               | 5만 스텝 후                                  |

### DQN 전체 코드
```python
from keras.layers.convolutional import Conv2D
from keras.layers import Dense, Flatten
from keras.optimizers import RMSprop
from keras.models import Sequential
from skimage.transform import resize
from skimage.color import rgb2gray
from collections import deque
from keras import backend as K
import tensorflow as tf
import numpy as np
import random
import gym

EPISODES = 50000


# 브레이크아웃에서의 DQN 에이전트
class DQNAgent:
    def __init__(self, action_size):
        self.render = False
        self.load_model = False
        # 상태와 행동의 크기 정의
        self.state_size = (84, 84, 4)
        self.action_size = action_size
        # DQN 하이퍼파라미터
        self.epsilon = 1.
        self.epsilon_start, self.epsilon_end = 1.0, 0.1
        self.exploration_steps = 1000000.
        self.epsilon_decay_step = (self.epsilon_start - self.epsilon_end) \
                                  / self.exploration_steps
        self.batch_size = 32
        self.train_start = 50000
        self.update_target_rate = 10000
        self.discount_factor = 0.99
        # 리플레이 메모리, 최대 크기 400000
        self.memory = deque(maxlen=400000)
        self.no_op_steps = 30
        # 모델과 타겟모델을 생성하고 타겟모델 초기화
        self.model = self.build_model()
        self.target_model = self.build_model()
        self.update_target_model()

        self.optimizer = self.optimizer()

        # 텐서보드 설정
        self.sess = tf.InteractiveSession()
        K.set_session(self.sess)

        self.avg_q_max, self.avg_loss = 0, 0
        self.summary_placeholders, self.update_ops, self.summary_op = \
            self.setup_summary()
        self.summary_writer = tf.summary.FileWriter(
            'summary/breakout_dqn', self.sess.graph)
        self.sess.run(tf.global_variables_initializer())

        if self.load_model:
            self.model.load_weights("./save_model/breakout_dqn.h5")

    # Huber Loss를 이용하기 위해 최적화 함수를 직접 정의
    def optimizer(self):
        a = K.placeholder(shape=(None,), dtype='int32')
        y = K.placeholder(shape=(None,), dtype='float32')

        prediction = self.model.output

        a_one_hot = K.one_hot(a, self.action_size)
        q_value = K.sum(prediction * a_one_hot, axis=1)
        error = K.abs(y - q_value)

        quadratic_part = K.clip(error, 0.0, 1.0)
        linear_part = error - quadratic_part
        loss = K.mean(0.5 * K.square(quadratic_part) + linear_part)

        optimizer = RMSprop(lr=0.00025, epsilon=0.01)
        updates = optimizer.get_updates(self.model.trainable_weights, [], loss)
        train = K.function([self.model.input, a, y], [loss], updates=updates)

        return train

    # 상태가 입력, 큐함수가 출력인 인공신경망 생성
    def build_model(self):
        model = Sequential()
        model.add(Conv2D(32, (8, 8), strides=(4, 4), activation='relu',
                         input_shape=self.state_size))
        model.add(Conv2D(64, (4, 4), strides=(2, 2), activation='relu'))
        model.add(Conv2D(64, (3, 3), strides=(1, 1), activation='relu'))
        model.add(Flatten())
        model.add(Dense(512, activation='relu'))
        model.add(Dense(self.action_size))
        model.summary()
        return model

    # 타겟 모델을 모델의 가중치로 업데이트
    def update_target_model(self):
        self.target_model.set_weights(self.model.get_weights())

    # 입실론 탐욕 정책으로 행동 선택
    def get_action(self, history):
        history = np.float32(history / 255.0)
        if np.random.rand() <= self.epsilon:
            return random.randrange(self.action_size)
        else:
            q_value = self.model.predict(history)
            return np.argmax(q_value[0])

    # 샘플 <s, a, r, s'>을 리플레이 메모리에 저장
    def append_sample(self, history, action, reward, next_history, dead):
        self.memory.append((history, action, reward, next_history, dead))

    # 리플레이 메모리에서 무작위로 추출한 배치로 모델 학습
    def train_model(self):
        if self.epsilon > self.epsilon_end:
            self.epsilon -= self.epsilon_decay_step

        mini_batch = random.sample(self.memory, self.batch_size)

        history = np.zeros((self.batch_size, self.state_size[0],
                            self.state_size[1], self.state_size[2]))
        next_history = np.zeros((self.batch_size, self.state_size[0],
                                 self.state_size[1], self.state_size[2]))
        target = np.zeros((self.batch_size,))
        action, reward, dead = [], [], []

        for i in range(self.batch_size):
            history[i] = np.float32(mini_batch[i][0] / 255.)
            next_history[i] = np.float32(mini_batch[i][3] / 255.)
            action.append(mini_batch[i][1])
            reward.append(mini_batch[i][2])
            dead.append(mini_batch[i][4])

        target_value = self.target_model.predict(next_history)

        for i in range(self.batch_size):
            if dead[i]:
                target[i] = reward[i]
            else:
                target[i] = reward[i] + self.discount_factor * \
                                        np.amax(target_value[i])

        loss = self.optimizer([history, action, target])
        self.avg_loss += loss[0]

    # 각 에피소드 당 학습 정보를 기록
    def setup_summary(self):
        episode_total_reward = tf.Variable(0.)
        episode_avg_max_q = tf.Variable(0.)
        episode_duration = tf.Variable(0.)
        episode_avg_loss = tf.Variable(0.)

        tf.summary.scalar('Total Reward/Episode', episode_total_reward)
        tf.summary.scalar('Average Max Q/Episode', episode_avg_max_q)
        tf.summary.scalar('Duration/Episode', episode_duration)
        tf.summary.scalar('Average Loss/Episode', episode_avg_loss)

        summary_vars = [episode_total_reward, episode_avg_max_q,
                        episode_duration, episode_avg_loss]
        summary_placeholders = [tf.placeholder(tf.float32) for _ in
                                range(len(summary_vars))]
        update_ops = [summary_vars[i].assign(summary_placeholders[i]) for i in
                      range(len(summary_vars))]
        summary_op = tf.summary.merge_all()
        return summary_placeholders, update_ops, summary_op


# 학습속도를 높이기 위해 흑백화면으로 전처리
def pre_processing(observe):
    processed_observe = np.uint8(
        resize(rgb2gray(observe), (84, 84), mode='constant') * 255)
    return processed_observe


if __name__ == "__main__":
    # 환경과 DQN 에이전트 생성
    env = gym.make('BreakoutDeterministic-v4')
    agent = DQNAgent(action_size=3)

    scores, episodes, global_step = [], [], 0

    for e in range(EPISODES):
        done = False
        dead = False

        step, score, start_life = 0, 0, 5
        observe = env.reset()

        for _ in range(random.randint(1, agent.no_op_steps)):
            observe, _, _, _ = env.step(1)

        state = pre_processing(observe)
        history = np.stack((state, state, state, state), axis=2)
        history = np.reshape([history], (1, 84, 84, 4))

        while not done:
            if agent.render:
                env.render()
            global_step += 1
            step += 1

            # 바로 전 4개의 상태로 행동을 선택
            action = agent.get_action(history)
            # 1: 정지, 2: 왼쪽, 3: 오른쪽
            if action == 0:
                real_action = 1
            elif action == 1:
                real_action = 2
            else:
                real_action = 3

            # 선택한 행동으로 환경에서 한 타임스텝 진행
            observe, reward, done, info = env.step(real_action)
            # 각 타임스텝마다 상태 전처리
            next_state = pre_processing(observe)
            next_state = np.reshape([next_state], (1, 84, 84, 1))
            next_history = np.append(next_state, history[:, :, :, :3], axis=3)

            agent.avg_q_max += np.amax(
                agent.model.predict(np.float32(history / 255.))[0])

            if start_life > info['ale.lives']:
                dead = True
                start_life = info['ale.lives']

            reward = np.clip(reward, -1., 1.)
            # 샘플 <s, a, r, s'>을 리플레이 메모리에 저장 후 학습
            agent.append_sample(history, action, reward, next_history, dead)

            if len(agent.memory) >= agent.train_start:
                agent.train_model()

            # 일정 시간마다 타겟모델을 모델의 가중치로 업데이트
            if global_step % agent.update_target_rate == 0:
                agent.update_target_model()

            score += reward

            if dead:
                dead = False
            else:
                history = next_history

            if done:
                # 각 에피소드 당 학습 정보를 기록
                if global_step > agent.train_start:
                    stats = [score, agent.avg_q_max / float(step), step,
                             agent.avg_loss / float(step)]
                    for i in range(len(stats)):
                        agent.sess.run(agent.update_ops[i], feed_dict={
                            agent.summary_placeholders[i]: float(stats[i])
                        })
                    summary_str = agent.sess.run(agent.summary_op)
                    agent.summary_writer.add_summary(summary_str, e + 1)

                print("episode:", e, "  score:", score, "  memory length:",
                      len(agent.memory), "  epsilon:", agent.epsilon,
                      "  global_step:", global_step, "  average_q:",
                      agent.avg_q_max / float(step), "  average loss:",
                      agent.avg_loss / float(step))

                agent.avg_q_max, agent.avg_loss = 0, 0

        # 1000 에피소드마다 모델 저장
        if e % 1000 == 0:
            agent.model.save_weights("./save_model/breakout_dqn.h5")

```

### DQN 코드 설명
- 우선, 에이전트와 환경과의 상호작용을 알아보자.
> 1. 상태에 따른 행동 선택
> 2. 선택한 행동으로 환경에서 한 타임스텝 진행
> 3. 환경으로부터 다음 상태와 보상을 받음
> 4. 샘플 $(s, a, r, s')$을 리플레이 메모리에 저장
> 5. 리플레이 메모리에서 무작위로 추출한 32개의 샘플로 학습
> 6. 5만 타임스텝마다 타깃 네트워크 업데이트 

상태에 따른 행동을 선택하려면, 히스토리를 입력으로 받아서 큐함수를 출력하는 모델이 필요하다. 그 이전에, 게임 화면을 전처리할 함수가 필요하다.

전처리는 `pre_processing`이며 1개의 화면만을 입력으로 받는다. `[210, 160, 3] -> [84, 84, 1]`이 된다.
```python
# 학습속도를 높이기 위해 흑백화면으로 전처리
def pre_processing(observe):
    processed_observe = np.uint8(
        resize(rgb2gray(observe), (84, 84), mode='constant') * 255)
    return processed_observe
```

히스토리를 생성하는 코드는 메인 루프에 있다.
```python
        state = pre_processing(observe)
        history = np.stack((state, state, state, state), axis=2)
        history = np.reshape([history], (1, 84, 84, 4))
```
- `np.stack(axis=)`의 경우, 각 이미지가 `[84, 84, 1]`을 갖는데 `1` 부분에 스택을 쌓고 싶다. 따라서 `axis`는 가장 뒤쪽 인덱스를 가리키는 `2`로 지정한다.

한 타임스텝을 진행한 후, 환경은 에이전트에게 새로운 `observe`를 준다. 이를 이용해 새로운 `state`를 채운다.
```python
            # 선택한 행동으로 환경에서 한 타임스텝 진행
            observe, reward, done, info = env.step(real_action)
            # 각 타임스텝마다 상태 전처리
            next_state = pre_processing(observe)
            next_state = np.reshape([next_state], (1, 84, 84, 1))
            next_history = np.append(next_state, history[:, :, :, :3], axis=3)
```

`build_model` : 히스토리를 입력으로 받고 큐함수를 출력하는 모델을 생성한다. 코드 자체는 위에서 한 것과 동일하다. 단, **타깃 모델도 똑같은 형태로 생성하며, 가중치를 통일**해야 한다.
```python
	def __init__(self):
		# ...
		
        # 모델과 타겟모델을 생성하고 타겟모델 초기화
        self.model = self.build_model()
        self.target_model = self.build_model()
        self.update_target_model()
```

모델 생성 후, 에이전트는 모델을 통해 행동을 선택할 수 있다. `model.preict()` 함수를 이용해 출력된 큐함수를 통해 선택하며, 에이전트는 $\varepsilon$ - 탐욕 정책을 통해 행동을 선택한다. 이 때 모델에 들어가는 입력은 `0 ~ 1` 사이의 값이 되도록 한다.
```python
    # 입실론 탐욕 정책으로 행동 선택
    def get_action(self, history):
        history = np.float32(history / 255.0)
        if np.random.rand() <= self.epsilon:
            return random.randrange(self.action_size)
        else:
            q_value = self.model.predict(history)
            return np.argmax(q_value[0])
```

행동을 선택하면 환경은 에이전트에게 `observe`, 보상ㅇ, 에피소드 종료 여부, 남은 목숨에 관한 정보를 준다.
```python
# main 함수
            # 선택한 행동으로 환경에서 한 타임스텝 진행
            observe, reward, done, info = env.step(real_action)
```

5개의 목숨 정보를 참조해야 한다. 목숨 정보는 `info`에 있으며, 현재 에이전트가 목숨을 잃었는지 여부를 아래 코드로 판단할 수 있다.
```python
            if start_life > info['ale.lives']:
                dead = True
                start_life = info['ale.lives']
```

보상의 클립
```python
            reward = np.clip(reward, -1., 1.)
```
- 보상 `reward`는 `1`을 넘을 수 있는데, `1`을 넘는 보상을 전부 1로 만들어주는 함수다. 여러 아타리 게임에 `DQN`을 적용하기 위해 있는 것으로, 생략해도 학습은 된다.

에이전트가 환경에서 한 스텝을 진행하고 정보를 얻으면 리플레이 메모리에 저장한다. 리플레이 메모리에는 현재 히스토리, 행동, 보상, 다음 히스토리, 목숨을 잃었는지 등의 정보를 넣는다.
```python
    # 샘플 <s, a, r, s'>을 리플레이 메모리에 저장
    def append_sample(self, history, action, reward, next_history, dead):
        self.memory.append((history, action, reward, next_history, dead))
```

에이전트는 리플레이 메모리를 이용해 학습을 진행한다. 메모리 사이즈가 5만 이하일 때는 학습하지 않고, 학습을 시작하면 $\varepsilon$을 타임스텝마다 감소시킨다.
```python
    # 리플레이 메모리에서 무작위로 추출한 배치로 모델 학습
    def train_model(self):
        if self.epsilon > self.epsilon_end:
            self.epsilon -= self.epsilon_decay_step
		
		# 미니 배치
        mini_batch = random.sample(self.memory, self.batch_size)

		# 입력과 정답의 배치 형태 지정
        history = np.zeros((self.batch_size, self.state_size[0],
                            self.state_size[1], self.state_size[2]))
        next_history = np.zeros((self.batch_size, self.state_size[0],
                                 self.state_size[1], self.state_size[2]))
        target = np.zeros((self.batch_size,))
        action, reward, dead = [], [], []

		# 무작위 추출한 샘플 넣기
        for i in range(self.batch_size):
            history[i] = np.float32(mini_batch[i][0] / 255.)
            next_history[i] = np.float32(mini_batch[i][3] / 255.)
            action.append(mini_batch[i][1])
            reward.append(mini_batch[i][2])
            dead.append(mini_batch[i][4])

        target_value = self.target_model.predict(next_history)

        for i in range(self.batch_size):
            if dead[i]:
                target[i] = reward[i]
            else:
                target[i] = reward[i] + self.discount_factor * \
                                        np.amax(target_value[i])

        loss = self.optimizer([history, action, target])
        self.avg_loss += loss[0]
```

만약 에이전트가 목숨을 잃었다면, 아래 수식에서 $R_{t+1}$ 만 정답으로 계산한다. 그 외에는 $R_{t+1} + \gamma \underset{a'}{max}Q(S_{t+1}, a',\theta^-)$을 계산해서 `target`에 저장한다. 이 때, 타깃 모델을 사용한다.
$$
MSE = (정답 - 예측)^2 = (R_{t+1} + \gamma \underset{a'}{max}Q(S_{t+1}, a',\theta^-) -Q(S_t, A_t, \theta))^2
$$

```python
        target_value = self.target_model.predict(next_history)

        for i in range(self.batch_size):
            if dead[i]:
                target[i] = reward[i]
            else:
                target[i] = reward[i] + self.discount_factor * \
                                        np.amax(target_value[i])
```

이후, 케라스의 `optimizer`를 사용해 모델을 업데이트해야 한다. `히스토리, 행동, 타깃`을 인수로 넣는다. 오류함수로 MSE를 이용하지 않고 `Huber Loss`라는 오류 함수를 사용하며, 아래처럼 구현한다.
```python
    # Huber Loss를 이용하기 위해 최적화 함수를 직접 정의
    def optimizer(self):
        a = K.placeholder(shape=(None,), dtype='int32')
        y = K.placeholder(shape=(None,), dtype='float32')

        prediction = self.model.output
        
		# 오류함수 부분
        a_one_hot = K.one_hot(a, self.action_size)
        q_value = K.sum(prediction * a_one_hot, axis=1)
        error = K.abs(y - q_value)
        quadratic_part = K.clip(error, 0.0, 1.0)
        linear_part = error - quadratic_part
        loss = K.mean(0.5 * K.square(quadratic_part) + linear_part)

        optimizer = RMSprop(lr=0.00025, epsilon=0.01)
        updates = optimizer.get_updates(self.model.trainable_weights, [], loss)
        train = K.function([self.model.input, a, y], [loss], updates=updates)

        return train
```
- 특수한 형태의 오류함수를 정의하려면 텐서플로우 백엔드를 `K`로 불러와서 사용한다. 이는 `REINFORCE`에서 썼던 방법과 동일하다. 
- `후버 로스`는 `[-1, 1]` 구간에서는 2차함수이며, 그 외의 구간에서는 1차함수인 오류함수이다. 이로 인해 큰 오류에서도 학습이 민감하게 반응하지 않는다는 장점이 있다. 
- 오류함수를 계산하면 이를 최소화하는 방향으로 모델을 업데이트한다.  `RMSprop`을 사용하며, `0.00025`를 사용한다. 각 가중치에 대한 업데이트 값을 `get_updates` 함수로 구한다. 이렇게 구한  `updates`로 가중치 업데이트 함수를 만들어 `train_model` 함수에서 `self.optimizer`를 호출할 때 모델을 업데이트하게 된다.

타깃 모델의 업데이트는 5만번마다 1번 업데이트된다.
```python
            # 일정 시간마다 타겟모델을 모델의 가중치로 업데이트
            if global_step % agent.update_target_rate == 0:
                agent.update_target_model()
```

`no_op_steps`라는 것이 있는데, 처음에 공이 날아오는 지점은 왼쪽 구석 아니면 오른쪽 구석이다. 이로 인해 에이전트는 구석에 붙는 오류를 범할 수 있다. 따라서 **에이전트가 초반에 아무 것도 하지 않는 구간을 무작위로 설정**한다. 무작위 설정의 상한선이 `no_op_steps`로, `30`ㅇ으로 설정한다. `step(1)`은 "아무 것도 하지 않음"을 의미한다.
```python
        for _ in range(random.randint(1, agent.no_op_steps)):
            observe, _, _, _ = env.step(1)
```

### 텐서보드 사용법
- 에이전트가 매 스텝 학습을 하며 타깃 모델을 업데이트한다. 이를 그래프로 보는 과정.

`__init__` 함수 내의 코드
```python
        # 텐서보드 설정
        self.sess = tf.InteractiveSession()
        K.set_session(self.sess)

        self.avg_q_max, self.avg_loss = 0, 0
        self.summary_placeholders, self.update_ops, self.summary_op = \
            self.setup_summary()
        self.summary_writer = tf.summary.FileWriter(
            'summary/breakout_dqn', self.sess.graph)
        self.sess.run(tf.global_variables_initializer())
```

그래프로 그리고자 하는 변수들은 학습 상황을 보여주는 변수다. `에피소드마다 받는 보상, 각 스텝에서의 최고 큐함수의 값의 평균, 에피소드의 길이, 에피소드의 오류함수의 평균값` 등이다.  텐서플로우는 미리 형태를 생성하고 그 형태에 값을 넣고 실행하는 구조로, 미리 변수에 대한 형태를 생성할 필요가 있다. 이를 `setup_summary`에서 할 수 있다.

```python
    # 각 에피소드 당 학습 정보를 기록
    def setup_summary(self):
        episode_total_reward = tf.Variable(0.)
        episode_avg_max_q = tf.Variable(0.)
        episode_duration = tf.Variable(0.)
        episode_avg_loss = tf.Variable(0.)

        tf.summary.scalar('Total Reward/Episode', episode_total_reward)
        tf.summary.scalar('Average Max Q/Episode', episode_avg_max_q)
        tf.summary.scalar('Duration/Episode', episode_duration)
        tf.summary.scalar('Average Loss/Episode', episode_avg_loss)

        summary_vars = [episode_total_reward, episode_avg_max_q,
                        episode_duration, episode_avg_loss]
        summary_placeholders = [tf.placeholder(tf.float32) for _ in
                                range(len(summary_vars))]
        update_ops = [summary_vars[i].assign(summary_placeholders[i]) for i in
                      range(len(summary_vars))]
        summary_op = tf.summary.merge_all()
        return summary_placeholders, update_ops, summary_op
```

- 위 변수들은 `tf.summary.FileWriter`를 통해 `summary/breakout_dqn`에 저장된다. 아래 코드로 변수가 초기화된다.
```python
	def __init__(self):
        self.sess.run(tf.global_variables_initializer())
```

그리고 변수에 해당하는 값들은 에피소드 종료마다 `summary_writer` 함수를 통해 변수들을 `summary`에 저장한다.
```python
# main함수
            if done:
                # 각 에피소드 당 학습 정보를 기록
                if global_step > agent.train_start:
                    stats = [score, agent.avg_q_max / float(step), step,
                             agent.avg_loss / float(step)]
                    for i in range(len(stats)):
                        agent.sess.run(agent.update_ops[i], feed_dict={
                            agent.summary_placeholders[i]: float(stats[i])
                        })
                    summary_str = agent.sess.run(agent.summary_op)
                    agent.summary_writer.add_summary(summary_str, e + 1)
```

텐서보드에서 그래프를 확인하는 방법으로, 우선 서버를 통해 그래프를 그려야 하므로 서버에 접속해야 한다. 따라서 터미널이나 커맨드에서 
```sh
~$ tensorboard --logdir=<파일 경로>
```

를 입력한다. 코드 파일이 있는 위치 기준 `--logdir = summary/breakout_dqn`으로 옵션을 지정하면 서버가 열린다. 브라우저는 `localhost:6006`으로 접속해서 볼 수 있으며, 위에서 지정한 `Average_Loss`, `Average_Max_Q`, `Duration`, `Total_reward`의 그래프를 각각 볼 수 있다.

### 브레이크아웃 DQN 학습 결과
- 참고) 코드는 정상적으로 실행되지 않는다 : gym에서 `BreakoutDeterministic-v4`이 없어서 막힌다. 이외에도 아마 `get_updates`가 없다는 이유로 막힐 것 같다(다른 장에서도 비슷한 문제가 있었음)

깃으로 같이 받은 모델의 경우, GTX 1060과 i5-4690으로 40시간 정도 학습했다. 8000 에피소드임. 
오류함수는 처음에는 감소하다가 이후에는 증가하는 양상을 보인다. 지도학습은 학습이 이뤄지면 오류함수가 감소하지만, **강화학습은 타깃이 없기 때문에 단조로운 감소 양상을 보이진 않는다.**
즉, **강화학습에서는 오류함수만으로 모델이 잘 학습되고 있는지 확인할 수 없다.** 그런데 **에이전트의 목표가 보상의 최대화**이기 때문에, 에피소드 동안 에이전트가 선택했던 상태들의 함수의 최댓값을 평균 낸 `Average_Max_Q`가 현재 에이전트가 잘 학습되고 있는지를 알 수 있다. 실제로 많은 논문에서 이 값으로 학습 상황을 판단한다.

## 브레이크아웃 A3C

### DQN의 한계
DQN의 핵심 아이디어는, 큐러닝을 인공신경망과 사용하기 위해 경험 리플레이를 사용한다는 것이다. 그런데 **샘플 $(s, a, r, s')$은 에이전트가 처한 상황에 따라 많이 변화**한다. 또한, 그때그때의 샘플로 에이전트가 학습한다면 각 샘플끼리의 연관성 때문에 **학습이 이상한 방향**으로 흘러갈 수 있다.
이러한 문제는 **샘플을 많이 모으는 것**으로 해결할 수 있다. 에이전트는 환경으로부터 얻은 샘플을 메모리에 저장하고, 인공신경망을 학습할 때는 메모리에 저장된 샘플로 배치 크기만큼 임의로 추출하여 학습에 사용한다. 이러한 아이디어로 아타리 게임에서 에이전트가 성공적으로 학습했다.

하지만 딥마인드의 DQN에서는 리플레이 메모리로 100만 크기의 메모리를 사용하는데, 컴퓨터의 메모리를 많이 차지하고 느린 학습 속도의 원인이 된다. 또한, 리플레이 메모리로 학습한다 = 이전 정책을 통해 모은 샘플로 학습한다는 것으로, **리플레이 메모리는 오프폴리시 강화학습을 사용해야 한다는 단점**이 있다. DQN은 그 중 큐러닝을 사용한 것이다.

### A3C란?
DQN과 다른 방식으로 이 문제에 접근한 방식으로, `A3C Asynchronous Advantage Actor-Critic` 알고리즘이 있다. 샘플 간의 연관성을 깨기 위해 메모리에 많은 샘플을 쌓는 방식`DQN`이 아니라, **에이전트를 아예 여러 개를 사용하는 것**이다.
샘플을 모으는 각 에이전트는 `액터러너Actor-Learner`라고 한다. 각 액터러너는 **각기 다른 환경에서 학습을 하기 떄문에 각 액터러너가 모으는 샘플은 서로 연관성이 현저히 떨어진다.** 
액터러너가 일정 타임스텝 동안 모은 샘플로 글로벌 신경망을 업데이트하고, 자신을 글로벌 신경망으로 업데이틓나다. 여러 액터러너가 이 과정을 `비동기적Asynchronous`으로 진행하기 때문에 `A3C`라는 이름이 붙는다. 

> A3C를 통해 에이전트가 학습하는 과정은 아래와 같다.
> 1. 글로벌신경망의 생성 + 여러 개의 (환경 + 액터러너) 생성
> 2. 각 액터러너는 일정 타임스텝 동안 자신의 모델로 샘플을 모음
> 3. 일정 타임스텝이 끝나면 각 액터러너는 글로벌 네트워크를 모은 샘플로 업데이트
> 4. 글로벌 신경망을 업데이트한 액터러너는 다시 글로벌 신경망으로 자신을 업데이트

- 어떻게 액터러너를 각 환경에서 플레이시킬 수 있을까? -> **멀티스레딩**이 여기서 나온다.

### 멀티스레딩 소개
- `스레드Thread` : `프로세스Process` 내에서 실행되는 실행 단위다.
`프로그램Program`을 실행하면 먼저 프로세스가 실행된다. 그리고 프로세스 내에서 스레드가 실행된다. **파이썬**에선 프로그래머가 직접적으로 정의하지 않는 이상 **1개의 프로세스가 1개의 스레드를 생성**한다.
그러나 꼭 1개의 프로그램이 1개의 프로세스만을 실행하거나, 1개의 프로세스가 1개의 스레드를 실행해야 하는 것은 아니다. 

- `멀티프로세싱Multi-Processing` : 한 프로그램에서 여러 프로세스를 생성
- `멀티스레딩Multi-Threading` : 한 프로세스에서 여러 개의 스레드를 생성

파이썬은 구조상 멀티스레딩이 효율적이지 않다. 동시성이나 병렬성을 가진 프로그램이 멀티스레딩을 사용할 때 여러 문제를 일으킬 수 있고, 파이썬은 편의를 위해 `GIL : Global Interpreter Lock`이라는 장치를 갖고 있다.
여러 스레드가 메모리에서 같은 정보를 동시에 쓰고 지우면 복잡하고 디버깅하기 어려운 상태가 될 수 있는데, `GIL`은 **여러 스레드가 동시에 실행되는 걸 막고 한 번씩 돌아가면서 처리**하게 해준다.

파이썬에서 멀티스레딩은 `threading`이라는 라이브러리를 이용할 수 있다. 예제 코드에선 이런 식으로 구현되었다.
```python
import threading
import time # ipynb에서 실행 시

class Agent(threading.Thread):
	def __init__(self):
		threading.Thread.__init__(self):
		pass
		
	def run(self):
		for i in range(100):
			print(i)

agents = [Agent() for i in range(8)] # 8개의 에이전트 객체 생성

for agent in agents:
	agent.start()
	time.sleep(0.1) # ipynb에서 실행 시
	
```
- `Agent` 클래스 생성 시 상속을 받으면 `run` 함수 실행 시 `run` 함수가 여러 개의 스레드에서 실행된다.
- 생성자의 `threading.Thread.__init__(self)`는 상속 받은 스레드 클래스를 초기화하는 코드이다.
> 참고) 주피터로 위 코드를 실행하면 순차적으로 실행된다. 멀티스레드 코드가 동작할 때 모든 스레드가 하나의 셀에서 순차적으로 실행되므로 `agent.start()`가 호출될 때까지 다음 셀로 넘어가지 않는다.  시각적으로 확인하려면 `time.sleep()`을 넣어주면, 동시에 같은 번호를 8개씩 출력하면서 다음으로 넘어간다.
> 사실 그럼에도 `.py`파일에서 실행했을 때와 `.ipynb` 파일에서 실행했을 때의 차이는 있다.

### 브레이크아웃 A3C 코드
```python
from skimage.color import rgb2gray
from skimage.transform import resize
from keras.layers import Dense, Flatten, Input
from keras.layers import Conv2D
from keras.optimizers import RMSprop
from keras import backend as K
from keras.models import Model
import tensorflow as tf
import numpy as np
import threading
import random
import time
import gym

# 멀티쓰레딩을 위한 글로벌 변수
global episode
episode = 0
EPISODES = 8000000
# 환경 생성
env_name = "BreakoutDeterministic-v4"


# 브레이크아웃에서의 A3CAgent 클래스(글로벌신경망)
class A3CAgent:
    def __init__(self, action_size):
        # 상태크기와 행동크기를 갖고옴
        self.state_size = (84, 84, 4)
        self.action_size = action_size
        # A3C 하이퍼파라미터
        self.discount_factor = 0.99
        self.no_op_steps = 30
        self.actor_lr = 2.5e-4
        self.critic_lr = 2.5e-4
        # 쓰레드의 갯수
        self.threads = 8

        # 정책신경망과 가치신경망을 생성
        self.actor, self.critic = self.build_model()
        # 정책신경망과 가치신경망을 업데이트하는 함수 생성
        self.optimizer = [self.actor_optimizer(), self.critic_optimizer()]

        # 텐서보드 설정
        self.sess = tf.InteractiveSession()
        K.set_session(self.sess)
        self.sess.run(tf.global_variables_initializer())

        self.summary_placeholders, self.update_ops, self.summary_op = \
            self.setup_summary()
        self.summary_writer = \
            tf.summary.FileWriter('summary/breakout_a3c', self.sess.graph)

    # 쓰레드를 만들어 학습을 하는 함수
    def train(self):
        # 쓰레드 수만큼 Agent 클래스 생성
        agents = [Agent(self.action_size, self.state_size,
                        [self.actor, self.critic], self.sess,
                        self.optimizer, self.discount_factor,
                        [self.summary_op, self.summary_placeholders,
                         self.update_ops, self.summary_writer])
                  for _ in range(self.threads)]

        # 각 쓰레드 시작
        for agent in agents:
            time.sleep(1)
            agent.start()

        # 10분(600초)에 한번씩 모델을 저장
        while True:
            time.sleep(60 * 10)
            self.save_model("./save_model/breakout_a3c")

    # 정책신경망과 가치신경망을 생성
    def build_model(self):
        input = Input(shape=self.state_size)
        conv = Conv2D(16, (8, 8), strides=(4, 4), activation='relu')(input)
        conv = Conv2D(32, (4, 4), strides=(2, 2), activation='relu')(conv)
        conv = Flatten()(conv)
        fc = Dense(256, activation='relu')(conv)

        policy = Dense(self.action_size, activation='softmax')(fc)
        value = Dense(1, activation='linear')(fc)

        actor = Model(inputs=input, outputs=policy)
        critic = Model(inputs=input, outputs=value)

        # 가치와 정책을 예측하는 함수를 만들어냄
        actor._make_predict_function()
        critic._make_predict_function()

        actor.summary()
        critic.summary()

        return actor, critic

    # 정책신경망을 업데이트하는 함수
    def actor_optimizer(self):
        action = K.placeholder(shape=[None, self.action_size])
        advantages = K.placeholder(shape=[None, ])

        policy = self.actor.output

        # 정책 크로스 엔트로피 오류함수
        action_prob = K.sum(action * policy, axis=1)
        cross_entropy = K.log(action_prob + 1e-10) * advantages
        cross_entropy = -K.sum(cross_entropy)

        # 탐색을 지속적으로 하기 위한 엔트로피 오류
        entropy = K.sum(policy * K.log(policy + 1e-10), axis=1)
        entropy = K.sum(entropy)

        # 두 오류함수를 더해 최종 오류함수를 만듬
        loss = cross_entropy + 0.01 * entropy

        optimizer = RMSprop(lr=self.actor_lr, rho=0.99, epsilon=0.01)
        updates = optimizer.get_updates(self.actor.trainable_weights, [],loss)
        train = K.function([self.actor.input, action, advantages],
                           [loss], updates=updates)
        return train

    # 가치신경망을 업데이트하는 함수
    def critic_optimizer(self):
        discounted_prediction = K.placeholder(shape=(None,))

        value = self.critic.output

        # [반환값 - 가치]의 제곱을 오류함수로 함
        loss = K.mean(K.square(discounted_prediction - value))

        optimizer = RMSprop(lr=self.critic_lr, rho=0.99, epsilon=0.01)
        updates = optimizer.get_updates(self.critic.trainable_weights, [],loss)
        train = K.function([self.critic.input, discounted_prediction],
                           [loss], updates=updates)
        return train

    def load_model(self, name):
        self.actor.load_weights(name + "_actor.h5")
        self.critic.load_weights(name + "_critic.h5")

    def save_model(self, name):
        self.actor.save_weights(name + "_actor.h5")
        self.critic.save_weights(name + "_critic.h5")

    # 각 에피소드 당 학습 정보를 기록
    def setup_summary(self):
        episode_total_reward = tf.Variable(0.)
        episode_avg_max_q = tf.Variable(0.)
        episode_duration = tf.Variable(0.)

        tf.summary.scalar('Total Reward/Episode', episode_total_reward)
        tf.summary.scalar('Average Max Prob/Episode', episode_avg_max_q)
        tf.summary.scalar('Duration/Episode', episode_duration)

        summary_vars = [episode_total_reward,
                        episode_avg_max_q,
                        episode_duration]

        summary_placeholders = [tf.placeholder(tf.float32)
                                for _ in range(len(summary_vars))]
        update_ops = [summary_vars[i].assign(summary_placeholders[i])
                      for i in range(len(summary_vars))]
        summary_op = tf.summary.merge_all()
        return summary_placeholders, update_ops, summary_op


# 액터러너 클래스(쓰레드)
class Agent(threading.Thread):
    def __init__(self, action_size, state_size, model, sess,
                 optimizer, discount_factor, summary_ops):
        threading.Thread.__init__(self)

        # A3CAgent 클래스에서 상속
        self.action_size = action_size
        self.state_size = state_size
        self.actor, self.critic = model
        self.sess = sess
        self.optimizer = optimizer
        self.discount_factor = discount_factor
        [self.summary_op, self.summary_placeholders,
         self.update_ops, self.summary_writer] = summary_ops

        # 지정된 타임스텝동안 샘플을 저장할 리스트
        self.states, self.actions, self.rewards = [], [], []

        # 로컬 모델 생성
        self.local_actor, self.local_critic = self.build_local_model()

        self.avg_p_max = 0
        self.avg_loss = 0

        # 모델 업데이트 주기
        self.t_max = 20
        self.t = 0

    def run(self):
        global episode
        env = gym.make(env_name)

        step = 0

        while episode < EPISODES:
            done = False
            dead = False

            score, start_life = 0, 5
            observe = env.reset()
            next_observe = observe

            # 0~30 상태동안 정지
            for _ in range(random.randint(1, 30)):
                observe = next_observe
                next_observe, _, _, _ = env.step(1)

            state = pre_processing(next_observe, observe)
            history = np.stack((state, state, state, state), axis=2)
            history = np.reshape([history], (1, 84, 84, 4))

            while not done:
                step += 1
                self.t += 1
                observe = next_observe
                action, policy = self.get_action(history)

                # 1: 정지, 2: 왼쪽, 3: 오른쪽
                if action == 0:
                    real_action = 1
                elif action == 1:
                    real_action = 2
                else:
                    real_action = 3

                # 죽었을 때 시작하기 위해 발사 행동을 함
                if dead:
                    action = 0
                    real_action = 1
                    dead = False

                # 선택한 행동으로 한 스텝을 실행
                next_observe, reward, done, info = env.step(real_action)

                # 각 타임스텝마다 상태 전처리
                next_state = pre_processing(next_observe, observe)
                next_state = np.reshape([next_state], (1, 84, 84, 1))
                next_history = np.append(next_state, history[:, :, :, :3],
                                         axis=3)

                # 정책의 최대값
                self.avg_p_max += np.amax(self.actor.predict(
                    np.float32(history / 255.)))

                if start_life > info['ale.lives']:
                    dead = True
                    start_life = info['ale.lives']

                score += reward
                reward = np.clip(reward, -1., 1.)

                # 샘플을 저장
                self.append_sample(history, action, reward)

                if dead:
                    history = np.stack((next_state, next_state,
                                        next_state, next_state), axis=2)
                    history = np.reshape([history], (1, 84, 84, 4))
                else:
                    history = next_history

                # 에피소드가 끝나거나 최대 타임스텝 수에 도달하면 학습을 진행
                if self.t >= self.t_max or done:
                    self.train_model(done)
                    self.update_local_model()
                    self.t = 0

                if done:
                    # 각 에피소드 당 학습 정보를 기록
                    episode += 1
                    print("episode:", episode, "  score:", score, "  step:",
                          step)

                    stats = [score, self.avg_p_max / float(step),
                             step]
                    for i in range(len(stats)):
                        self.sess.run(self.update_ops[i], feed_dict={
                            self.summary_placeholders[i]: float(stats[i])
                        })
                    summary_str = self.sess.run(self.summary_op)
                    self.summary_writer.add_summary(summary_str, episode + 1)
                    self.avg_p_max = 0
                    self.avg_loss = 0
                    step = 0

    # k-스텝 prediction 계산
    def discounted_prediction(self, rewards, done):
        discounted_prediction = np.zeros_like(rewards)
        running_add = 0

        if not done:
            running_add = self.local_critic.predict(np.float32(
                self.states[-1] / 255.))[0]

        for t in reversed(range(0, len(rewards))):
            running_add = running_add * self.discount_factor + rewards[t]
            discounted_prediction[t] = running_add
        return discounted_prediction

    # 정책신경망과 가치신경망을 업데이트
    def train_model(self, done):
        discounted_prediction = self.discounted_prediction(self.rewards, done)

        states = np.zeros((len(self.states), 84, 84, 4))
        for i in range(len(self.states)):
            states[i] = self.states[i]

        states = np.float32(states / 255.)

        values = self.local_critic.predict(states)
        values = np.reshape(values, len(values))

        advantages = discounted_prediction - values

        self.optimizer[0]([states, self.actions, advantages])
        self.optimizer[1]([states, discounted_prediction])
        self.states, self.actions, self.rewards = [], [], []

    # 로컬신경망을 생성하는 함수
    def build_local_model(self):
        input = Input(shape=self.state_size)
        conv = Conv2D(16, (8, 8), strides=(4, 4), activation='relu')(input)
        conv = Conv2D(32, (4, 4), strides=(2, 2), activation='relu')(conv)
        conv = Flatten()(conv)
        fc = Dense(256, activation='relu')(conv)
        policy = Dense(self.action_size, activation='softmax')(fc)
        value = Dense(1, activation='linear')(fc)

        local_actor = Model(inputs=input, outputs=policy)
        local_critic = Model(inputs=input, outputs=value)

        local_actor._make_predict_function()
        local_critic._make_predict_function()

        local_actor.set_weights(self.actor.get_weights())
        local_critic.set_weights(self.critic.get_weights())

        local_actor.summary()
        local_critic.summary()

        return local_actor, local_critic

    # 로컬신경망을 글로벌신경망으로 업데이트
    def update_local_model(self):
        self.local_actor.set_weights(self.actor.get_weights())
        self.local_critic.set_weights(self.critic.get_weights())

    # 정책신경망의 출력을 받아서 확률적으로 행동을 선택
    def get_action(self, history):
        history = np.float32(history / 255.)
        policy = self.local_actor.predict(history)[0]
        action_index = np.random.choice(self.action_size, 1, p=policy)[0]
        return action_index, policy

    # 샘플을 저장
    def append_sample(self, history, action, reward):
        self.states.append(history)
        act = np.zeros(self.action_size)
        act[action] = 1
        self.actions.append(act)
        self.rewards.append(reward)


# 학습속도를 높이기 위해 흑백화면으로 전처리
def pre_processing(next_observe, observe):
    processed_observe = np.maximum(next_observe, observe)
    processed_observe = np.uint8(
        resize(rgb2gray(processed_observe), (84, 84), mode='constant') * 255)
    return processed_observe

if __name__ == "__main__":
    global_agent = A3CAgent(action_size=3)
    global_agent.train()
```


### 코드 설명
- `A3C`의 핵심은 `액터러너`로, **글로벌 신경망에 관한 클래스 1개와, 액터러너에 관한 클래스 1개**를 가진다.

전체적인 코드 실행은
> 1. `main`에서 A3CAgent 클래스 생성
> 2. `main`에서 A3CAgent.train 실행
> 3. train 함수는 정해진 `thread`의 수만큼 `Agent`를 생성한다.

1번 과정에서 클래스가 만들어질 때, 글로벌신경망을 생성한다.
```python
    # 정책신경망과 가치신경망을 생성
    def build_model(self):
        input = Input(shape=self.state_size)
        conv = Conv2D(16, (8, 8), strides=(4, 4), activation='relu')(input)
        conv = Conv2D(32, (4, 4), strides=(2, 2), activation='relu')(conv)
        conv = Flatten()(conv)
        fc = Dense(256, activation='relu')(conv)

        policy = Dense(self.action_size, activation='softmax')(fc)
        value = Dense(1, activation='linear')(fc)

        actor = Model(inputs=input, outputs=policy)
        critic = Model(inputs=input, outputs=value)

        # 가치와 정책을 예측하는 함수를 만들어냄(멀티스레딩 오류 제거 함수)
        actor._make_predict_function()
        critic._make_predict_function()

        actor.summary()
        critic.summary()

        return actor, critic
```
- `액터-크리틱` 에이전트처럼 정책신경망과 가치신경망을 가진다. 하지만 액터-크리틱은 완전히 독립적이었고 따로 생성했다.(신경망 구조에 실제로 차이가 있었음)
- 하지만 `A3C` 에이전트의 글로벌 신경망은 1개의 함수를 사용하여 생성한다.
- 케라스의 `함수형 API`를 이용해 동일한 구조를 갖되, 출력층만 다르게 구성한 2개의 모델을 만들었다. 
- `_make_predict_function()` : 케라스 내장 함수로, **학습과 관계가 없고 멀티스레딩을 케라스에서 이용할 때 발생하는 에러를 제거**하는 함수이다. 

`Agent` 클래스 생성 시 `로컬신경망Local-Network`도 생성한다. 함수는 아래와 같다.
```python
    # 로컬신경망을 생성하는 함수
    def build_local_model(self):
        input = Input(shape=self.state_size)
        conv = Conv2D(16, (8, 8), strides=(4, 4), activation='relu')(input)
        conv = Conv2D(32, (4, 4), strides=(2, 2), activation='relu')(conv)
        conv = Flatten()(conv)
        fc = Dense(256, activation='relu')(conv)
        policy = Dense(self.action_size, activation='softmax')(fc)
        value = Dense(1, activation='linear')(fc)

        local_actor = Model(inputs=input, outputs=policy)
        local_critic = Model(inputs=input, outputs=value)

        local_actor._make_predict_function()
        local_critic._make_predict_function()
		
		# 글로벌 신경망의 가중치값으로 업데이트
        local_actor.set_weights(self.actor.get_weights())
        local_critic.set_weights(self.critic.get_weights())

        local_actor.summary()
        local_critic.summary()

        return local_actor, local_critic
```

`train` 함수는 아래와 같다. `agent.start()`는 각 액터러너 안의 `run()` 함수를 실행시킨다.
```python
    # 쓰레드를 만들어 학습을 하는 함수
    def train(self):
        # 쓰레드 수만큼 Agent 클래스 생성
        agents = [Agent(self.action_size, self.state_size,
                        [self.actor, self.critic], self.sess,
                        self.optimizer, self.discount_factor,
                        [self.summary_op, self.summary_placeholders,
                         self.update_ops, self.summary_writer])
                  for _ in range(self.threads)]

        # 각 쓰레드 시작
        for agent in agents:
            time.sleep(1)
            agent.start()

        # 10분(600초)에 한번씩 모델을 저장
        while True:
            time.sleep(60 * 10)
            self.save_model("./save_model/breakout_a3c")

```

`run()`함수는 아래와 같다. 메인 루프, 즉 환경과 상호작용하는 부분이 모두 `run()` 함수 내에 들어있다.
```python
    def run(self):
        global episode
        env = gym.make(env_name)

        step = 0

        while episode < EPISODES:
            done = False
            dead = False

            score, start_life = 0, 5
            observe = env.reset()
            next_observe = observe

            # 0~30 상태동안 정지
            for _ in range(random.randint(1, 30)):
                observe = next_observe
                next_observe, _, _, _ = env.step(1)

            state = pre_processing(next_observe, observe)
            history = np.stack((state, state, state, state), axis=2)
            history = np.reshape([history], (1, 84, 84, 4))

            while not done:
                step += 1
                self.t += 1
                observe = next_observe
                action, policy = self.get_action(history)

                # 1: 정지, 2: 왼쪽, 3: 오른쪽
                if action == 0:
                    real_action = 1
                elif action == 1:
                    real_action = 2
                else:
                    real_action = 3

                # 죽었을 때 시작하기 위해 발사 행동을 함
                if dead:
                    action = 0
                    real_action = 1
                    dead = False

                # 선택한 행동으로 한 스텝을 실행
                next_observe, reward, done, info = env.step(real_action)

                # 각 타임스텝마다 상태 전처리
                next_state = pre_processing(next_observe, observe)
                next_state = np.reshape([next_state], (1, 84, 84, 1))
                next_history = np.append(next_state, history[:, :, :, :3],
                                         axis=3)

                # 정책의 최대값
                self.avg_p_max += np.amax(self.actor.predict(
                    np.float32(history / 255.)))

                if start_life > info['ale.lives']:
                    dead = True
                    start_life = info['ale.lives']

                score += reward
                reward = np.clip(reward, -1., 1.)

                # 샘플을 저장
                self.append_sample(history, action, reward)

                if dead:
                    history = np.stack((next_state, next_state,
                                        next_state, next_state), axis=2)
                    history = np.reshape([history], (1, 84, 84, 4))
                else:
                    history = next_history

                # 에피소드가 끝나거나 최대 타임스텝 수에 도달하면 학습을 진행
                if self.t >= self.t_max or done:
                    self.train_model(done)
                    self.update_local_model()
                    self.t = 0

                if done:
                    # 각 에피소드 당 학습 정보를 기록
                    episode += 1
                    print("episode:", episode, "  score:", score, "  step:",
                          step)

                    stats = [score, self.avg_p_max / float(step),
                             step]
                    for i in range(len(stats)):
                        self.sess.run(self.update_ops[i], feed_dict={
                            self.summary_placeholders[i]: float(stats[i])
                        })
                    summary_str = self.sess.run(self.summary_op)
                    self.summary_writer.add_summary(summary_str, episode + 1)
                    self.avg_p_max = 0
                    self.avg_loss = 0
                    step = 0
```
> 액터러너의 run 함수 순서는 아래와 같다.
> 1. 액터러너의 로컬신경망에 따라 행동을 선택
> 2. 환경으로부터 다음 상태와 보상을 받음
> 3. 샘플 저장
> 4. 에이전트가 목숨을 잃거나, t_max 타임스텝 동안 반복
> 5. 저장한 샘플로 글로벌 신경망으로 보냄
> 6. 글로벌 신경망은 로컬 신경망으로부터 받은 샘플로 자신을 업데이트
> 7. 업데이트된 글로벌 신경망으로 액터러너를 업데이트

`옵티마이저Optimizer`를 생성해야 한다. 액터, 크리틱은 오류함수에 대한 정의가 달라서 각각 따로 옵티마이저를 갖고 있다. `액터`의 옵티마이저인 `actor_optimizer` 함수는 다음과 같다.
```python
    # 정책신경망을 업데이트하는 함수
    def actor_optimizer(self):
        action = K.placeholder(shape=[None, self.action_size])
        advantages = K.placeholder(shape=[None, ])

        policy = self.actor.output

        # 정책 크로스 엔트로피 오류함수
        action_prob = K.sum(action * policy, axis=1)
        cross_entropy = K.log(action_prob + 1e-10) * advantages
        cross_entropy = -K.sum(cross_entropy)

        # 탐색을 지속적으로 하기 위한 엔트로피 오류
        entropy = K.sum(policy * K.log(policy + 1e-10), axis=1)
        entropy = K.sum(entropy)

        # 두 오류함수를 더해 최종 오류함수를 만듬
        loss = cross_entropy + 0.01 * entropy

        optimizer = RMSprop(lr=self.actor_lr, rho=0.99, epsilon=0.01)
        updates = optimizer.get_updates(self.actor.trainable_weights, [],loss)
        train = K.function([self.actor.input, action, advantages],
                           [loss], updates=updates)
        return train
```
- 액터러너가 모은 샘플 `states, actions, rewards`을 가지고 글로벌 신경망의 업데이트 값을 구한다.
- 카트폴 액터-크리틱 에이전트의 옵티마이저와 오류함수 부분을 빼면 동일하다.
- A3C의 액터(정책신경망)의 오류함수는 2가지로 구성되어 있다.
1. 자신이 했던 행동에 대해 받은 보상을 기반으로 정책신경망 업데이트. `actor_loss`에 관한 부분이다. REINFORCE와 비슷하며, A3C에서는 반환값 대신 어드밴티지 함수를 이용, 행동이 좋은지 나쁜지 여부를 판단한다.
- 아래처럼 구현하며, 한 타임스텝을 진행한 후 구한다고 해서 1-스텝 시간차 에러라고도 부른다.
$$
Advantage = R_{t+1} + \gamma V_v(S_{t+1}) - V_v(S_t)
$$
그러나 A3C에서는 여러 타임스텝이 지난 후 어드밴티지 함수를 계산한다. 만약 `k-타임스텝`을 환경에서 진행한 후 받은 보상들을 통해 어드밴티지 함수를 구한다면 아래와 같다.
$$
Advantage = R_{t+1} + \gamma R_{t+2} + ... + \gamma^kV_v(S_{t+k}) - V_v(S_t)
$$
- 위처럼 k-타임스텝 후의 어드밴티지 계산을 `멀티스텝 시간차학습Multi-step Temporal-Difference Learning`이라고 하며, 살사와 몬테카를로의 중간이라고 볼 수 있다. 

위 수식을 구하는 함수는 `discounted_prediction`이며, `Agent` 클래스 내에 아래처럼 정의되어 있다.
```python
    # k-스텝 prediction 계산
    def discounted_prediction(self, rewards, done):
        discounted_prediction = np.zeros_like(rewards)
        running_add = 0

        if not done:
            running_add = self.local_critic.predict(np.float32(
                self.states[-1] / 255.))[0]

        for t in reversed(range(0, len(rewards))):
            running_add = running_add * self.discount_factor + rewards[t]
            discounted_prediction[t] = running_add
        return discounted_prediction
```
- 액터러너 클래스 내의 `run()` 함수에서 `t_max`라고 정의된 값이 몇 스텝 후에 어드밴티지를 계산해서 글로벌 신경망을 업데이트할 것인가에 관한 값이다. 
- A3C에서는 t_max 값을 20으로 놓고, 20타임스텝마다 1번씩 글로벌 신경망을 업데이트하게 된다.
- A3C는 추가로, 엔트로피가 오류 함수에 추가된다.
$$
엔트로피 = -\Sigma_i \, p_i\, logp_i
$$
정책의 엔트로피를 구하는 코드는 아래와 같다.
```python
entropy = K.sum(policy * k.log(policy + 1e-10), axis = 1)
entropy = K.sum(entropy)
```
- 이전에 다뤘던 것처럼 엔트로피의 최소화 방향은 **모든 행동의 확률을 동등하게 하는 방향**이다. 즉, 액터러너로 더 탐험을 하게 하는 오류함수이다.

크리틱 : 가치신경망의 옵티마이저는 다음과 같다.
```python
    # 가치신경망을 업데이트하는 함수
    def critic_optimizer(self):
        discounted_prediction = K.placeholder(shape=(None,))

        value = self.critic.output

        # [반환값 - 가치]의 제곱을 오류함수로 함
        loss = K.mean(K.square(discounted_prediction - value))

        optimizer = RMSprop(lr=self.critic_lr, rho=0.99, epsilon=0.01)
        updates = optimizer.get_updates(self.critic.trainable_weights, [],loss)
        train = K.function([self.critic.input, discounted_prediction],
                           [loss], updates=updates)
        return train
```

가치신경망 업데이트하는 오류함수는 아래와 같다. 어드밴티지 함수로 구한 값이다.
$$
Advantage = R_{t+1} + \gamma R_{t+2} + ... + \gamma^kV_v(S_{t+k}) - V_v(S_t)
$$
단지 여러 타임 스텝으로 액터-크리틱 알고리즘에서 사용했던 크리틱의 오류함수를 변형시킨 것이다. 

이렇게 정의한 `actor_optimizer`와 `critic_optimizer`, `discounted_prediction` 함수를 통해 글로벌 신경망을 업데이트하는 함수 `train_model`은 다음과 같다.

```python
    # 정책신경망과 가치신경망을 업데이트
    def train_model(self, done):
        discounted_prediction = self.discounted_prediction(self.rewards, done)

        states = np.zeros((len(self.states), 84, 84, 4))
        for i in range(len(self.states)):
            states[i] = self.states[i]

        states = np.float32(states / 255.)

        values = self.local_critic.predict(states)
        values = np.reshape(values, len(values))

        advantages = discounted_prediction - values

        self.optimizer[0]([states, self.actions, advantages])
        self.optimizer[1]([states, discounted_prediction])
        self.states, self.actions, self.rewards = [], [], []
```
- 글로벌 신경망인 `self.actor`와 `self.critic`을 액터러너가 모은 샘플로 업데이트하면, 그 액터러너는 글로벌 신경망으로 자신을 업데이트한다. 

글로벌신경망으로 액터러너의 로컬신경망을 업데이트하는 함수 `update_local_model`은 아래와 같다.
```python
    # 로컬신경망을 글로벌신경망으로 업데이트
    def update_local_model(self):
        self.local_actor.set_weights(self.actor.get_weights())
        self.local_critic.set_weights(self.critic.get_weights())
```
- 8개의 액터러너가 이 작업을 비동기로 진행하고, 이 과정으로 A3C의 에이전트가 학습한다.

>A3C 알고리즘이 액터-크리틱 알고리즘과 다른 점
>1. Threading을 이용한다
>2. 정책신경망의 오류함수로 업데이트하는 함수 부분

A3C에서 각 액터러너는 20타임스텝마다 글로벌신경망을 업데이트한다. 각자 다른 환경에서 모은 샘플로 계산한 그래디언트로 글로벌 신경망을 비동기로 업데이트하기 때문에 샘플 간의 상관관계를 어느 정도 꺨 수 있다. 

### A3C 실행 결과
- 26시간, 31700 에피소드 학습 결과이다.

DQN에서 오류함수 값은 단조롭게 감소하지 않았기 때문에, 학습을 판단하는 방법으로 에피소드마다 큐함수의 최댓값을 평균내는 방법을 이용했다. 
A3C에서도 학습 판단은 오류함수를 이용하지 않고, **에이전트가 지나온 상태에 대해, 정책으로 나오는 확률의 최댓값들을 평균 취한 값을 이용**한다. 확률의 최댓값이 판단 기준으로 좋은 이유는 **확률의 최댓값이 1에 가까워질수록 정책의 수렴을 직관적으로 알 수 있기 때문이다.**  
확률의 최댓값이 1에 가까운데, 점수가 낮다면 잘못 학습되었음을 알 수 있다. 만약 값이 오르지 않는다면 학습이 아예 이뤄지지 않는다고 판단할 수 있다.

DQN 대비 A3C의 장점은 **빠른 학습 속도**다. 비슷한 점수를 얻기 위해 DQN은 약 40시간 정도 학습해야 하지만 A3C은 26시간 정도 학습하면 된다. 최근에는 A3C를 GPU에 최적화해서 더 빠른 시간에 학습을 하기도 한다. 

### 저장된 모델 플레이
학습이 매우 오래 걸려서 저장된 모델로 플레이할 수 있게 코드가 주어진다. `3-atari/1-break-out/`에 들어 있으며 `play_~~~_model.py`이 플레이 코드이다. 
각각 번호가 붙어 있는데, 1단계가 학습의 시작, 5단계가 학습의 끝이다.

## 학습 결과 업로드
- `OpenAI GYM`은 다양한 예제를 제공한다. 카트폴 예제는 `Classic Control`에, 브레이크아웃 예제는 `Atari`에 포함되어 있다. OpenAI GYM은 자신이 학습시킨 모델을 사이트에 올릴 수 있도록 환경이 구축되어 있다.

### OpenAI GYM에 학습 결과 업로드
- 학습시킨 에이전트를 업로드할 수 있다.
- `write up`을 설정하면 학습에 사용된 코드도 공개할 수 있다.

1. 회원 가입에는 깃허브 계정이 필요하다.
2. 업로드 관련 

> 1. 에이전트 학습시키기 전에 업로드와 관련 코드 설정을 해야 한다.
```python
from gym import wrappers
```
- wrappers 모듈은 에이전트가 학습 에피소드별로 학습되는 과정을 `mp4`로 녹화하고, 보상, 타임스텝, 에피소드 등의 통계 자료는 `json` 형태로 저장한다. `env` 객체를 생성한 다음, 곧바로 `wrappers` 모듈의 `Monitor` 함수를 사용한다. 
- `Monitor` 함수의 1번째 인자는 `env`라는 `OpenAI` 환겨이고, 2번째 인자는 모니터링된 에이전트 정보가 저장되는 경로다. 공식 문서는 `/tmp/` 디렉터리에 위치하도록 설정했지만 원하는 디렉터리를 설정해도 된다. 

```python
env = gym.make('CartPole-v1')
env = wrappers.Monitor(env, 'tmp/cartpole_upload', force = True)
```
- `/cartpole_upload`는 학습된 에이전트의 정보가 있는 곳으로, 학습 후 업로드할 때 이 이름이 사용된다.

업로드는 `OpenAI` 계정의 api_key가 필요하며, 계정 화면에서 확인할 수 있다.
이후, 업로드는 아래의 명령어를 이용한다.
```python
env.close()
gym.scoreboard.api_key = 'myapikey'
gym.upload('/tmp/cartpole_upload')
```

OpenAI GYM의 환경은 각각 `solved`의 기준을 가지고 있다. 기준에 부합하면 그래프 하단에 `solved` 부분이 체크된다. 
OpenAI는 `write up`한 사용자를 Evaluation 화면 상위에 게시한다. `write up`은 학습시킨 코드를 깃허브의 `gist`를 통해 공개하는 방식을 말한다.