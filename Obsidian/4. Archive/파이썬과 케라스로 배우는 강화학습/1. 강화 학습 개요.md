
1. [[#개념|개념]]
	1. [[#개념#스키너의 강화 연구|스키너의 강화 연구]]
	2. [[#개념#머신러닝과 강화학습|머신러닝과 강화학습]]
	3. [[#개념#에이전트|에이전트]]
2. [[#강화학습 문제|강화학습 문제]]
	1. [[#강화학습 문제#순차적 행동 결정 문제|순차적 행동 결정 문제]]
	2. [[#강화학습 문제#순차적 행동 결정 문제의 구성 요소|순차적 행동 결정 문제의 구성 요소]]
		1. [[#순차적 행동 결정 문제의 구성 요소#상태(State)|상태(State)]]
		2. [[#순차적 행동 결정 문제의 구성 요소#행동(Action)|행동(Action)]]
		3. [[#순차적 행동 결정 문제의 구성 요소#보상(Reward)|보상(Reward)]]
		4. [[#순차적 행동 결정 문제의 구성 요소#정책(Policy)|정책(Policy)]]
	3. [[#강화학습 문제#방대한 상태를 가진 문제에서의 강화학습|방대한 상태를 가진 문제에서의 강화학습]]
3. [[#강화학습 예시 : 브레이크아웃|강화학습 예시 : 브레이크아웃]]
	1. [[#강화학습 예시 : 브레이크아웃#아타리 게임|아타리 게임]]
	2. [[#강화학습 예시 : 브레이크아웃#브레이크아웃의 MDP 구성과 학습 방법|브레이크아웃의 MDP 구성과 학습 방법]]
		1. [[#브레이크아웃의 MDP 구성과 학습 방법#MDP|MDP]]
		2. [[#브레이크아웃의 MDP 구성과 학습 방법#학습|학습]]

## 개념

### 스키너의 강화 연구
- 행동심리학 : `강화Reinforcement` - 이전에 배우지 않았지만 **직접 시도하면서 행동과 결과로 나타나는 좋은 보상 사이의 상관관계를 이해하는 것**. 그러면서 좋은 보상을 얻게 해주는 행동을 더 많이 하게 된다. 
- "왜 좋은 보상이 오는지"를 이해하는 것이 아니다. "그 행동을 해서, 좋은 보상이 온다"를 알기 떄문에 행동을 반복하게 되는 것.

### 머신러닝과 강화학습
- `머신러닝`은 지도학습, 비지도학습, 강화학습으로 나뉜다.
	- 지도학습 : 회귀, 분류
	- 비지도학습 : 군집화 - 비슷한 그룹을 묶거나, 시장 조사 시 시장을 특성에 따라 나눌 수 있음

- **강화학습은 지도, 비지도학습과 성격이 아예 달라서 따로 분류된다. 정답이 주어진 것은 아니지만, 주어진 데이터에 대해 학습하는 것도 아니기 때문이다.**
	- `보상Reward` : 선택한 `행동Action`에 대한 환경의 반응.
	- 지도학습은 `오차`로 학습하지만 강화학습은 `보상`을 통해 학습한다.

### 에이전트
- 강화학습을 통해 스스로 학습하는 컴퓨터. 
- 사전지식이 없는 상태에서 학습한다. 자신이 놓인 환경에서 상태를 인식한 후 행동한다. 그러면 환경은 에이전트에게 보상을 주고, 그 다음 상태를 알려준다. 더 좋은 상태가 된다면 그 행동은 좋은 행동이 될 것이므로, 이를 반복하면 어떤 행동이 좋은 행동인지 학습할 수 있게 된다.

```
(에이전트 -> 행동 -> 환경 -> 다음 상태 -> 보상) -> (에이전트 -> ...)
```

- 강화학습의 목적은 에이전트가 보상의 합을 최대화하는 **최적의 행동양식, 최적의 정책을 학습**하는 것이다.
- 보상은 양수, 음수 모두 가능하며 음수 보상은 처벌이 된다. 보상을 구성할 때, **상만 있거나 벌만 있는 것보다는 상과 벌이 같이 있는 것이 에이전트가 무엇을 학습해야 하는지 더 명확하게 알 수 있다.**

- 강화학습의 장점 : **환경에 대한 사전 지식이 필요 없다.**
	- 환경을 알더라도 환경 정보를 통해 계산하는 과정은 많은 시간이 걸린다.


## 강화학습 문제
- 강화학습은 어떤 문제에 적용해야 하는가?
- **결정을 순차적으로 내려야 하는 문제에 적용한다.** 물론 `DP`나 `진화 알고리즘`이 있지만, 각각 한계를 지니고 있다.

### 순차적 행동 결정 문제
- 에이전트가 학습하고 발전하려면 문제를 수학적으로 표현해야 한다. 이 문제를 정의할 때 사용하는 방법으로 `MDP Markov Decision Process`을 이용한다.

### 순차적 행동 결정 문제의 구성 요소

#### 상태(State)
에이전트의 정적인 요소와 동적인 요소를 모두 포함한다. **정의가 중요한데, 에이전트가 행동을 결정하기 위해 충분한 정보가 제공되어야 한다.**   
예를 들어 탁구를 치는 상황에서, 탁구공의 위치만으로는 탁구를 칠 수 없다. 위치, 속도, 가속도 등의 정보가 모두 필요하다.  

#### 행동(Action)
에이전트가 어떠한 상태에서 **취할 수 있는 행동.** 게임이라면 게임기의 입력이다. 학습을 거듭하면서 에이전트는 무작위 -> 특정 행동의 확률을 높여나가는 과정을 반복한다. **에이전트가 행동을 취하면 환경은 에이전트에게 보상을 주고, 다음 상태를 알려준다.**

#### 보상(Reward)
다른 머신러닝 기법과 강화학습을 다르게 만드는 가장 핵심적인 요소로, **에이전트가 학습할 수 있는 유일한 정보**가 보상이다. 보상은 에이전트에 속하지 않는 환경의 일부이며, 에이전트는 어떤 상황에서 얼마의 보상이 나오는지 미리 알 수 없다.

#### 정책(Policy)
순차적 행동 결정 문제에서 구할 답으로, **모든 상태에 대해 에이전트가 취해야 할 행동을 정해놓은 것이다.** `순차적 행동 결정 문제`를 풀었을 때 얻어진 정책을 `최적 정책Optimal Policy`이라고 하며, 에이전트는 최적 정책에 따라 행동했을 때 보상의 합을 최대로 받을 수 있다. 

### 방대한 상태를 가진 문제에서의 강화학습
- 로봇 같은 경우를 생각해보면, 로봇이 관찰할 정보와 행동, 보상이 모두 연속적이라 가능한 경우의 수는 무한이라고 생각할 수 있으며, 로봇의 움직임까지 이어질 피드백을 생각하면 실제로 문제는 더 어렵다. 
- `인공신경망ANN`이 현실 세계의 문제를 학습하는 길을 만들었음.

## 강화학습 예시 : 브레이크아웃

### 아타리 게임
- 브레이크아웃 = 벽돌깨기. 아타리 게임으로, 강화학습이 적용된 대표적인 예시다. 딥마인드의 `Playing Atari with Deep Reinforcement Learning`이라는 논문에 그 내용이 있다. 

### 브레이크아웃의 MDP 구성과 학습 방법

#### MDP
- `상태` : 게임 화면으로, 에이전트가 상황을 파악할 수 있게 4개의 연속적인 화면을 받는다. 4개의 화면이 1개의 상태로 에이전트에 제공되며, 각 화면은 2차원 픽셀 데이터이다.
- `행동` : `제자리, 왼쪽, 오른쪽, 발사`로, `발사`는 게임을 시작할 때만 쓴다. 1개의 상태에서 에이전트가 행동을 결정하면 잠시 그 행동을 반복하는데, 이는 상태를 화면 4개로 받기 때문이다. 
- `보상` : 벽돌이 깨지면 +1, 더 위의 벽돌일수록 보상이 크다. 아무 것도 깨지지 않았을 때는 0, 목숨을 잃으면 -1이다.

#### 학습
최초에는 상황을 모르기 때문에 무작위로 제자리, 왼쪽, 오른쪽으로 움직인다. 그러다가 벽돌을 깨면 보상을 얻는다. 게임을 하면서 이 과정이 반복된다.  
강화학습을 통해 인공신경망이 학습되며, 4개의 연속적인 게임 화면을 입력으로 받아 에이전트가 할 수 있는 행동이 얼마나 좋은지 출력으로 반환한다. **행동이 얼마나 좋은지가 행동의 가치가 되며, 이를 `큐 함수Q Function`라고 한다.**  
이렇게 학습된 인공신경망을 `DQN : Deep Q-Network`라고 한다. DQN은 출력으로 3개의 큐 함수인 `Q(제자리)`, `Q(왼쪽)`, `Q(오른쪽)`을 내놓는다. 에이전트는 이 3개의 큐함수에 따라 행동한다.   

- 사람의 학습 과정과의 공통점 : 화면을 보고 학습한다
- 차이점 : 
	- 사람은 규칙을 알아보고 게임을 한다. 규칙을 모른 채 할 수도 있지만, 초반의 느린 학습 속도의 원인이 되기도 한다. 사람의 학습은 다른 곳에도 영향을 미친다. 예를 들어, 수학을 배웠다면 과학도 배우기 수월하다. 
	- 강화학습은 규칙을 모른 채 게임을 한다. 그리고 에이전트는 각 학습을 모두 별개로 취급하기 때문에 항상 밑바닥부터 학습해야 한다.  
