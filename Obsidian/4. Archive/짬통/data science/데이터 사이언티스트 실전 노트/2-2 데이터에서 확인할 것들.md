- 회사는 외부에서 데이터를 구매해 사용하기도 한다. 현재 쓰고 있는 상황과 다르기 떄문에 필요에 따라 전체를 쓰기도 하고, 일부만 추출해서 쓰기도 한다.

- 데이터에서 확인할 것들

### 1) 크기
- 단순히 행x열의 크기만 따지는 게 아님!

#### 1. DB에서 불러오거나 저장 or 클라이언트에서 데이터 받을 때
-  이 데이터는 내가 읽어야 하는 데이터인가?
- 모든 데이터가 잘 저장되었는가?
- 커뮤니케이션 과정에서 잘못된 파일을 주고받을 수 있는 상황이 생길 수 있기 때문에 **파일을 보낼 때 데이터 크기도 같이 명시**해주는 게 좋다. 

#### 2. 데이터 처리 과정에서의 크기
- 처리 과정에서 데이터 손실이 발생하는데, 이게 얼만큼인지 파악하는 게 필요함

#### 3. 결측치 및 유일값 갯수
- 결측치가 있다면 얼마나 있는지, 그 정도의 결측치가 타당한 것인지
- 범주, 수가 제한되었다면 유일한 값의 갯수가 매우 많거나 매우 한정적일 수 있음.
	- 유일값의 갯수가 타당한지, 전체에서 각 유일값이 차지하는 정도가 타당한지
		- ex) 4개의 범주 중 특정 1개에만 과도하게 몰려있지 않은가?

#### 4. 이 데이터 크기로 모델을 세워도 괜찮은가?
- 데이터의 크기가 작은 경우, 훈련 - 시험 데이터로 분리하면 패턴 & 식을 얻는 데 충분하지 않을 수 있음
- 이 때는 **데이터를 더 얻거나, 모수적 모델**을 사용한다. 
- 데이터가 많다고 무조건 좋은 건 아니다.
	- 차원의 저주
	- 계산 과정에서의 메모리 한계
	- 통계 모델은 다량의 데이터에서 유의성 파악에 방해가 됨.

### 2) 데이터 값에 따른 종류
- 정성적 데이터
	- 이항(Binary) 데이터 : T/F
	- 순서(Ordinal) 데이터 : 대/중/소, 등수
	- 명목(Nominal) 데이터 : 과일, 색 종류, 도시 이름
- 정량적 데이터
	- 이산(Discrete) 데이터 : 세는 게 가능 / 인원 수, 점수, 불량 개수
	- 연속(Continuous) 데이터 : 실수 형태 / 키, 몸무게 등 단위가 있는 것

[[부록 2_ .loc, .apply(), np.where() 비교]]

- 각 데이터에 들어간 정보를 제대로 활용할 수 있는가는 데이터 타입부터 살펴봐야 하며, `df.info()`로 시작하면 됨. 이 부분은 넘어감

### 3) 결측치
1) 무엇이 결측치인가
2) 결측치를 어떻게 표현할 것인가
3) 결측치를 어떻게 다룰 것인가

- ex1) 데이터프레임에서 `None`으로 표시되어 있다면 `np.nan`으로 바꿔줄 필요가 있을 것이다
- `np.nan`과 `None`의 차이
	- `np.nan`은 실수 클래스의 객체이다.
	- `None`은 `NoneType` 클래스의 객체이다.
```python
x = np.nan # class 'float'
y = None # class 'NoneType'

x is np.nan # 같은 객체인가 : True
x == np.nan # 같은 값인가 : False

y is None # True
y == None # True
```

#### np.nan
- `np.nan`은 **값을 비교할 수 없는 특징을 가졌다.** 따라서 판다스의 조건식에 들어간다면, `nan` 값이 처리되지 않는다
- 이 때는 `isnull()`을 쓰면 됨. 

#### None
- 값이 존재하지 않거나, 정의되지 않을 때 사용된다.
- `dict`에서 `.get(없는 키)`를 하면 `None`을 반환함

- **데이터프레임의 `NaN`은 실수로 취급되지만, `None`은 말 그대로 없거나 비어있음**을 뜻한다.
- `Datetime`의 `NaN`은 `NaT`로 표현된다.


### 결측치 표현을 알아야 하는 이유 
- **데이터베이스는 `np.nan`을 인식하지 못해서 `None`으로 바꿔줘야 하기 때문이다.**

- 또, 이런 상황도 있다.
```python
for col in cols:
	df[col].replace(np.nan, None)
```
- 모든 column들의 null값을 None으로 바꾸는 과정인데, `None`은 특정한 값이 지정되지 않는다. 
	-  **`df[col].replace(np.nan)`과 동일**한 상황이며, 바뀌는 값이 없기 때문에 `replace`는 **이전 행의 값을 채우는 방식으로 수행**된다.
- 대신 `df.where`을 통해 수행할 수 있다.
```python
df.where(pd.notnull(df), None)
```
- `np.where`와 달리 `df.where`은 조건문이 `False`인 것들의 값을 변경한다. 이 코드는 잘 작동함.

#### 결측치를 다루는 4가지 방법
- 우선 결측치가 무작위로 나타나는지 패턴으로 나타나는지, 얼마나 관측되는지를 본다.

1. 결측치 포함 행 전체 지우기
	- 전체 데이터가 적을 경우 이 행을 지울지 말지에 대한 고민 필요
2. 결측치 포함 열 전체 지우기 
	- 열에 결측치가 많다 = 담은 정보가 충분하지 않다
	- 마찬가지로 열이 적다면 다른 방법을 고민해야 할 수도.
3. 결측치 대체하기
	- 평균값, 중앙값, 최빈값, 그룹별 평균값, 그룹평 중간값 등이 있음
	- 결측치 예측 모델을 사용하는 방법도 있음
	- 방법 자체는 금방 찾을 수 있지만 "왜 그렇게 해야 하는지, 그렇게 해서 더 나은 결과값이 나왔는지"를 확인하는 게 더 중요하다.
4. 결측치 유지하기
	- 결측치를 지우거나 대체할 경우 편향이 생길 수 있음

- 어느 것이든 데이터의 오류나 불확실성을 줄이는 방향으로 결측치를 다뤄야 한다.

### 4) 중복 데이터 
- 대체로 비슷하거나 동일한 값을 가지며 같은 열, 행이 반복되어 나타나는 상황
- 중복 데이터를 처리하기 전 해야 할 질문
	1. 업무의 목적
	2. 중복 데이터의 비중
	3. 왜 중복 데이터가 발생함?

- 중복 데이터를 유지하는 경우도 있다 : **타깃 클래스 간의 불균형이 심한 경우.**
- 중복 데이터를 지우는 건 오래 걸리지 않지만, 찾는 게 어려울 수 있다.
	- ex) `USA`와 `US`는 같음. 근데 나머지 값들이 같고 이것만 다르다고 하면? 중복된 데이터임에도 찾기 어려운 상황임.

- 파이썬, R에서는 문자열을 비교하는 `FuzzyWuzzy`라는 라이브러리가 있다.
```python
from fuzzywuzzy import fuzz
from fuzzywuzzy import process

input_str = 'US'

choices = ['US!', 'U.S.', 'USA', 'United States', 'SU']

process.extract(input_str, choices, limit = 3)

# [(`'US!'`, 100), ('USA', 90), ('U.S.', 80)]
```
- `input_str`과 유사한 단어를 점수와 함께 내림차순으로 반환한다.
- 이를 이용하면 크기가 큰 데이터이더라도, 유사도가 높지만 다른 문자열 데이터끼리 같은 그룹으로 묶을 수 있다(새로운 특성을 만든다든지..)

- **데이터가 커질수록 수작업이 힘들어지기 때문에 이 방법이 되게 좋을 듯**

### 5) 식별키
- 식별키 : PK - 특정 정보를 확인하거나 식별하도록 도와줌
- 조건 3가지 : **유일함, 고정됨, 결측값 없음**

### 6) 스키마 생성 예시
> 외부에서 데이터를 주기적으로 받아 DB에 저장하는 경우를 가정해보자

1. 최초 업무
	- 데이터 이해하기 : 어떤 데이터인가, 어떻게 활용할 수 있는가
	- 데이터 가공
	- 데이터 저장을 위한 스키마 생성 : 식별키 만들거나 사용할 수 있는 필드 찾기, 유일한지 결측치 허용인지. / 이후 데이터 타입에 대한 스키마를 설정하는데, 각 열에 예상가능한 값을 미리 파악해둬야 한다.
	- 클라우드 서비스는 이를 자동으로 하는 기능을 제공함
2. 차후 과정

