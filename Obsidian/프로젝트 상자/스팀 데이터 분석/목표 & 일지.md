 #스팀프로젝트 
## 목표
1. `steamspypi`에 있는 데이터를 지속적으로 수집 & 가공하는 파이프라인 만들기
	- 구현 가능하다면 자동화도 시도하기
2. 날짜에 따른 `ccu`, `positive`, `negative` 추적 및 분석해보기
3. 카테고리나 태그에 따른 분석하기(가능하다면)
4. 할인율에 따른 분석도 ㅇㅋ

---

## 일지

### 230529
- `pymysql.connect` 객체를 함수로 생성하고 테스트
	- 테스트는 `Collect_Preprocessing.ipynb` 파일에서 DB 변수만 바꿔주면서 하고 있음
- **저장은 SQLAlchemy를 이용하고, 불러오는 건 pymysql을 이용한다.**
	- `DatabaseError: Execution failed on sql 'SELECT name FROM sqlite_master WHERE type='table' AND name=?;': not all arguments converted during string formatting` 에러 관련.
- **그냥 `SQlAlchemy`로만 데이터 저장하고 불러오는 게 낫지 않나?** 라고 생각해서 스크립트를 바꿈(steamtest 데이터 수집 잘 작동하면 ㅇㅋ)
- `to_sql()`관련



### 230526
- 자동화에 앞서 프로세스 정리(사이트가 뻑난 경우는 제외)
```
1. 오늘 실행된 적 있는가 검토
	- 기준 1 : time_value.csv가 있는가 -> 없다면 최초 실행
	- 기준 2 : time_value.csv의 마지막 데이터가 코드 실행 하루 전의 날짜인가
		- 그렇다면 이미 실행되었으므로 함수 실행
		- 아니라면 오늘의 데이터만 수집

2-1. 최초 실행
	1) 5000개의 데이터를 수집함
	2) 수집한 데이터들의 appid를 바탕으로 다시 detail에 관한 appid를 수집함
	3) 수집된 데이터들을 4개의 데이터프레임으로 나눈 뒤,  csv와 MySQL에서도 데이터베이스와 테이블을 만들고 저장

2-2. 2회 이상부터
	1) 5000개의 데이터를 수집함
	2) appid를 기준으로 데이터를 3가지로 나눔 
		<1> 이미 있는 appid이면서 또 수집된 데이터
		<2> 이미 있는 appid이면서 이번엔 수집되지 않은 데이터
		<3> 기존에 없는 appid
	3) <2>와 <3>은 detail을 다시 추적해야 함
		<2> : 오늘의 데이터가 추적되지 않았음 
		<3> : 테이블에 추가할 정보가 새로 필요함.
	4) 시간과 관련된 정보는 <1> ~ <3> 모두에 해당하며, 해당 테이블(1개)에 저장함. <3>의 경우 새로 생긴 데이터이므로 시간에 관련없는 테이블(3개)에 새로 추가함

```
- SQL에 저장하는 경우만 생각한다면 csv에 따른 프로세스를 sql을 조회하는 형태로 바꿔야 함
- **데이터 수집 기준 등을 모두 MySQL로 변경**
- `csv`파일은 Tableau에 연결할 때 등 데이터를 분석하기 위해서만 이용하겠음
	- 혹시나 파일에 엑셀 등으로 접근했다가 실수로 저장한 경우 인코딩이 흐트러지기 때문에 csv로 저장하는 건 좀 위험할 수도 있겠다는 생각이 들었음
  

#### 생각해볼 점
- **raw 데이터의 csv파일을 왜 모아두는가?**
	- 데이터 수집 시에는 눈치채지 못하다가, 나중에 보니까 데이터가 이상하게 수집되는 상황이 있을 수 있음
	- 그런데 가공 전 데이터를 갖고 있지 않다면, 해당 데이터들을 버려야 하는 상황이 발생할 수도 있음
- 그러면 SQL에도 원본 데이터를 저장해두자
	- 방식은 동일하게 
		- `all` : 디테일 정보가 빠진 원본 데이터
		- `detail` : detail로 그날그날 검색한 정보

### 230525
- MySQL에 저장하는 과정 계속 하기
	- MySQL의 `time_value` 테이블 : 다른 테이블은 `PK`로 `appid`를 지정해도 되지만, 얘는 굳이 그럴 필요 없다 : 날짜에 따라 `appid`를 가지므로, 여러 날짜라면 `appid`가 여러 개일 수 있기 때문이다.

#### 테이블 생성
- **문자로 들어오는 COLUMN들의 `VARCHAR()`값을 얼마로 할까?**
- MySQL 5.0버전 이후로는 `varchar()`에 들어가는 숫자는 **문자의 갯수**라고 한다. 바이트 계산할 필요 없음.
	- 추가로, `VARCHAR(255)`에 관련한 이야기가 있음 : 길이를 표현하는 별도의 공간이 필요한데, `255`까지는 1바이트이지만 `256`부터는 2바이트이므로 데이터의 구조 자체를 뜯어고쳐야 한다는 것.
	- 찾아보면 `255`가 글자라는데 바이트를 다루듯이 쓰고 있어서 헷갈린다. 일단 글자수로 취급함. 
	- `info`의 경우 모든 데이터가 255자를 넘지 않는 상황이지만, `,`로 구분된 데이터로 저장하고 있기 때문에 새로운 데이터가 255자를 넘을 수 있어서 `varchar(255)`로 저장함.
	- 하나 빼고 다 `varchar(255)`로 지정
- 숫자에서도 비슷한 이슈가 있는데, `df.describe()`로 최댓값을 확인한 다음 그 값에 맞게 자료형을 지정했음.



### 230524

- **문제 상황** 해결
	1. 엑셀 파일 `utf-8`로 지정 -> 하나하나 수정함, 이 과정에서 5월 21일 데이터 날아간 거 참고!
	2. 문제 발생했던 날들의 데이터 취합
		- 결측치 처리를 안했던 듯 하다 
			-  `developer`, `publisher`의 결측치 처리 - 그냥 비워놔도 된단다. 
			- 심지어 판다스에서 `NaN`값은 SQL 저장 시 `NULL`로 들어간다고 함.
- `set()`으로 `appid` 간의 차이를 구하는 것과 `drop_duplicates(subset = ['appid']`의 차이도 있다. 왜???
	- 예전 `info` 테이블에 중복된 값이 있었던 것으로 판명... 
	- 처음 5000개의 파일을 수집할 때 중복을 제거하는 과정을 분명 넣었음
	- 근데도 중복 데이터가 있다? 
	- 이런 가능성이 있을 수 있겠다
		- 기존에 수집된 데이터가 있음 -> 5000위 밖에 있다가 진입한 동명의 게임이 있음(다른 appid) -> **추가하는 상황에서는 중복 데이터 체크 없이 csv파일에 들어가기만 하므로, 다른 appid, 같은 이름의 게임이 추가될 수 있음**
	- 수정) 데이터를 추가하기 전, 기존 `info.csv`와 비교해서 이름이 동일한 데이터가 새로 생겼다면, **기존의 데이터만 유지**함 (`add_data_to_csv` 수정)


#### 문제 상황 1. 
![[Pasted image 20230524133331.png]]
- 기존 시간 값에 대한 테이블에 에러가 이런 식으로 발생한 상황임. 또한 `encoding_errors = 'ignore'`을 설정해야만 pandas로 접근할 수 있음

- 근데 새로 만든 테이블은 
![[Pasted image 20230524133430.png]]
- 이런 식으로 잘 들어와 있음.
- 일단 `pandas`는 한자에 대한 데이터 수집을 했을 때 화면에 잘 보여준다
- 반면 엑셀이나 워드패드로 csv 파일을 조회하면 아래와 같이 뜸
![[Pasted image 20230524134432.png]]
- 찾아보니까 **`pandas`의 기본 인코딩 포맷은 `utf-8`인데, 엑셀은 `ANSI`를 사용하기 때문인 듯?**
- 저장을 안했으면 상관이 없는데, 엑셀에서 다시 저장을 누르면 문제가 발생

#### 문제 상황 2.
- `set`을 이용해서 중복된 정보를 제거하는 과정 vs `drop_duplicates`를 이용한 과정 

#### 문제 상황 3.
![[Pasted image 20230524181456.png]]
- `appid`가 동일하고 `name`이 다른 데이터들의 경우이다. 위는 새로 수집된 데이터이고 아래는 기존에 유지되고 있던 데이터이다.
- 기존에 전제했던 게 약간 틀어진 상황이므로, `appid`가 다르다면 다른 게임으로 간주하겠음
- 그 와중에 `discount = 0`인데 가격 오락가락하는 거 실화냐?



### 230523
#### 문제 상황 발생
- 데이터 수집 과정 중 예상치 못한 상황 발생 : 5000개의 데이터를 수집했는데 모두 기존에 없던 데이터여서 또 5000개의 디테일을 수집하려고 함
	- `time_value_df`를 살펴보니까 전부 `float`로 저장되어 있음 : 근데 값을 보면 막상 소수값은 없음 -> 아마 `NaN` 값이 생기면서 다른 csv 파일들을 불러왔을 때는 `int`로 잘 저장되어 있음
- 또, `time_value.csv`의 `discount`에는 `NULL`값도 있음

#### 원인 추정
- `appdetail`을 불러올 때 기존에 있는 앱 이름을 불러오는 곳을 `info`에서 `time_value`로 바꿨는데, 이게 문제가 된 듯
> 1. 테이블을 보니까 아예 처음부터 Null값이 발생했던 것 같음 
> - `time_value`에서만 발생한 문제로 추정, 다른 테이블을 같은 id로 조회해보면 데이터들이 멀쩡하게 잘 들어가 있음




### 230522
#### 진행중
- 데이터 수집이 잘 되면 SQL에 저장하는 방법도 시도하기(진행중)
	- SQL은 MySQL을 쓰기로(넥슨이 MySQL을 쓰는 것 같아서 ㅎ)

#### 완료
-  사이트 뻑이 생각보다 자주 나는 편임 : 일본어 공부할 때 데이터 수집을 미리 해두자
- 가상환경 설정해서 `.py` 파일로 데이터 수집이 잘 되는지 테스트하기
	- 가상환경 설정 시 `Windows Shell`을 이용하자(`cmd`는 리눅스 커맨드를 인식 못함)
	- `Scripts/activate` 입력해주면 됨 : [[가상환경 세팅법]] 참고

### 230519
- 오늘 데이터를 수집하려고 했으나 다시 5월 15일과 같은 에러가 발생함. 보유자 순으로 상위 5000개 데이터가 보여야 하는데, 중간의 어딘가부터 보이는 상황
	- 만약 **이러한 에러가 발생했다면, `info.csv`에 이미 있는 `appid`들만 다시 수집해야겠다** : `request = all`에서만 에러가 발생하고 `request = appdetails`는 잘 작동하고 있기 때문이다.
	- 훨씬 오래 걸릴 수밖에 없는(5분 + $\alpha$ vs 기본 5000초 이상) 단점은 있다.

- 가상환경 설정 안하고 시작했다 ㅋㅋ;
	- 오늘 데이터 수집한 다음 가상환경 만들어봄
	- 가상환경에서 실행 확인 -> `py`로 만들고 실행 확인 -> 도커에 올리기?

- 일본어 공부 중일 때 돌려놔야겠다;

#### 앞으로 할 일
1. **자동화 전까지 데이터 수동으로 수집**
2. 자동화 만들기
	1) 가상환경 -> `.py` 파일로 변환 -> `csv`에 저장
	2) `(1)`이 완료되면 도커를 이용하는 방법도 구상
3. 데이터가 1주일 정도 쌓이면 기간에 따른 리뷰 수, ccu, 할인에 따른 ccu 변화 등의 추이를 볼 수 있을 듯


### 230518
- `get_details()`에 날짜 column 추가
	- 순위를 벗어난 데이터는 `get_details()`로만 정보를 얻기 때문에 여기에 대한 날짜를 추가로 넣음
- 일단 `tag.csv`, `info_genre.csv`는 `key, value` 형태를 그대로 유지함
	- 이걸 쓰게 된다면 원핫인코딩을 할 거 같은데, 모아놨다가 필요할 때 원핫인코딩을 하는 게 나을 거 같음 :  들어올 때마다 원핫인코딩으로 처리하면 데이터프레임의 스키마가 달라질 수 있기 때문(없는 태그에 대해서는 feature가 안생기는데, 기존 데이터프레임에는 있었다고 하면 좀 골치아파짐)
- 데이터 수집 & 가공 과정 파이프라인으로 정리 중

- 머리로 생각하려니까 헷갈린다..
> 어떤 날 수집한 데이터와 이미 있는 데이터를 비교해보자
> 데이터는 크게 3가지 종류로 나뉜다.
> 1. `info.csv`에 이미 있으면서 5000개를 수집했을 때 이미 있는 데이터  
>
> 2. `info.csv`에 이미 있지만, 5000개를 수집했을 때는 나타나지 않는 데이터(순위에서 벗어남)  
> 3. `info.csv`에 없지만 5000개를 수집했을 때 나타나는 데이터  
1. 바로 날짜와 수치 정보만 떼내서 날짜&수치 테이블에 저장함
2. `detail`에 관한 쿼리를 날려서 오늘의 정보를 얻은 뒤, 날짜와 수치 정보를 떼내서 날짜&수치 테이블에 저장함
3. `detail`에 관한 쿼리를 날려서 오늘의 정보를 얻고, 얻은 정보를 4개의 테이블에 나눠 저장함


### 230517
- 어떤 날의 `ccu`는 모든 데이터 수집할 때 얻을 수 있다
- `detail`에 관한 정보는 새로운 게임이 생겼을 때, 해당되는 게임들만 수집하면 되며 얘네들은 날짜에 따라 업데이트되는 데이터가 아님(태그를 제거하기 떄문에)
- 따라서 데이터 수집 과정은 이런 식으로 정리할 수 있음
```
데이터 수집 과정
# 최초 실행
1. 상위 5000개 게임 수집 및 중복값, 결측치 처리
2. 5000개 게임의 지원 언어와 장르, 태그를 얻기 위해 `appdetails`를 리퀘스트
3. 얻은 데이터를 테이블을 구분해서 저장함
	- 게임 정보 : `appid, name, developer, publisher, initialprice`
	- 날짜와 수치 정보 : `name, ccu, positive, negative, average,median 2weeks`
	- 세부 정보(1) : name, 언어, 장르
	- 세부 정보(2) : name, 태그
- 언어, 장르, 태그는 `,`로 구분된 정보들이므로 일단 원핫인코딩을 처리함
- 정보는 일단 다 남겨둔다
---
# 2회 실행 이후
1. 상위 5000개 데이터 수집
2. 기존 게임 정보 테이블과 `appid`를 비교, 새로 생긴 데이터와 `appdetails`를 받아옴. 이미 수집된 적 있는 데이터라면 역시 `appid`를 이용해 계속적으로 추적함
3. 새로운 데이터는 4개로 나눠 저장함. 날짜와 수치 정보 테이블에 해당 정보들을 갱신함

# 추가
- 특정 시간마다 자동 실행되면 좋을 듯 -> 아마 `py` 파일로 바꾼 다음에 윈도우 스케줄러를 이용해야 할 듯
- 아니면 무한 반복 & 컨테이너를 쓰는 방법도 있을 듯
```

#### 자동화 관련
- 도커 컨테이너를 하나 띄운다음에, 무한히 반복되지만 특정 시간에만 실행되는 스크립트를 짜면 되지 않을까? 
- 근데 그러면 컨테이너 밖으로 파일을 빼는 방법을 생각해봐야겠다. 

#### 앞으로 할 일
1. 새로 추가된 데이터 : 디테일을 얻은 다음, `info, lang_genre, tag`에 정보 저장
2. 5000위 내에 있는 데이터 : 새로 데이터를 얻은 날짜의 수치 데이터를 `time_value`에 저장
3. 5000위를 벗어난 데이터 : 디테일을 얻은 다음 `time_value`에 저장
4. 위 과정 함수화 & 자동화
	- 하나씩 ㄱㄱ


### 230516
- `steamspypi` 다시 멀쩡하게 작동함 : 기존 약 4800여개의 데이터로 진행하려던 내용 수정
- 중복값 처리 이슈
	- `ccu`가 더 큰 데이터를 살림 : `sort` 이후 `drop_duplicate` 이용
> 원래 스크래핑하려고 했는데 기존 코드도 손볼 게 많다 많아

#### 앞으로 할 일(1.Collect_Preprocesssing)
```
1. `details` 수집하기
2. `genre` 분리해서 별도의 `column`으로 만들기
3. 데이터프레임을 `info`, `time_value`, `category`로 나눠서 보관하고, `time_value`는 날마다 수집하기
4. (가능하면) 자동화하기

- 이후는 탐색
```



## 230515 
- `steamspypi`가 이상함 : `download_all_pages()`를 해도 `Dota 2`부터 받아지는 게 아니라 이상한 게임 이름이 맨 앞으로 옴
- 그래서 이미 있는 데이터로 진행함
