
## 일지


### 230706
- 리뷰에 따른 평가 변동을 살펴보기 위해 리뷰 수집 & SQL에 저장하는 코드를 작성하는 중이다.

#### 이슈 - 리뷰는 어떤 자료형으로 저장해야 하는가?
- [VARCHAR과 TEXT의 차이 참고](https://leezzangmin.tistory.com/49)
- VARCHAR은 '바이트의 개수'를 저장함. 
- UTF-8로 인코딩된 한글 단일 글자는 3바이트이고, 스팀의 글자수 제한은 8000자이다.
- 그러면 해당 데이터를 저장하려면 `VARCHAR(24000)`을 저장해야 하는가? 아니다.
- 영어 1글자이든 한글 1글자이든 '바이트의 개수'는 1개로 동일하다. 둘의 용량은 1바이트와 3바이트로 다르더라도 말이다.
- 스팀의 글자수 제한이 8000자라는 이슈가 있고, 실제로 도배성 리뷰를 조회했을 때도 8000자인 리뷰가 있는 걸 봐서는 `VARCHAR(8000)`으로 지정하면 맞을 것 같음.

- SQL에 옮기려고 하는데, 유저 id의 길이가 너무 긺 -> 이거도 그냥 `varchar()`로 저장해도 될 거 같음(숫자로서의 의미가 없으니 BIGINT를 쓸 이유가 없음)

#### 소수 저장
- 일단 소수점 4번째 자리에서 round로 저장했음
- 부동소수점이 없어졌다는 이슈가 있어서 `decimal(N, D)` (N : 총 자릿 수 : 0.xxx면 4) / D : 소수점 자릿 수(0.xxx면 3)

#### Dict에서 키가 없는 경우
- 데이터를 수집하는데 어떤 데이터에서는 특정 키만 없는 경우가 있었음
- 이 때는 `dict.get(key, default_value)`을 통해 키가 없을 때 기본값을 반환할 수 있음

#### 같은 게임 리뷰인데 다른 appid를 가진 경우
- 이 때 리뷰를 썼다면 다른 게임에서도 해당 리뷰가 잡히는 경우가 있다
- 

### 230705
- 분석 시작 : 일단 한국 게임을 추리고 싶은데 수작업으로 추려야 할 듯 
	- 개발사 - 국적 데이터가 있으면 좋은데 그게 안 보이니까..
- 파라미터를 바꿀 때 정렬도 바꾸고 싶은데 그 방법을 못 찾겠다. 어떤 피쳐들만 딱딱 바꾸는 것이 아니어서.. : 이 부분은 **못한다고 큰일 나는 거 아니니까 지금은 신경쓰지 말자.**
- 스팀 자체에서 제공하는 API가 있어서 그걸 살펴봐야 할 것 같음.
	- 일단 리뷰 자체는 [이 링크](https://partner.steamgames.com/doc/store/getreviews)처럼 리퀘스트 작성하면 가능할 것 같은데?
- 한국어 리뷰를 수집하고 한국어만 분석을 할지, 아니면 전체 언어에 대해 하되 언어는 영어로 분석할지가 고민이다. 리뷰가 생각보다 많지가 않다. (글옵 한국어 리뷰가 900개 내외..)



### 230704
- SQL -> csv 전환 시 그냥 `error`로 뜨는 에러가 발생하는 경우가 있음 
	- 우분투 쉘이 뜨지 않은 상태라면, WSL의 도커는 작동하고 있지 않음 : 따라서 `33060` 포트로 엔진을 얻고 있는지 확인해보고, 그렇다면 포트를 3306으로 바꿔주든가 우분투 쉘을 띄운 다음 스크립트를 실행해야 한다. 

### 230620
- 일본어 공부하느라 바쁘다 ㅠㅠ
- 같은 날에 사이트가 수정돼서 데이터를 중복 수집하는 문제가 발생했음 : 사이트가 수정된 원인은 모르겠지만, 롤백된 것으로 보임
- 지금 데이터 수집 조건을 사이트에 있는 CS:GO의 5가지 피쳐가 내 DB의 5가지 피쳐와 같은지를 체크하고 있는데, 이런 상황이 발생하면 데이터를 중복해서 수집할 우려가 있음. 따라서 **글옵을 점검하는 조건을 취소하고, `time_value`에 오늘 날짜가 있는지만 점검**하겠음
	- `time_value`는 raw 데이터를 불러온 뒤, 파이썬에서 다시 `pd.to_sql()`로 한꺼번에 저장되므로 맞는 방법이라고 생각함. 문제가 생기면 그 때 고치면 되지~

### 230614
- 디테일 수집 : 날짜가 늘어남에 따라 디테일 갯수도 증가해야 하는데 6월 8일 전후로 그 갯수가 줄어들었음
- 체크 : 들여쓰기 상태는 상관 없고, **마지막 수집 이후에 `to_sql`이 빠져 있어서 그럼 : 수정 완료**
- 그 외에도 `get_detail`에서 저장하는 과정 함수로 따로 뺐음


### 230612
- 로컬에선 `lang_genre`, `tag`가 잘 저장되는데 컨테이너에선 저장되지 않았음 : 왜?


### 230609
- 어제 체크포인트를 저장하는 과정에서, `get_details` 함수가 반환하는 값은 `raw_detail`에 수집된 전체 데이터여야 하는데 마지막 체크포인트 부분만 반환되는 문제가 있다. 이를 수정함.
- `get_details`의 마지막 부분에서 `raw_detail`의 어제 날짜 부분만 긁어서 데이터프레임으로 받아오면 될 듯?

> 수집이 완료되면 `RAW_DETAIL`에 오늘 수집된 데이터들을 모두 불러오는 쿼리를 날려서 데이터를 가져오게끔 바꿈

#### 다시 스크립트가 왜 실행되지 않는가를 해결해야 함
- 스크립트 자체에 문제가 있을 확률이 높음
- chatGPT에 물어봐도 계속 질문 내용이 반영이 안된 대답을 한다. 돌겠다 ㅋㅋ


#### 답을 찾은 듯 하다?
`python script.py` -> `python -u script.py`
- `-u`를 추가해주면 의도한 로그들이 잘 뜸
- 그러면 `-u`가 무엇이며, 왜 위 스크립트랑 달리 아래 스크립트는 `-u`를 추가해야 `print()`문이 보이는 걸까? -> 블로그에 정리해둠
- `-u`는 출력을 바로바로 내보내는 역할을 함 : **`터미널`에서 실행한 경우는 상관 없는데, 중간에 다른 매개체를 끼면 `-u`를 지정해야 한다. 그렇지 않으면 버퍼가 쌓이기 전에는 로그가 보이지 않음.**



### 230608
- 서버가 이상한 상태인 경우나 점검 중인 상태에서 실행되면 `get_detail()`은 약 5000개의 데이터를 `appid`에 대한 쿼리 1개씩을 보내서 처리되게 됨.
- 현재는 5000개 전체를 받은 다음 저장 과정을 실행하는데, 중간에 인터넷 이슈 등이 발생할 경우 다시 처음부터 수집을 진행하게 된다. 1초에 1개의 쿼리를 리퀘스트하므로, 작업이 더 늘어지게 됨
- 따라서 중간에 집계된 것들을 저장하는 과정을 `get_detail` 내에 넣어야 할 것 같다.
- 데이터 수집 결과 자체는 `raw DB`에 저장하고 있기 때문에 한꺼번에 저장하는 스크립트 대신 중간중간에 저장하는 방식으로 구현하면 될 것 같음
- 근데 steamspy 리퀘스트 관련 오류는 내가 얻고 싶을 때 얻을 수 있는 게 아니라서, 오류가 발생한 날에 시도해야 할 것 같다. 일단은 함수만 만들어놓자.
- 이거에 영향을 받는 함수가 `check_today_raw_data`이다. 예전엔 데이터를 모두 수집한 다음 넣었기 때문에 이 함수가 유효했지만, 지금은 중단됨 -> 근데 중간에 날짜는 넣었네? -> 저 함수대로 실행하면 오늘은 더이상 실행할 필요가 없지만 실제로는 더 수집해야 하는 상황이 발생할 수도 있다. 

#### 컨테이너 켜놓고 냅두니까 작동했는데?
- 근데 다시 건드리니까 또 안됨. 미치겠다.
- 일단 내가 작성한 스크립트가 문제인지부터 확인하기 위해 컨테이너 내부 시간을 반환하는 파일을 하나 넣어놓고 그 파일도 컨테이너 시작 시 작동하도록 설정함
- 얘는 작동한다 : 그러면 스크립트가 문제라는 뜻.



### 230607
- 왜 실패로그도 안 뜨는 걸까?
- 테스트 : 잘못된 파이썬 이름 전달하기
	- 파일 이름 뒤에 `1`을 붙였음 
	- 로그 조회
	- `python: can't open file '/app/container_collect_data1.py': [Errno 2] No such file or directory` - 엔트리포인트는 작업 경로 잘 인식하는 거 확인 가능
- 테스트2 : 호스트 이름 이상하게 전달하기 (`steamspy-mysql111`)
	- **로그 반환 안함!**
- 테스트3 :
```dockerfile
ENTRYPOINT ["python", "container_collect_data.py"]

CMD ["--db-host", "steamspy-mysql"]
```
- `--db-host` 위치 변경
- `container_collect_data.py [-h] [--db-host MYSQL_HOST] container_collect_data.py: error: unrecognized arguments: steamspy-mysql` 에러 반환
- 테스트4:
```dockerfile
RUN chmod +x /app/container_collect_data.py

ENTRYPOINT ["python", "container_collect_data.py", "--db-host"]

CMD ["steamspy-mysql"]
```
- 권한도 솔직히 별 의미는 없는 듯

#### 진전
- restart: always를 collector에 안하고 mysql에 했었다 ㅋㅋㅋㅋㅋ collector에 하니까 2003 오류가 여러 번 뜨다가 1049오류로 바뀜
- **생각했던 것처럼 네트워크가 구축되기 전에 통신을 시도했던 것 같음.** 
- 스크립트에 `time.sleep(5)`을 넣고 2003번 예외는 함수를 바로 끝내게 구성했음
- 근데 또 컨테이너는 떠 있는데 아무런 로그가 없다. 미치겠네 ㄹㅇㅋㅋ

### 230606

- 컨테이너에 접근해서 동일한 커맨드로 스크립트를 실행하면 잘 작동한단 말임?
- 뭐가 문제인지 모르겠다 ㅠㅠ 오늘 거의 6시간 넘게 박았는데

#### 1. create_engine 및 connect 수 줄이기
- `SQLAlchemy`의 `create_engine`은 1번만 수행해도 되며, DB에 연결하고 끊는 과정은 `connect` 객체를 만들고, `close`로 닫으면 된다.
- 또한, 일반적으로 1번의 `connect`객체 내에서 많은 작업을 수행하는 것이 좋다고 한다. 네트워크 연결 설정, 인증, 설정 초기화 등의 작업이 반복되기 떄문이다.
- `df.to_sql(if_exists = 'append')`항에서 문제가 발생함 : 저렇게 지정했는데 새로운 데이터를 추가하는 게 아니라 새로 테이블을 만들려고 시도함 : 왜???
	- **엔진을 DB에 접근하지 않은 채로 만들었는데, 이 상태에서 `use db` 이후 `df.read_sql()`을 쓰는 것과 DB에 접근하는 엔진 자체를 이용하는 방법에는 차이가 있는 것으로 보인다.**
	- 따라서 엔진을 2개로 만들겠음 : RAW DB에 접근하는 엔진과 가공 데이터에 접근하는 엔진
	- 수정해보니까 엔진 이슈가 맞음! `df.to_sql()`을 이용할 때는 `db`까지 접근해 있는 엔진을 이용해야 함
- 위 과정을 아나콘다에서 했음 : 컨테이너까지 옮겨봄

#### 2. 그래도 안되는데 좀 다른 이유로 안됨
- 대충 `conn`에서 이슈가 생긴 것 같은데, 컨테이너에 올렸을 때 에러가 발생함
- `2003` 에러 : 연결할 수 없는 경우 발생한다고 하며, 해당 컨테이너에 `mysql-client`를 설치해봄
	- `default-mysql-client` 설치해도 안됨
			- 호스트명 : `steamspy-mysql root 0000 33060` (X, 에러 내용 동일)
			- 호스트명 : `localhost root 0000 33060` (X, 에러 내용 동일)

##### 다른 컨테이너를 만들어서 mysql 컨테이너에 연결을 시도해 봄(mysqladmin)
- 33060 포트는 인식하지 못했고, 3306 포트는 인식했음 : 즉, **3306 포트를 넣어야 맞는 것 같음**
- 그래도 안된다. 2003 에러.




### 230605
- **데이터를 수집하는 컨테이너를 올리고, 가능하다면 `docker-compose`까지 작성**해서 다 띄워놔보자.


### 230603~04
- 6월 3일은 해외 직구 환불 이슈로 하루를 날렸다. 그래도 매일 컴퓨터는 켜서 데이터 수집은 했음..
- 일단 호스트 OS에서 `MySQL` 컨테이너를 올린 다음, `localhost`와 `33060` 포트를 이용해 데이터를 수집하고 저장하는 과정은 구현했다.

### 230602
- 제일 먼저 할 일
1. 현재 Container에 저장하는 파일의 호스트 명을 `localhost`로 지정한 상태임 : Host OS에서 컨테이너에 저장하라면 `localhost:33060`으로 설정하면 된댔음 -> (사이트가 멀쩡하다면) 이걸 호스트 OS에서 실행해서 MySQL에 데이터가 저장되는지 확인할 수 있을 거임
	- 이게 잘 작동하면, 호스트 이름만 바꿔서 컨테이너에 파일 올리면 됨


### 230601
- 데이터를 수집하는 조건에 대해 생각해본다
	- 지금은 **수동으로**, 하루 중 특정 시간(오후 2시 전후)에 스크립트를 실행시켜서 데이터를 받고 있다.
	- 일부러 시간을 유지하고 있는데, `steamspy` 사이트가 하루에 1번 갱신되지만 그 정확한 시간이 나와있지 않기 때문이다.
	- 따라서, 지금까지는 사람이 신경을 써가면서 데이터를 일정한 시간에 받았다.
- 그런데 `appdetails`으로 특정한 id만 계속 추적한다면, 굳이 날짜를 신경쓸 필요가 없을 수 있다.
	- 사이트가 이상해지더라도 `appid`를 이용한 추적은 여전히 유효하다
	- 특정 appid의 데이터를 받음 -> 이미 있는 최근 날짜의 raw 데이터와 비교 -> 날짜에 따라 변하는 값들이 있는데, 이 값들이 동일하다면 이미 수집된 적이 있는 것이고, 값이 달라졌다면 데이터가 갱신되었다는 의미가 된다!
- 이걸 하는 이유는 예를 들면 이런 상황이 있기 때문임
> 한국 날짜로 6월 2일이 됐다고 하자. 
> 근데 steamspypi가 갱신되는 시간을 모른다. 
> 지금 방식으로는 6월 2일이 되면 스크립트를 또 실행할 수 있다. 날짜만 비교하기 때문.
> 그런데 사이트가 갱신되지 않았다면, 중복된 데이터를 받아오게 되는 것이다.

- 코드는 대충 이런 식으로 작성됨
```python
conn = get_connection(db = MYSQL_DB)

q = f"""SELECT * FROM {TABLE_TIME_VALUE} WHERE appid = 730 ORDER BY date DESC LIMIT 1"""
df = pd.read_sql(q, conn)
df = df.drop('date', axis = 1)
df

def check_same(data_sql, data_site: dict):
    if (data_sql.positive[0] == data_site['positive'] and
        data_sql.negative[0] == data_site['negative'] and
        data_sql.ccu[0] == data_site['ccu'] and
        data_sql.average_2weeks[0] == data_site['average_2weeks'] and
        data_sql.median_2weeks[0] == data_site['median_2weeks']):
        return True
    else:
        return False
```
- `appid = 730`은 글옵으로, 스팀에서 가장 피크 동접자가 많은 게임이므로 같은 날짜가 아닌 이상 모든 기간에 거쳐 정보가 다를 것이라 선정했음.
- 이거를 `check_today_executed`에 넣으면 되지 않을까?

### 230531
- **도커에 올리기 시작**
	- 하루에 1번 데이터를 수집함
	- 수집된 데이터를 MySQL에 옮김
	- 윈도우 스케쥴러를 이용해 스크립트를 지정하는 방법도 있지만, 클라우드를 사용하는 경우도 생각했을 때 도커를 사용해보는 것도 좋을 것 같다.

#### 할 일
- 일단 우분투 컨테이너 띄우고 명령어들 입력해서 작동하는 거 확인
	- mysql client 설치, 파이썬 라이브러리 설치, 로컬에 있는 스크립트 복붙
	- 이게 잘 작동하면 dockerfile, docker compose 파일 만들어서 데이터 넣는 것까지 확인해보겠음


### 230530
- 테스트 케이스 분리 : 수집 시간이 너무 길어져서
- 프로젝트 폴더에 옮겨 뒀음
- 

### 230529
- `pymysql.connect` 객체를 함수로 생성하고 테스트
	- 테스트는 `Collect_Preprocessing.ipynb` 파일에서 DB 변수만 바꿔주면서 하고 있음
- **저장은 SQLAlchemy를 이용하고, 불러오는 건 pymysql을 이용한다.**
	- `DatabaseError: Execution failed on sql 'SELECT name FROM sqlite_master WHERE type='table' AND name=?;': not all arguments converted during string formatting` 에러 관련.
- **그냥 `SQlAlchemy`로만 데이터 저장하고 불러오는 게 낫지 않나?** 라고 생각해서 스크립트를 바꿈(steamtest 데이터 수집 잘 작동하면 ㅇㅋ)
- 그렇다고 `pymysql`이 필요 없는 건 아님 : sqlalchemy는 pymysql이 필요함!
- `to_sql()`관련



### 230526
- 자동화에 앞서 프로세스 정리(사이트가 뻑난 경우는 제외)
```
1. 오늘 실행된 적 있는가 검토
	- 기준 1 : time_value.csv가 있는가 -> 없다면 최초 실행
	- 기준 2 : time_value.csv의 마지막 데이터가 코드 실행 하루 전의 날짜인가
		- 그렇다면 이미 실행되었으므로 함수 실행
		- 아니라면 오늘의 데이터만 수집

2-1. 최초 실행
	1) 5000개의 데이터를 수집함
	2) 수집한 데이터들의 appid를 바탕으로 다시 detail에 관한 appid를 수집함
	3) 수집된 데이터들을 4개의 데이터프레임으로 나눈 뒤,  csv와 MySQL에서도 데이터베이스와 테이블을 만들고 저장

2-2. 2회 이상부터
	1) 5000개의 데이터를 수집함
	2) appid를 기준으로 데이터를 3가지로 나눔 
		<1> 이미 있는 appid이면서 또 수집된 데이터
		<2> 이미 있는 appid이면서 이번엔 수집되지 않은 데이터
		<3> 기존에 없는 appid
	3) <2>와 <3>은 detail을 다시 추적해야 함
		<2> : 오늘의 데이터가 추적되지 않았음 
		<3> : 테이블에 추가할 정보가 새로 필요함.
	4) 시간과 관련된 정보는 <1> ~ <3> 모두에 해당하며, 해당 테이블(1개)에 저장함. <3>의 경우 새로 생긴 데이터이므로 시간에 관련없는 테이블(3개)에 새로 추가함

```
- SQL에 저장하는 경우만 생각한다면 csv에 따른 프로세스를 sql을 조회하는 형태로 바꿔야 함
- **데이터 수집 기준 등을 모두 MySQL로 변경**
- `csv`파일은 Tableau에 연결할 때 등 데이터를 분석하기 위해서만 이용하겠음
	- 혹시나 파일에 엑셀 등으로 접근했다가 실수로 저장한 경우 인코딩이 흐트러지기 때문에 csv로 저장하는 건 좀 위험할 수도 있겠다는 생각이 들었음
  

#### 생각해볼 점
- **raw 데이터의 csv파일을 왜 모아두는가?**
	- 데이터 수집 시에는 눈치채지 못하다가, 나중에 보니까 데이터가 이상하게 수집되는 상황이 있을 수 있음
	- 그런데 가공 전 데이터를 갖고 있지 않다면, 해당 데이터들을 버려야 하는 상황이 발생할 수도 있음
- 그러면 SQL에도 원본 데이터를 저장해두자
	- 방식은 동일하게 
		- `all` : 디테일 정보가 빠진 원본 데이터
		- `detail` : detail로 그날그날 검색한 정보

### 230525
- MySQL에 저장하는 과정 계속 하기
	- MySQL의 `time_value` 테이블 : 다른 테이블은 `PK`로 `appid`를 지정해도 되지만, 얘는 굳이 그럴 필요 없다 : 날짜에 따라 `appid`를 가지므로, 여러 날짜라면 `appid`가 여러 개일 수 있기 때문이다.

#### 테이블 생성
- **문자로 들어오는 COLUMN들의 `VARCHAR()`값을 얼마로 할까?**
- MySQL 5.0버전 이후로는 `varchar()`에 들어가는 숫자는 **문자의 갯수**라고 한다. 바이트 계산할 필요 없음.
	- 추가로, `VARCHAR(255)`에 관련한 이야기가 있음 : 길이를 표현하는 별도의 공간이 필요한데, `255`까지는 1바이트이지만 `256`부터는 2바이트이므로 데이터의 구조 자체를 뜯어고쳐야 한다는 것.
	- 찾아보면 `255`가 글자라는데 바이트를 다루듯이 쓰고 있어서 헷갈린다. 일단 글자수로 취급함. 
	- `info`의 경우 모든 데이터가 255자를 넘지 않는 상황이지만, `,`로 구분된 데이터로 저장하고 있기 때문에 새로운 데이터가 255자를 넘을 수 있어서 `varchar(255)`로 저장함.
	- 하나 빼고 다 `varchar(255)`로 지정
- 숫자에서도 비슷한 이슈가 있는데, `df.describe()`로 최댓값을 확인한 다음 그 값에 맞게 자료형을 지정했음.



### 230524

- **문제 상황** 해결
	1. 엑셀 파일 `utf-8`로 지정 -> 하나하나 수정함, 이 과정에서 5월 21일 데이터 날아간 거 참고!
	2. 문제 발생했던 날들의 데이터 취합
		- 결측치 처리를 안했던 듯 하다 
			-  `developer`, `publisher`의 결측치 처리 - 그냥 비워놔도 된단다. 
			- 심지어 판다스에서 `NaN`값은 SQL 저장 시 `NULL`로 들어간다고 함.
- `set()`으로 `appid` 간의 차이를 구하는 것과 `drop_duplicates(subset = ['appid']`의 차이도 있다. 왜???
	- 예전 `info` 테이블에 중복된 값이 있었던 것으로 판명... 
	- 처음 5000개의 파일을 수집할 때 중복을 제거하는 과정을 분명 넣었음
	- 근데도 중복 데이터가 있다? 
	- 이런 가능성이 있을 수 있겠다
		- 기존에 수집된 데이터가 있음 -> 5000위 밖에 있다가 진입한 동명의 게임이 있음(다른 appid) -> **추가하는 상황에서는 중복 데이터 체크 없이 csv파일에 들어가기만 하므로, 다른 appid, 같은 이름의 게임이 추가될 수 있음**
	- 수정) 데이터를 추가하기 전, 기존 `info.csv`와 비교해서 이름이 동일한 데이터가 새로 생겼다면, **기존의 데이터만 유지**함 (`add_data_to_csv` 수정)


#### 문제 상황 1. 
![[Pasted image 20230524133331.png]]
- 기존 시간 값에 대한 테이블에 에러가 이런 식으로 발생한 상황임. 또한 `encoding_errors = 'ignore'`을 설정해야만 pandas로 접근할 수 있음

- 근데 새로 만든 테이블은 
![[Pasted image 20230524133430.png]]
- 이런 식으로 잘 들어와 있음.
- 일단 `pandas`는 한자에 대한 데이터 수집을 했을 때 화면에 잘 보여준다
- 반면 엑셀이나 워드패드로 csv 파일을 조회하면 아래와 같이 뜸
![[Pasted image 20230524134432.png]]
- 찾아보니까 **`pandas`의 기본 인코딩 포맷은 `utf-8`인데, 엑셀은 `ANSI`를 사용하기 때문인 듯?**
- 저장을 안했으면 상관이 없는데, 엑셀에서 다시 저장을 누르면 문제가 발생

#### 문제 상황 2.
- `set`을 이용해서 중복된 정보를 제거하는 과정 vs `drop_duplicates`를 이용한 과정 

#### 문제 상황 3.
![[Pasted image 20230524181456.png]]
- `appid`가 동일하고 `name`이 다른 데이터들의 경우이다. 위는 새로 수집된 데이터이고 아래는 기존에 유지되고 있던 데이터이다.
- 기존에 전제했던 게 약간 틀어진 상황이므로, `appid`가 다르다면 다른 게임으로 간주하겠음
- 그 와중에 `discount = 0`인데 가격 오락가락하는 거 실화냐?



### 230523
#### 문제 상황 발생
- 데이터 수집 과정 중 예상치 못한 상황 발생 : 5000개의 데이터를 수집했는데 모두 기존에 없던 데이터여서 또 5000개의 디테일을 수집하려고 함
	- `time_value_df`를 살펴보니까 전부 `float`로 저장되어 있음 : 근데 값을 보면 막상 소수값은 없음 -> 아마 `NaN` 값이 생기면서 다른 csv 파일들을 불러왔을 때는 `int`로 잘 저장되어 있음
- 또, `time_value.csv`의 `discount`에는 `NULL`값도 있음

#### 원인 추정
- `appdetail`을 불러올 때 기존에 있는 앱 이름을 불러오는 곳을 `info`에서 `time_value`로 바꿨는데, 이게 문제가 된 듯
> 1. 테이블을 보니까 아예 처음부터 Null값이 발생했던 것 같음 
> - `time_value`에서만 발생한 문제로 추정, 다른 테이블을 같은 id로 조회해보면 데이터들이 멀쩡하게 잘 들어가 있음




### 230522
#### 진행중
- 데이터 수집이 잘 되면 SQL에 저장하는 방법도 시도하기(진행중)
	- SQL은 MySQL을 쓰기로(넥슨이 MySQL을 쓰는 것 같아서 ㅎ)

#### 완료
-  사이트 뻑이 생각보다 자주 나는 편임 : 일본어 공부할 때 데이터 수집을 미리 해두자
- 가상환경 설정해서 `.py` 파일로 데이터 수집이 잘 되는지 테스트하기
	- 가상환경 설정 시 `Windows Shell`을 이용하자(`cmd`는 리눅스 커맨드를 인식 못함)
	- `Scripts/activate` 입력해주면 됨 : [[가상환경 세팅법]] 참고

### 230519
- 오늘 데이터를 수집하려고 했으나 다시 5월 15일과 같은 에러가 발생함. 보유자 순으로 상위 5000개 데이터가 보여야 하는데, 중간의 어딘가부터 보이는 상황
	- 만약 **이러한 에러가 발생했다면, `info.csv`에 이미 있는 `appid`들만 다시 수집해야겠다** : `request = all`에서만 에러가 발생하고 `request = appdetails`는 잘 작동하고 있기 때문이다.
	- 훨씬 오래 걸릴 수밖에 없는(5분 + $\alpha$ vs 기본 5000초 이상) 단점은 있다.

- 가상환경 설정 안하고 시작했다 ㅋㅋ;
	- 오늘 데이터 수집한 다음 가상환경 만들어봄
	- 가상환경에서 실행 확인 -> `py`로 만들고 실행 확인 -> 도커에 올리기?

- 일본어 공부 중일 때 돌려놔야겠다;

#### 앞으로 할 일
1. **자동화 전까지 데이터 수동으로 수집**
2. 자동화 만들기
	1) 가상환경 -> `.py` 파일로 변환 -> `csv`에 저장
	2) `(1)`이 완료되면 도커를 이용하는 방법도 구상
3. 데이터가 1주일 정도 쌓이면 기간에 따른 리뷰 수, ccu, 할인에 따른 ccu 변화 등의 추이를 볼 수 있을 듯


### 230518
- `get_details()`에 날짜 column 추가
	- 순위를 벗어난 데이터는 `get_details()`로만 정보를 얻기 때문에 여기에 대한 날짜를 추가로 넣음
- 일단 `tag.csv`, `info_genre.csv`는 `key, value` 형태를 그대로 유지함
	- 이걸 쓰게 된다면 원핫인코딩을 할 거 같은데, 모아놨다가 필요할 때 원핫인코딩을 하는 게 나을 거 같음 :  들어올 때마다 원핫인코딩으로 처리하면 데이터프레임의 스키마가 달라질 수 있기 때문(없는 태그에 대해서는 feature가 안생기는데, 기존 데이터프레임에는 있었다고 하면 좀 골치아파짐)
- 데이터 수집 & 가공 과정 파이프라인으로 정리 중

- 머리로 생각하려니까 헷갈린다..
> 어떤 날 수집한 데이터와 이미 있는 데이터를 비교해보자
> 데이터는 크게 3가지 종류로 나뉜다.
> 1. `info.csv`에 이미 있으면서 5000개를 수집했을 때 이미 있는 데이터  
>
> 2. `info.csv`에 이미 있지만, 5000개를 수집했을 때는 나타나지 않는 데이터(순위에서 벗어남)  
> 3. `info.csv`에 없지만 5000개를 수집했을 때 나타나는 데이터  
1. 바로 날짜와 수치 정보만 떼내서 날짜&수치 테이블에 저장함
2. `detail`에 관한 쿼리를 날려서 오늘의 정보를 얻은 뒤, 날짜와 수치 정보를 떼내서 날짜&수치 테이블에 저장함
3. `detail`에 관한 쿼리를 날려서 오늘의 정보를 얻고, 얻은 정보를 4개의 테이블에 나눠 저장함


### 230517
- 어떤 날의 `ccu`는 모든 데이터 수집할 때 얻을 수 있다
- `detail`에 관한 정보는 새로운 게임이 생겼을 때, 해당되는 게임들만 수집하면 되며 얘네들은 날짜에 따라 업데이트되는 데이터가 아님(태그를 제거하기 떄문에)
- 따라서 데이터 수집 과정은 이런 식으로 정리할 수 있음
```
데이터 수집 과정
# 최초 실행
1. 상위 5000개 게임 수집 및 중복값, 결측치 처리
2. 5000개 게임의 지원 언어와 장르, 태그를 얻기 위해 `appdetails`를 리퀘스트
3. 얻은 데이터를 테이블을 구분해서 저장함
	- 게임 정보 : `appid, name, developer, publisher, initialprice`
	- 날짜와 수치 정보 : `name, ccu, positive, negative, average,median 2weeks`
	- 세부 정보(1) : name, 언어, 장르
	- 세부 정보(2) : name, 태그
- 언어, 장르, 태그는 `,`로 구분된 정보들이므로 일단 원핫인코딩을 처리함
- 정보는 일단 다 남겨둔다
---
# 2회 실행 이후
1. 상위 5000개 데이터 수집
2. 기존 게임 정보 테이블과 `appid`를 비교, 새로 생긴 데이터와 `appdetails`를 받아옴. 이미 수집된 적 있는 데이터라면 역시 `appid`를 이용해 계속적으로 추적함
3. 새로운 데이터는 4개로 나눠 저장함. 날짜와 수치 정보 테이블에 해당 정보들을 갱신함

# 추가
- 특정 시간마다 자동 실행되면 좋을 듯 -> 아마 `py` 파일로 바꾼 다음에 윈도우 스케줄러를 이용해야 할 듯
- 아니면 무한 반복 & 컨테이너를 쓰는 방법도 있을 듯
```

#### 자동화 관련
- 도커 컨테이너를 하나 띄운다음에, 무한히 반복되지만 특정 시간에만 실행되는 스크립트를 짜면 되지 않을까? 
- 근데 그러면 컨테이너 밖으로 파일을 빼는 방법을 생각해봐야겠다. 

#### 앞으로 할 일
1. 새로 추가된 데이터 : 디테일을 얻은 다음, `info, lang_genre, tag`에 정보 저장
2. 5000위 내에 있는 데이터 : 새로 데이터를 얻은 날짜의 수치 데이터를 `time_value`에 저장
3. 5000위를 벗어난 데이터 : 디테일을 얻은 다음 `time_value`에 저장
4. 위 과정 함수화 & 자동화
	- 하나씩 ㄱㄱ


### 230516
- `steamspypi` 다시 멀쩡하게 작동함 : 기존 약 4800여개의 데이터로 진행하려던 내용 수정
- 중복값 처리 이슈
	- `ccu`가 더 큰 데이터를 살림 : `sort` 이후 `drop_duplicate` 이용
> 원래 스크래핑하려고 했는데 기존 코드도 손볼 게 많다 많아

#### 앞으로 할 일(1.Collect_Preprocesssing)
```
1. `details` 수집하기
2. `genre` 분리해서 별도의 `column`으로 만들기
3. 데이터프레임을 `info`, `time_value`, `category`로 나눠서 보관하고, `time_value`는 날마다 수집하기
4. (가능하면) 자동화하기

- 이후는 탐색
```



## 230515 
- `steamspypi`가 이상함 : `download_all_pages()`를 해도 `Dota 2`부터 받아지는 게 아니라 이상한 게임 이름이 맨 앞으로 옴
- 그래서 이미 있는 데이터로 진행함
