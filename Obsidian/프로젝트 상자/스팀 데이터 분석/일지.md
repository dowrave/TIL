## 230517
- 어떤 날의 `ccu`는 모든 데이터 수집할 때 얻을 수 있다
- `detail`에 관한 정보는 새로운 게임이 생겼을 때, 해당되는 게임들만 수집하면 되며 얘네들은 날짜에 따라 업데이트되는 데이터가 아님(태그를 제거하기 떄문에)
- 따라서 데이터 수집 과정은 이런 식으로 정리할 수 있음
```
데이터 수집 과정
# 최초 실행
1. 상위 5000개 게임 수집 및 중복값, 결측치 처리
2. 5000개 게임의 지원 언어와 장르, 태그를 얻기 위해 `appdetails`를 리퀘스트
3. 얻은 데이터를 테이블을 구분해서 저장함
	- 게임 정보 : `appid, name, developer, publisher, initialprice`
	- 날짜와 수치 정보 : `name, ccu, positive, negative, average,median 2weeks`
	- 세부 정보(1) : name, 언어, 장르
	- 세부 정보(2) : name, 태그
- 언어, 장르, 태그는 `,`로 구분된 정보들이므로 일단 원핫인코딩을 처리함
- 정보는 일단 다 남겨둔다
---
# 2회 실행 이후
1. 상위 5000개 데이터 수집
2. 기존 게임 정보 테이블과 `appid`를 비교, 새로 생긴 데이터와 `appdetails`를 받아옴. 이미 수집된 적 있는 데이터라면 역시 `appid`를 이용해 계속적으로 추적함
3. 새로운 데이터는 4개로 나눠 저장함. 날짜와 수치 정보 테이블에 해당 정보들을 갱신함

# 추가
- 특정 시간마다 자동 실행되면 좋을 듯 -> 아마 `py` 파일로 바꾼 다음에 윈도우 스케줄러를 이용해야 할 듯
- 아니면 무한 반복 & 컨테이너를 쓰는 방법도 있을 듯
```

#### 자동화 관련
- 도커 컨테이너를 하나 띄운다음에, 무한히 반복되지만 특정 시간에만 실행되는 스크립트를 짜면 되지 않을까? 
- 근데 그러면 컨테이너 밖으로 파일을 빼는 방법을 생각해봐야겠다. 

#### 앞으로 할 일
1. 새로 추가된 데이터 : 디테일을 얻은 다음, `info, lang_genre, tag`에 정보 저장
2. 5000위 내에 있는 데이터 : 새로 데이터를 얻은 날짜의 수치 데이터를 `time_value`에 저장
3. 5000위를 벗어난 데이터 : 디테일을 얻은 다음 `time_value`에 저장
4. 위 과정 함수화 & 자동화
	- 하나씩 ㄱㄱ


## 230516
- `steamspypi` 다시 멀쩡하게 작동함 : 기존 약 4800여개의 데이터로 진행하려던 내용 수정
- 중복값 처리 이슈
	- `ccu`가 더 큰 데이터를 살림 : `sort` 이후 `drop_duplicate` 이용
> 원래 스크래핑하려고 했는데 기존 코드도 손볼 게 많다 많아

#### 앞으로 할 일(1.Collect_Preprocesssing)
```
1. `details` 수집하기
2. `genre` 분리해서 별도의 `column`으로 만들기
3. 데이터프레임을 `info`, `time_value`, `category`로 나눠서 보관하고, `time_value`는 날마다 수집하기
4. (가능하면) 자동화하기

- 이후는 탐색
```



## 230515 
- `steamspypi`가 이상함 : `download_all_pages()`를 해도 `Dota 2`부터 받아지는 게 아니라 이상한 게임 이름이 맨 앞으로 옴
- 그래서 이미 있는 데이터로 징행함
