- 정규분포에는 확률 변수가 여러 개 올 수 있으며, 일변량, 이변량, 다변량 정규분포로 나뉜다.
- 여기선 변수 2개를 먼저 살펴봄

- 두 확률 변수는 정규 분포를 따름
$$
X_1 -> N(\mu_1, \sigma^2_1), X_2 -> N(\mu_2, \sigma^2_2)
$$
> 원래는 ~인데 LaTeX에서 ~가 안들어감;

- 확률 벡터는
$$
X = \left(\begin{matrix} X_1 \\ X_2\end{matrix} \right) 
$$

### 분산-공분산 행렬

- 모집단 **분산-공분산 행렬($\Sigma$)** 은 확률 변수가 2개이므로 2X2 행렬이다. 
	- 주대각성분은 $X_1, X_2$의 분산이다. (모분산) 
	- 이를 제외한 성분들은 두 확률벡터 사이의 **공분산(Covariance)** 으로, 두 변수 $X_1, X_2$가 동시에 변하는 정도를 뜻한다.

$$
\Sigma = \left(\begin{matrix}\sigma_1^2 & \sigma_{12} \\
\sigma_{21} & \sigma_2^2\end{matrix}\right)
$$

### 공분산
- 모집단 **공분산** $\sigma_{12}$ = $cov(X_1, X_2) = \frac{\Sigma(X_{1i} - X_1)(X_{2i} - X_2)}{n}$
- 모집단 **상관계수** $\rho = corr(X_1, X_2) = \frac{cov(X_1, X_2)}{\sqrt{var(X_1)var(X_2)}}$

### 결합확률밀도함수
$$
\begin{matrix}
f(x_1, x_2)= \frac{1}{2\pi\sigma_1\sigma_2\sqrt{1-\rho^2}}exp\left[
\frac{1}{2(1-\rho^2)}\left\{
\left(
\frac{x_1 - \mu_1}{\sigma_1}
\right)^2 - 
2\rho 
\left( \frac{x_1 - \mu_1}{\sigma_1}\right)
\left( \frac{x_2 - \mu_2}{\sigma_2}\right) +
\left( \frac{x_2 - \mu_2}{\sigma_2}\right) ^2
\right\}
\right]\\
-\infty < x_1, x_2  < + \infty \\
-\infty <  \mu_1, \mu_2 < + \infty \\
-1 < \rho < 1
\end{matrix}
$$
- 확률 변수가 2개이므로 확률밀도함수를 결합확률밀도함수라고 한다. 
	- 변수 각각이 정규 분포를 띄므로 2개의 정규분포확률밀도를 곱한 다음, 상관계수$\rho$가 반영된 식이다. 

> 예를 들면 정규분포를 띄는 키와 몸무게의 상관계수를 알면 170cm 이상의 키를 갖고 70kg 이상의 몸무게를 가질 확률을 계산할 수 있다.


### 이변량 정규 분포(BVN)
- 위 분포를 기호로 분포 표현하면 아래와 같다
$$

{X_1 \choose X_2} = BVN(\mu_1, \mu_2, \sigma^2_1, \sigma^2_2, \rho)

$$
- BVN : BiVariate Normal Distribution(이변량 정규분포)


#### 예제 
##### 1. 평균 (0, 0), 공분산행렬 `[2, 0] , [0, 2]`
![[Pasted image 20230510235138.png]]
- 두 변수가 관계 없을 경우 상관계수가 0이므로 공분산도 0이 된다.

##### 2. 분산이 다른 경우 : 공분산행렬 `[4, 0], [0, 1]`과 `[1, 0], [0, 4]`
![[Pasted image 20230510235331.png]]
![[Pasted image 20230510235346.png]]

- **분산이 더 큰 방향으로 값이 퍼지는 것을 알 수 있다.** 
- 공분산이 없기 때문에 $X_1, X_2$ 중 한 방향의 축으로 퍼진다.

##### 3. 두 변수의 분산이 같고 공분산이 다를 경우
`[4, 3.5] [3.5, 4]`와 `[4, -3.5] [-3.5, 4]`

![[Pasted image 20230510235709.png]]
![[Pasted image 20230510235717.png]]
- 분산이 같기 떄문에 평균을 기준으로 X, Y 축으로 퍼진 정도는 같으나, **데이터가 퍼지는 방향은 두 축 사이의 어딘가일 것이다**

---
## **실무**에서의 관계 파악
- **상관계수나 산포도**를 통해 관계를 파악한다.
- 관측치가 많다면 모든 관측치로 산포도를 그리지 않고 **표집을 한 다음 산포도**를 그린다.

### 상관계수
- **공분산을 각 변수의 표준편차로 나눈 값**
- `[-1, 1]`의 범위를 갖는다. 0에 가까울수록 상관성이 없다.
![[Pasted image 20230511000657.png]]
- 평균과 표준편차가 0, 2로 동일한 상황에서 **상관계수만 0.95(High), 0.7(Low)로 +, -을 따로 준 산점도**이다.
- **상관계수가 높으면 분포가 좁게 나타나고, 낮으면 넓게** 나타남
- **양의 상관계수는 우상향, 음의 상관계수는 우하향**하는 모양을 나타냄

#### 파이썬에서 상관계수는 `.corr()`함수로 쉽게 구할 수 있다. 산포도를 꼭 그려야 할까?
- 분석에서 제일 중요한 건 필요한 가정 조건을 만족했는지 여부이다.
- 연관성 분석을 할 때 **상관계수에만 의존하면 안되며, 산포도로 가정 조건이 충족되는지도 확인이 필요**하다.

#### 상관 연구의 기본 가정

1. 상관 연구의 기본 가정은 두 변수가 **선형성**을 가져야 한다
	- 증가하다가 감소 : 비선형(2차함수)
	- 이 떄는 구간을 나눠서 구간별로 상관분석을 하는 방법이 있음

2. 상관 연구의 다른 가정으로는 **등분산성**이 있다 
- 등분산성이란, 직선을 따라 분포된 데이터들이 **흩어진 정도가 같아야 한다**는 것이다(값이 커질수록 넓게 퍼지면 안된다는 뜻)
	- 값의 크기에 따라 분산이 달라지는 것을 이분산성(Heteroscedasticity)이라고 한다.
	- **등분산성은 선형회귀모델, ANOVA, 시계열 등에서도 확인**한다.
	- 등분산성을 확인하는 방법에는 산포도 이외에도 Bartlett's test, Levene's test 등이 있다.
	- 이분산성을 **해결하는 방법으로는 로그 변환이나 거듭곱 변환(파워 변환)** 이 있는데, 이상치에 의해 이분산성이 발생할 수 있기 떄문에 변환 전후에 생길 장단점을 잘 비교해야 한다.

3. 두 변수에 **이상치가 없어야 한다.**
- 통계치만으로 이상치를 확인할 수 없기 때문에 산포도를 먼저 그려볼 필요가 있다.

- 상관분석에서 다루는 변수는 **독립변수 간, 독립변수 - 종속변수** 모두를 포함한다.
- 그렇다면 독립변수 끼리의 연관성이 높은 상황은 어떨까?

## 공선성, 다중공선성
- **공선성(Collinearity)** : 하나의 독립 변수가 다른 독립변수와 높은 연관성이 있는 경우
- **다중공선성(MultiCollinearity)** 
	- 한 독립변수가 다른 여러 개의 독립변수로부터 예측할 수 있는 경우
	- 여러 개의 독립변수끼리 상관성을 가진 경우

> 실제로는 2개를 뚜렷한 구분 없이 비슷한 의미로 사용함

--- 
#### 다중공선성 발생상황

> 1. 1개의 정보가 여러 단위로 측정되어 다른 변수로 저장된 경우 : 섭씨와 화씨, 평수와  제곱미터 등 // 열 이름이 명확하지 않거나, 너무 많다면 어떻게 해야 할까?

> 2. 미가공된 데이터를 취합하는 과정에서 상관성이 높은 데이터가 포함된다.
> 예를 들면 집 데이터에서 집 크기, 방 개수, 화장실 개수 등이 있다고 할 때, 집 크기와 방들의 개수는 상관성이 있다. 근데 이러한 상관성이 높은 변수를 모두 쓰고 싶을 수도 있다.

> 3. 피쳐 엔지니어링 사용 시 자주 쓰는 방법 중 하나는 k-NN이다. k를 이용해 생긴 이웃들에 대해 평균, 중앙값 등 여러 통계치로 여러 피쳐를 만들 수 있다. 피쳐 끼리 상관성이 높아지는데, 여러 피쳐를 만들어도 괜찮을까?

---
- 다중공선성은 **VIF(분산팽창계수 : Variation Inflation Factor)와 산점도로 파악**한다.

#### VIF : 분산팽창계수
- 다중 회귀 모델에서 독립변수끼리 상관관계가 있는지 측정하는 정도를 말한다.
$$
\begin{matrix}
VIF = \frac{1}{1-R^2}  \\
R^2 = \frac{\Sigma(\hat{Y_i} - \bar{Y})^2}{\Sigma(Y_i - \bar{Y})^2}
\end{matrix}
$$
- 설명된 편차: $\hat{Y_i} - \bar{Y}$ 
- 총 편차 : $Y_i - \bar{Y}$
- **결정 계수 $R^2$**  `Coefficient of Determination`
- 회귀선을 통해 **종속변수와 설명할 수 있는 부분의 오차**와 **설명되지 않은 편차**로 나눌 수 있다. 이 둘을 합친 게 **총 편차**이다.
- **결정계수는 총 편차 대비 설명된 편차의 비율**을 말하며 상관관계가 높을수록 1에 가까워진다.

- 다중공선성을 가지면 이 변수들로 회귀모델을 만들었을 때 높은 $R^2$값을 얻을 수 있다. 
- 일반적으로 **$VIF$의 값이 5보다 큰 경우 다중공선성이 있다**고 해석한다.

![[Pasted image 20230511003200.png]]
- 각 표본의 평균은 10이며, 표준편차는 `[1, 2, 3, 4]`이다.  `0.85`의 상관계수를 이용해 공분산 값들을 얻었다.
- 상관계수 행렬로, `col5`는 `col1*3 + 10`으로 얻은 뒤 `df.corr()`을 가한 식이다.
	- 즉 1번 열과 5번 열은 상관관계가 높을 수밖에 없음

- 위 식에서 VIF를 구하면 아래와 같이 나타난다. 파이썬의 `statsmodels.stats.outliers_influence.variance_inflation_factor`에 있음.
![[Pasted image 20230511003726.png]]
- `col5`는 `col1`로 얻었으니 VIF값이 높다.
- 이 상태에서 `col1`을 지우면 다중공선성을 해결할 수 있을까?
![[Pasted image 20230511003959.png]]
- 데이터를 높은 상관계수로 만들었기 때문에, VIF 값이 전체적으로 올라간 것을 볼 수 있다.

#### 다중공선성 문제 상황
- 특히 **회귀모델**에서 문제가 되는 이유는, **독립변수끼리 독립이어야 한다는 가정이 망가지기 떄문**이다. 이 상황에서 얻은 회귀 모델은 계수를 신뢰할 수도 없고 모델을 잘못 해석하게 되기도 한다.
- 이 상황에서는 **트리 기반 알고리즘**을 사용하면 상관성이 높은 피쳐를 쓰더라도 예측 성능에 영향을 주지 않기 때문에 상관성이 높은 피쳐를 여러 개 사용하기도 한다.

- 그렇다면 회귀 모델을 쓰지 않을 때는 다중공선성이나 상관관계를 확인해야 할까? 다중공선성을 해결하는 방법은 무엇이 있을까? VIF가 낮아질 떄까지 변수를 지워야 할까?

## 차원의 저주 

- 변수가 많을 때 발생하는 문제들
1. 데이터 이해 및 업무에 필요한 변수 파악에 소요되는 시간
2. 메모리 및 저장 용량 이슈
3. 다중공선성 문제

- 1번 문제는 항상 발생하며, 
	- **중복된 열**이 있는지
	- **각 변수의 정보를 모른다면 VIF를 계산**하면서 비정상적인 수치가 있는지 등을 확인해야 한다.

- 2번 문제의 경우 아래 사항들로 줄일 수 있음
	- `float64` 대신 `float32`, 
	- `int64` 대신 `int32`
	- `object` 대신 `category`
		- 문자열을 직접 저장하는 대신 인덱스-문자열 조합 중 인덱스만 저장
	- 불필요한 값 저장 X
	- 데이터프레임 대신 리스트 / 딕셔너리 사용
	- 하드웨어(클라우드 컴퓨팅 파워) 변경

#### 차원의 저주
- **변수가 너무 많아**지면서 데이터 간의 **패턴을 찾는 게 힘들어지고 모델 성능이 떨어짐**
> 예시 : 50개의 점을 1 ~ 100까지 있는 1차원, 2차원, 3차원 공간에 무작위로 배치한다
> 1차원은 50개의 점을 100개 속에 넣는다
> 2차원은 50개의 점을 1만개 속에 넣는다
> 3차원은 50개의 점을 100만개 속에 넣는다


- 관측치 수가 동일할 때 **차원이 하나 늘수록 존재하는 데이터 공간이 기하급수적으로 증가**하고, 관측치 간의 빈 공간이 늘어나는 현상(Sparsity)이 나타난다.
- 데이터 간의 거리, 패턴을 이용하는 알고리즘에서 성능 저하가 더 뚜렷하게 나타난다.

- `k-NN`의 경우 꼭 필요한 최소한의 변수들만 사용해야 한다.


## 차원의 저주 해소하기 : PCA
- **주성분분석(PCA : Principle Component Analysis)은 다중공선성을 해결하면서 고차원의 데이터를 저차원의 데이터로 바꾼다.**
- 주성분분석의 목표는 **상관관계가 있는 k개의 변수를 상관관계가 없는 k보다 적은 변수로 변환하는 것**이다.
	-  이 때, 변환된 저차원은 k차원에 있는 정보를 최대한 담고 있어야 한다.

### 정보와 정보 손실
> 예제 : 30대, 40대가 있고, 특성 X와 Y가 있다고 하자.
>    평균 | X : 9.5, Y : 8.83
>    분산 | X : 0.3, Y : 18.97
> 30대와 40대가 어떤 특성을 나타내는지를 알고 싶다고 하자.

1. X의 분산이 거의 없는데, 아예 0이라고 가정해보자.

#### 분산은 정보의 양이다
- 모든 값이 **상수** -> 이 특성으로 30대와 40대를 구분할 수 있을까? **정보의 차이가 없다.**
- 여러 통계치가 데이터의 특징을 나타내지만, **분산을 특히 정보의 양으로 이해할 수 있다.**
- 따라서 30대와 40대를 구분하고 싶다면 분산이 0에 가까운 X보다는 Y가 더 유용한 특성이 될 것이다.

##### 분산 vs 범위
- 분산 : 데이터가 평균값에서 퍼진 정도
- 범위 : 최댓값 ~ 최솟값
- **분산으로 범위를 추정하거나, 범위로 분산을 추정해선 안된다.** 명백히 구분되는 용도.

- 따라서 PCA에서 `저차원은 k차원에서의 정보를 충분히 담고 있어야 한다`라는 말은, **k차원에서의 분산이 최대한 반영돼야 한다**는 의미가 된다.
	- 차원을 줄이는 과정에서 **정보 손실은 필연적으로 발생**하기 때문이다
---

### PCA 세부 사항

> (1, 1), (1, 2), (2, 1) 3개의 점을 가정해보자
> 1. x축으로 투영하면 2개의 점이 1개의 점으로 보임
> 2. y축으로 투영해도 2개의 점이 1개의 점으로 보임
- 위 상황은 2차원에서 1차원으로의 `차원 축소` 과정이며, x축과 y축 이외의, 2차원을 더 잘 반영하는 1차원 축을 찾아야 한다.

 - **`투영 오차(Projection Error)`**
	- 기존 데이터가 저차원으로 투영되었을 때의 거리
	- **투영 오차가 작을수록 데이터 손실이 최소화**된다. 따라서 최단거리인 축에 직교 방향으로 투영됨.

- **투영된 데이터의 분산**을 통해 어느 축이 고차원의 정보가 더 잘 반영된 선인지도 알 수 있다.
	- 투영된 데이터의 **분산이 가장 큰 축**이 `1번째 주성분(Principal Component)`이 된다.
	- **1번째 주성분에 직교한 축**이 `2번째 주성분`이 된다.
		- 기존 2개의 축에 상관관계가 있었더라도 새로운 축을 기준으로 하면 상관관계가 없어짐

#### 몇 개의 주성분을 선택해야 하는가?
- 두 변수를 합쳤을 때의 분산부터 알아보자
$$
\begin{matrix}
var(X+Y) = var(X) + var(Y) + 2cov(X, Y) \\
cov(X, Y) = E\left[(X - \mu_x)(Y - \mu_y)\right]
\end{matrix}
$$

> $PC_1$의 분산을 10, $PC_2$의 분산을 2라고 해보자

- `전체 변동Total Variance` : 주성분의 분산을 모두 합친 값
- `설명력` : 전체 변동에서 각 주성분이 설명하는 변동의 비율

1. `전체 변동 = 12`
2. $PC_1$의 설명력 : $\frac{10}{12} = 83\%$
3. $PC_2$의 설명력 : $\frac{2}{12} = 17\%$

- 이런 식으로 n개의 주성분을 사용하고자 할 때 **누적 변동이 85% 이상이면 그만큼의 주성분만을 선택**한다.

#### PCA 사용 시 주의
1. 당연히 각 주성분에 대한 이해도가 떨어질 수밖에 없다. **독립 변수에 대한 설명이 필요한 경우 PCA가 적합하지 않을 수 있다.** 
	- 이 경우, 대신 **다중공선성이 있는 독립 변수 중 일부만을 사용**하는 방법이 있다.

2. **PCA 진행 전 표준화 작업은 필수다**

3. PCA에도 가정 조건이 있다.
	- 기존 데이터는 **다변량 정규분포의 형태이며, 다중공선성 문제가 있다**는 가정
	- 예를 들면 변수 간의 관계가 `원형, 나선형, 휘어져 있는 등의 비선형` 
		- `커널 주성분 분석 Kernel PCA` : 이 관계를 나타내는 비선형 함수(Kernel Function)를 통해 데이터를 비선형으로 투영
		- `t-SNE : t-분산 확률적 이웃 임베딩` : 데이터 간의 거리를 잘 보존하며 2차원으로 표현

- PCA는 데이터 간의 상관관계로만 축을 찾기 떄문에 비지도학습으로 분류된다. 
- **`LDA 선형 판별 분석`** : 클래스마다 데이터 타깃이 분리되는 축을 찾아 차원을 줄이는 방법