## 1. 인공지능, 머신러닝, 딥러닝

### 인공지능
- 보통의 사람이 수행하는 지능적인 작업을 자동화
- 1980년까지는 `학습`이 언급되지 않았다.
- `심볼릭 AI` : 명시적인 규칙을 DB에 많이 저장시키면 인간 수준의 인공지능이 될 것이다
- `전문가 시스템`

- `해석 기관` : 우리가 어떤 것을 작동시키기 위한 명령을 알고 있으면, 장치는 무엇이든 할 수 있다

- 전통 프로그래밍 : `규칙 + 데이터 -> 해답`

### 머신러닝
- `데이터 + 해답 -> 규칙`
- 프로그래밍되지 않고 `훈련`된다.
- 수리통계학과 다른 점
	- 복잡한 데이터셋을 다루므로, 베이즈 분석을 적용하기 힘들다.
	- 경험적 발견에 의해 주도되고, 하드웨어나 소프트웨어의 발전에 크게 의존한다.

- 머신러닝을 하기 위해 필요한 3가지
	- `입력` 데이터 포인트 : 목적에 맞는 파일 형식
	- 기대 `출력` : 어떤 것을 반환할 것인가
	- `알고리즘 성능 출력`: 기대출력과 현재출력 사이의 차이를 결정하며, 알고리즘을 교정하는 신호로 피드백된다.
- 입력 데이터의 유용한 `표현Representation`을 학습한다.
	- 빨간 픽셀을 선택하는 문제는 `RGB` 포맷이 쉽다.
	- 채도를 낮추는 문제는 `HSV` 포맷이 쉽다.
	- 어떤 **표현**으로 해결하기 힘든 문제가 다른 표현에서는 쉽게 해결될 수 있으며, 이러한 것을 **데이터 변환**이라고 한다.
- `학습`은 **데이터 표현을 만드는 데이터 변환을 피드백 신호를 바탕으로 자동으로 탐색하는 과정**을 의미한다.

### 딥러닝
- `Deep` : 층 기반의 학습.
	- `수십 ~ 수백 개`의 층
	- 반댓말은 `얕은 학습Shallow Learning`이라고 한다.
- 딥러닝에서 `학습`은 신경망의 모든 가중치 값을 찾는 것을 의미한다.

- `손실함수 = 목적함수 = 비용함수` : 기댓값보다 출력이 벗어난 정도
- `옵티마이저` : 손실함수에 의해 측정된 점수를 피드백을 `역전파 알고리즘`을 구현하여 가중치 값을 수정한다.
- `훈련 반복Training Loop` : 충분한 횟수만큼 훈련을 반복하면 손실함수를 최소화하는 가중치 값을 산출한다. (일반적으로 수천 개의 샘플, 수십 번의 반복)

#### 딥러닝 이용 분야
- 이미지 분류
- 음성 인식
- 필기 인식
- 기계 번역
- TTS
- 디지털 비서
- 자율주행 능력
- 광고 타겟팅
- 웹 검색 엔진
- 자연어 질문에 대답
- 바둑 등등

## 2. 딥러닝 이전 : 머신러닝의 역사

### 1. 확률적 모델링(Probabilistic Modeling)
- 통계학 이론을 데이터 분석에 응용한 것.
- 요즘도 자주 쓰인다.

#### 나이브 베이즈 알고리즘
- 입력 데이터의 피쳐가 모두 독립이라는 가정 하에, 베이즈 정리를 적용한다.
- `순진한Naive` 가정이라서 나이브 베이즈 알고리즘이라고 함
- 분류기임

#### 로지스틱 회귀
- 현대 머신 러닝의 `Hello World`
- 이름은 회귀인데 **분류**임
- 간단하고 다목적으로 쓸 수 있음
- **분류 작업**에 대한 감을 빠르게 얻기 위해 **데이터셋에 적용하는 1번째 알고리즘**으로 자주 쓰인다.

### 2. 초창기 신경망
- 완전히 대체되었음
- 1989년 처음으로 성공적인 신경망 애플리케이션이 나왔다.

### 3. 커널 방법
- 1990년대 신경망을 묻어버림.
- `커널 방법Kernel Method` 자체는 분류 알고리즘의 일부이다


#### 서포트 벡터 머신(SVM)
- 결정 경계를 찾는 분류 알고리즘으로, 두 단계에 걸쳐 경계를 찾는다.
1. 결정 경계가 하나의 `초평면Hyperplane`으로 표현될 수 있는 새로운 고차원 표현으로 데이터를 매핑한다.
	- 2차원 데이터라면 `초평면`은 직선(1차원)이 된다.
2. `마진 최대화` : 초평면과 각 클래스의 가장 가까운 데이터 포인트 사이의 거리가 최대가 되는 최선의 결정 경계를 찾는다. 

- `커널 기법Kernel Trick` 
	- 새로운 공간에서 초평면을 찾기 위해선, 데이터 공간의 좌표를 구할 필요가 없다.
	- `커널 함수Kernel Function`를 이용하면 `새로운 데이터 공간`에서의 두 데이터 포인트 사이의 거리를 계산할 수 있다.
	- `원본 공간`의 데이터 포인트를 명시적으로 새로운 표현으로 변환하지 않고, `타깃 공간에 위치했을 때의 거리`만을 매핑해줄 수 있는 연산이 된다.
	- `커널 함수`는 데이터로부터 학습되지 않고, 직접 만들어야 한다.

- SVM은 대용량의 데이터셋에 쓰이기 어렵고, 이미지 분류 같은 `지각Recognition` 문제에서 성능이 떨어진다. 
- `SVM`을 적용하려면 수동으로 유용한 표현을 추출해야 한다.(`특성 공학`) 이는 매우 어렵고 불안정하다.
- 따라서 문제를 쉽게 만드는 유용한 표현을 수동으로 찾아야 한다는 단점이 있다.

### 4. DT, RF, GBM

#### 결정 트리
- 데이터에서 학습됨
- 2000년대부터 관심을 크게 받고, 2010년까지는 커널 방법보다 선호되었음

#### 랜덤 포레스트
- 서로 다른 결정 트리를 많이 만들고, 이들의 앙상블을 이용한다.
- **`얕은 학습`을 적용하는 알고리즘 중에서 어떤 문제 상황이든 거의 2등을 차지하는 좋은 알고리즘**

#### 그래디언트 부스팅 머신
- **2014년에 랜덤 포레스트를 이어받음**
- 결정 트리를 앙상블하는데, **이전 모델에서 놓친 데이터포인트를 보완하는 새로운 모델을 반복적으로 훈련해서 머신러닝 모델을 향상**시키는 `그래디언트 부스팅Gradient Boosting`을 이용한다.
- `지각`에 관련되지 않은 알고리즘 중 가장 뛰어난 알고리즘 중 하나이다.(최고는 아니라는 언급이 있음)
- 딥러닝 빼고 캐글에서 제일 많이 쓰임

### 5. 신경망의 부활
- 이미지 분류 : `ImageNet` 대회(`ILSVRC`)에서 2012년 83.6%(전년 대비 10% 상승), 2015년 96.4%
	- **2012년부터 DNN은 모든 컴퓨터 비전 작업의 주요 알고리즘**이 되었다.
	- 더 일반적으로는 **지각에 관한 모든 문제에 DNN을 적용**할 수 있다.
	- 2015년 이후, `ConvNet`을 적용하지 않은 발표는 거의 없어졌음
- 자연어 처리 등에도 적용됨
- 다양한 앱에서 SVM과 결정 트리를 완전히 대체하고 있다.

### 6. 딥러닝의 특징
- 많은 문제에서 **기존 머신러닝보다 더 좋은 성능**
- **특성 공학을 완전히 자동화**함
	- `특성 공학` : 데이터에 좋은 표현을 수동으로 만드는 일

- 딥러닝의 중요한 특징 2가지
	1. **층을 거치면서 점진적으로 더 복잡한 표현**이 만들어진다.
	2. **점진적인 중간 표현이 공동으로 학습**된다.
		- 각 층은 상위 층, 하위 층이 바뀌면 동시에 바뀐다

### 7. 머신러닝의 최근 동향
- 가장 좋은 방법은 `캐글`의 머신 러닝 경연을 살펴보는 것이다.
- 2019년 초 캐글의 설문조사 : 상위 5개 팀에 대해 어떤 도구를 많이 쓰나요?
	1. 케라스 (40%)
	2. LightGBM (29%)
	3. XGBoost (18%)
	4. 파이토치 (15%)
	5. 텐서플로우
	6. sklearn
	7. Fastai
	8. Caffe
- 설문조사 2 : ML 전문가와 데이터 과학자 중심의 조사
	1. sklearn 82.3%
	2. 텐서플로우 52.6%
	3. Xgboost 47.9%
	4. 케라스 47.3%
	5. 파이토치 33.7%
	...

> 딥러닝과 GBM을 많이 쓰는데,  
> GBM은 구조적인 데이터에서 많이 쓰이고  
> 딥러닝은 지각 문제에서 많이 쓰인다.

- 상위 딥러닝 라이브러리의 대부분은 파이썬으로 이용할 수 있다.
- 따라서, 머신러닝을 위해 알아야 하는 2가지 기술은 
	- **얕은 러닝을 위한 GBM**
	- **지각 문제를 위한 딥러닝** 
이 되겠다.

## 3. 왜 딥러닝인가?
- CNN, Back Propagation은 1989년, LSTM은 1997년에 소개되었다. 왜 지금 딥러닝이 떴는가?
	- 하드웨어 성능
	- 데이터셋, 벤치마크
	- 알고리즘 향상

### 1. 하드웨어
- 1990년 -> 2010년이 되는 동안 5000배 좋아짐. 노트북에서도 작은 딥러닝 모델이 실행 가능함.
- GPU 성능의 비약적인 향상 
	- 게임의 그래픽 성능을 위한 것이었지만, 병렬 연산 처리에 유리하기 때문에 딥러닝에도 사용 많음
	- 2007년 `NVIDIA의 CUDA` -> 대형 GPU 클러스터가 소량의 GPU로 대체되었음
	- 2019년 NVIDIA TITAN RTX : 초당 16조개의 float32 연산이 가능함
		- 2012년 `ImageNet` 모델을 몇 시간 만에 훈련시킬 수 있음.
- TPU : `GPU`보다 훨씬 빠르고 효율적인 에너지 소비 가능
	- `pod`이라는 대규모 환경 구성에 맞게 설계되었다

### 2.  데이터
- 인터넷에는 이미지, 비디오, 자연어 데이터셋이 무궁무진하다.
	- `Flickr` : 사용자가 붙인 이미지 태그
	- 유튜브 비디오
	- 위키피디아 - 자연어 처리에 핵심적인 데이터셋
- `ImageNet`은 데이터셋 이름이며, `ILSVRC`라는 대회가 열린 거임
- `캐글` 또한 대회에서 우승한다는 것 자체가 크게 부각되었기 때문에 경쟁과 성장을 강화시켰음

### 3. 알고리즘
- 2000년대 후반까지는 심층 신경망 훈련에 안정적인 방법을 찾지 못했다.
- `그래디언트 역전파`가 층이 깊어질수록 그 영향력이 약해졌기 때문인데,  
- 2009 ~ 2010년 경 이런 기술들이 등장했다.
	- 활성화 함수 : **ReLU**
	- 가중치 초기화 방법 : **Xavier 초기화 = Glorot 초기화**
		- 층별 사전 훈련이 불필요해짐
	- 최적화 방법 : **RMSProp, Adam**
- 2014년 ~ 2016년 
	- 배치 정규화(Batch Normalization)
	- 잔차 연결(Residual Connection)
	- 깊이별 분리 합성곱(Depthwise separable convolution)

- 매우 큰 모델의 사용이 가능해졌으며, 이런 모델은 매우 풍부한 가설 공간을 지닌다.
- 수십 개의 층과 수천만 개의 파라미터를 가진 모델 구조가 아래 분야에 중요한 발전을 가져옴
	- 컴퓨터 비전 : `ResNet, Inception, Xception`
	- 자연어 처리 : `BERT, GPT-3, XLNet`

- 딥러닝의 대중화
	- 초기 딥러닝은 C++와 CUDA의 전문가여야만 이용할 수 있었음
	- 요즘에는 파이썬만 대충 할 줄 알면 고수준의 딥러닝을 연구할 수 있음
	- 씨아노(개발 중지), 텐서플로우 등은 자동 미분을 지원함
	- 2015년 `케라스`는 레고를 만드는 것처럼 딥러닝 모델을 만들 수 있게 해줌

- 현재 딥러닝의 특징
	- **단순함**
		- 특성공학 필요 없음
	- **확장성**
		- 병렬화가 쉬움
		- 작은 Batch 데이터에서 반복 훈련, 어떤 크기의 데이터셋에서도 훈련 가능
		- 유일한 병목은 하드웨어
	- **다용도, 재사용성**
		- 처음부터 다시 시작할 필요 없이 추가되는 데이터로 훈련 가능함 
		- 온라인 학습이 가능함
		- 훈련된 모델은 다른 용도로 사용 가능
			- ex) 이미지 분류 모델은 비디오 처리 작업에도 쓰일 수 있음
			- 아주 작은 데이터셋에도 딥러닝 모델 적용 가능
- 