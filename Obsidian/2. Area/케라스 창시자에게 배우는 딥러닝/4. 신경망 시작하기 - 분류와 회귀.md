
[[#1. 이진 분류 : 영화 리뷰 분류하기|1. 이진 분류 : 영화 리뷰 분류하기]]
	 [[#1. 이진 분류 : 영화 리뷰 분류하기#1. IMDB 데이터셋|1. IMDB 데이터셋]]
	 [[#1. 이진 분류 : 영화 리뷰 분류하기#2. 데이터 준비|2. 데이터 준비]]
	 [[#1. 이진 분류 : 영화 리뷰 분류하기#3. 신경망 모델 만들기|3. 신경망 모델 만들기]]
	 [[#1. 이진 분류 : 영화 리뷰 분류하기#4. 검증 데이터 및 훈련|4. 검증 데이터 및 훈련]]
[[#2.|2.]]
[[#3.|3.]]


## 1. 이진 분류 : 영화 리뷰 분류하기

### 1. IMDB 데이터셋
- 양극단의 리뷰 5만개로 이뤄짐
- 훈련 25000개, 테스트 25000개, 긍정 부정 절반씩

```python
from tensorflow.keras.datasets import imdb

(train_data, train_labels), (test_data, test_labels) = imdb.load_data(num_words = 10000) # 가장 자주 나타나는 단어 1만개만 포함

print(train_data[0]) # 각 리뷰는 단어 인덱스의 리스트
print(train_labels[0]) # 부정 : 0, 긍정 : 1
```

### 2. 데이터 준비
- 신경망에 숫자 리스트를 바로 주입할 수 없다 : 숫자 리스트의 길이가 모두 다르기 때문( = 문장의 길이가 모두 다르기 때문)
- 신경망은 항상 동일한 크기의 배치를 인풋으로 받아야 한다.

- 리스트 -> 텐서 바꾸기
	1. 같은 길이가 되도록 패딩 추가, `(sample, max_length)` 크기의 정수 텐서로 변환
	2. `멀티 - 핫 인코딩`해서 리스트를 0과 1의 벡터로 변환한다.

- 여기선 2번째 방식을 사용함
```python
import numpy as np

def vectorize_sequences(sequences, dimension = 10000):
  results = np.zeros((len(sequences), dimension)) # (리뷰 갯수, 단어 갯수)의 크기를 가지면서 모든 값 0
  for i, sequence in enumerate(sequences):
    for j in sequence:
      results[i, j] = 1. # 특정 인덱스의 위치만 1로 만듦

x_train = vectorize_sequences(train_data)
x_test = vectorize_sequences(test_data)

y_train = np.asarray(train_labels).astype('float32')
y_test = np.asarray(test_labels).astype('float32')
```
- `멀티 - 핫 인코딩`이 뭐냐?
	- 기존 리스트는 숫자들의 연속임
	- 각 리뷰에 대해, 해당 숫자를 가지면 1, 없으면 0을 갖는 배열을 만드는 거임
	- `len(sequences)`는 리뷰 갯수
	- `dimension`은 최대 단어 개수(최초에 10000으로 설정)

### 3. 신경망 모델 만들기
- `Dense`층을 쌓을 때 중요한 구조상의 결정
	1. 얼마나 많은 층을 쓸 것인가
	2. 각 층에 얼마나 많은 유닛을 쓸 것인가

> **층 결정 원리는 5장에서 배운다.**   
> 여기서는 16개 유닛, 2개의 중간층 & 2개로 분류하는 마지막층(유닛 1개)으로 구성

```python
from tensorflow import keras
from tensorflow.keras import layers

model = keras.Sequential([
    layers.Dense(16, activation = 'relu'),
    layers.Dense(16, activation = 'relu'),
    layers.Dense(1, activaton = 'sigmoid')
])

model.compile(optimizer = 'rmsprop', loss = 'binary_crossentropy', metrics=['accuracy'])

```
1. 16개의 유닛이 있다 = 가중치 행렬의 크기가 `input_dimension, 16`이라는 뜻.
	- 즉 입력 데이터와 W를 내적하면 입력 데이터가 16차원으로 투영됨.
2. 마지막 층의 `sigmoid` : 0과 1 사이의 점수로, 출력 값을 확률처럼 해석할 수 있다.
3. `rmsprop` : 거의 모든 문제에서 기본 선택으로 좋음
4. 손실 함수 : 이진 분류라 `binary_crossentropy`를 이용했으며 `mean_sqaured_error`도 회귀에 많이 쓰지만 여기서도 사용은 가능하다. 그러나 **확률 문제는 `crossentropy`가 좋음.**

### 4. 검증 데이터 및 훈련
```python
x_val = x_train[:10000]
partial_x_train = x_train[10000:]
y_val = y_train[:10000]
partial_y_train = y_train[10000:]

# 훈련
history = model.fit(partial_x_train, partial_y_train, epochs = 20, batch_size = 512, validation_data = (x_val, y_val))
```

- `history` 객체 확인
```python
# history 속성 확인
history_dict = history.history
history_dict.keys()
```

- 훈련과 검증 손실 그리기
```python
import matplotlib.pyplot as plt
history_dict = history.history
loss_values = history_dict['loss']
val_loss_values = history_dict['val_loss']
epochs = range(1, len(loss_values) + 1)
plt.plot(epochs, loss_values, "bo", label = "Training Loss")
plt.plot(epochs, val_loss_values, "b", label = "Validation Loss")
plt.legend()
plt.show()


# 훈련과 검증 정확도 그리기

acc = history_dict['history']
val_acc = history_dict['val_accuracy']
plt.plot(epochs, acc, "ro", label = "Training accuracy")
plt.plot(epochs, val_acc, "r", label = "Validation accuracy")
plt.legend()
plt.show()
```
![[Pasted image 20230720234103.png]]
- 검증 손실, 검증 정확도가 4번째 이후부턴 오히려 감소하는 현상을 보이는데, 이는 과대적합되었다고 할 수 있다.
- 과대적합을 막는 여러 기술이 있는데 이건 5장에서 다룸.
- 에포크를 4회로 줄여서 다시 실행하고 `model.predict(x_test)`를 돌려본다.


## 2. 
## 3.
## 4.

