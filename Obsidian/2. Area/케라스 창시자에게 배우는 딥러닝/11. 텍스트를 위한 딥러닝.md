
1. [[#자연어 처리 소개|자연어 처리 소개]]
2. [[#텍스트 데이터 준비|텍스트 데이터 준비]]
	1. [[#텍스트 데이터 준비#텍스트 표준화|텍스트 표준화]]
	2. [[#텍스트 데이터 준비#텍스트 분할(토큰화)|텍스트 분할(토큰화)]]
	3. [[#텍스트 데이터 준비#어휘 사전 인덱싱|어휘 사전 인덱싱]]
	4. [[#텍스트 데이터 준비#TextVectorization 층 구현하기|TextVectorization 층 구현하기]]
3. [[#단어 그룹 표현 방법 : 집합과 시퀀스|단어 그룹 표현 방법 : 집합과 시퀀스]]
	1. [[#단어 그룹 표현 방법 : 집합과 시퀀스#IMDB 영화 리뷰 데이터 준비|IMDB 영화 리뷰 데이터 준비]]
	2. [[#단어 그룹 표현 방법 : 집합과 시퀀스#BoW 방식|BoW 방식]]
		1. [[#BoW 방식#이진 인코딩을 사용한 유니그램|이진 인코딩을 사용한 유니그램]]
		2. [[#BoW 방식#이진 인코딩을 사용한 바이그램|이진 인코딩을 사용한 바이그램]]
		3. [[#BoW 방식#TF-IDF 를 사용한 바이그램|TF-IDF 를 사용한 바이그램]]
	3. [[#단어 그룹 표현 방법 : 집합과 시퀀스#시퀀스 모델 방식|시퀀스 모델 방식]]
		1. [[#시퀀스 모델 방식#1번째 예제|1번째 예제]]
		2. [[#시퀀스 모델 방식#단어 임베딩|단어 임베딩]]
		3. [[#시퀀스 모델 방식#Embedding 층으로 단어 임베딩 학습하기|Embedding 층으로 단어 임베딩 학습하기]]
		4. [[#시퀀스 모델 방식#패딩과 마스킹 이해하기|패딩과 마스킹 이해하기]]
		5. [[#시퀀스 모델 방식#사전 훈련된 단어 임베딩 적용하기|사전 훈련된 단어 임베딩 적용하기]]
			1. [[#사전 훈련된 단어 임베딩 적용하기#`GloVe` 임베딩 적용하기|`GloVe` 임베딩 적용하기]]
4. [[#트랜스포머 아키텍처|트랜스포머 아키텍처]]
	1. [[#트랜스포머 아키텍처#셀프 어텐션 이해하기|셀프 어텐션 이해하기]]
		1. [[#셀프 어텐션 이해하기#일반화된 셀프 어텐션 : 쿼리 - 키 - 값 모델|일반화된 셀프 어텐션 : 쿼리 - 키 - 값 모델]]
		2. [[#셀프 어텐션 이해하기#멀티 헤드 어텐션|멀티 헤드 어텐션]]
	2. [[#트랜스포머 아키텍처#트랜스포머 인코더|트랜스포머 인코더]]
		1. [[#트랜스포머 인코더#위치 정보 주입하기 : 위치 인코딩|위치 정보 주입하기 : 위치 인코딩]]
	3. [[#트랜스포머 아키텍처#언제 시퀀스를 쓰고, 언제 BoW를 쓰나요?|언제 시퀀스를 쓰고, 언제 BoW를 쓰나요?]]
		1. [[#언제 시퀀스를 쓰고, 언제 BoW를 쓰나요?#텍스트 분류라는 과제에 한해서,|텍스트 분류라는 과제에 한해서,]]
5. [[#텍스트 분류를 넘어 : 시퀀스 투 시퀀스 학습|텍스트 분류를 넘어 : 시퀀스 투 시퀀스 학습]]
	1. [[#텍스트 분류를 넘어 : 시퀀스 투 시퀀스 학습#기계 번역 예제|기계 번역 예제]]
	2. [[#텍스트 분류를 넘어 : 시퀀스 투 시퀀스 학습#RNN 시퀀스투시퀀스 모델 만들기|RNN 시퀀스투시퀀스 모델 만들기]]
	3. [[#텍스트 분류를 넘어 : 시퀀스 투 시퀀스 학습#트랜스포머 시퀀스 투 시퀀스 모델|트랜스포머 시퀀스 투 시퀀스 모델]]
		1. [[#트랜스포머 시퀀스 투 시퀀스 모델#트랜스포머 디코더|트랜스포머 디코더]]



## 자연어 처리 소개

- `컴퓨터 과학`에서
	- `어셈블리Assembly` : LISP, XML 등 기계를 위해 고안된 언어
	- `자연어Natural Language` : 한국어, 영어 등

- 어셈블리는 규칙 생성 -> 사용 이지만, 자연어는 사용 -> 규칙 생성이다.
	- 기계어는 구조적이고 엄격하며 정확한 문법 규칙을 사용한다.
	- 자연어는 복잡하고, 모호하고, 불규칙하고, 끊임없이 변화한다.

- 초기 자연어 이해 능력은, `규칙 집합`을 이용하려 했다. 
	- 1960년대 ELIZA : 패턴 매칭
- 그러나 언어는 규칙에 맞지 않으며 쉽게 체계화할 수 없었다.
- 수동으로 규칙을 만드는 건 90년대까지 지배적이었다.
- `머신 러닝`을 자연어에 적용하는 건 80년대 후반 결정 트리로 시작되었다.
- 이후 로지스틱 회귀로 이어지기도 했다.

- **현대의 NLP 는 입력으로 언어를 받아 어떤 유용한 것을 반환**하는 것이다.
	- 텍스트 분류 : 주제
	- 콘텐츠 필터링 : 부적절한 내용 거르기
	- 감성 분석 : 긍정 vs 부정
	- 언어 모델링 : 문장을 완성하기 위한 다음 단어
	- 번역
	- 요약
	- 등

- 텍스트 처리 모델은 언어를 사람처럼 이해하는 것이 아니라, 통계적인 규칙성을 찾는다. 이는 여러 간단한 작업을 잘 수행하는 데에 충분하다.
- 컴퓨터 비전이 픽셀에 적용하는 패턴 인식이라면, NLP는 단어, 문장, 문단에 적용되는 패턴 인식이다.

- 2014~5년 쯤, LSTM의 언어 이해 능력이 두드러지기 시작했음
- 2017~8년 쯤, `트랜스포머Transformer`가 RNN을 대체했다.
- **오늘날 대부분의 NLP 시스템은 트랜스포머가 기반이다.**

## 텍스트 데이터 준비
- 딥러닝 모델은 수치 텐서만 처리할 수 있다.
- `텍스트 벡터화Vectorization`는 텍스트를 수치 텐서로 바꾸는 과정이다.
> 1. `표준화Standardization` : 소문자로 바꾸거나, 구두점을 제거하거나.
> 2. `토큰화Tokenization` : 텍스트를 토큰으로 분할한다. 문자, 단어, 단어 그룹 등이 될 수 있다.
> 3. `인덱싱Indexing` : 각 토큰을 수치 벡터로 바꾼다. 일반적으로 모든 토큰을 인덱싱한다.

### 텍스트 표준화
```
"Hello World"
"hello world"
```
- 두 문장의 의미가 같으나, **바이트 문자열로 바꾸면 매우 다른 표현**이 된다.
- 텍스트 표준화는 모델이 인코딩 차이를 고려하지 않게끔 제거하기 위한 기초적인 특성 공학이다.
	- 머신러닝 외에도 검색 엔진에 동일한 작업이 수행된다.
- 가장 널리 쓰이는 방법은 **`소문자로 바꾼다 + 구두점 문자를 삭제한다`** 이다. 
	- 이외에도 특수 문자를 표준 형태로 바꿀 수 있다. e위에 점 찍힌 거를 e로 바꾼다든가 하는 식.

- 이외에도 `어간 추출Stemming`이라는 고급 표준화 패턴이 있다.
	- 어형이 변형된 단어를 공통된 하나의 표현으로 바꾼다.
	- `caught, been catching` -> `catch` / `cats` -> `cat`

- 텍스트 표준화는 모델 훈련 데이터를 줄일 수 있고, 더 나은 일반화 성능을 낳을 수 있다.
- 일정량의 정보를 삭제할 수도 있음에 유의하자. 
	- 질문 추출 모델에서 `?`은 작업에서 유용한 신호이므로 삭제하지 않고 토큰으로 다뤄야 한다.


### 텍스트 분할(토큰화)
- 텍스트 표준화 후, 벡터화할 단위(토큰)으로 나눈다.

- 3가지 방법이 있음.
1. **단어 수준 토큰화** : 토큰이 공백이나 구두점으로 구분된 부분 문자열이다.
	- 비슷) 단어를 `부분 단어Subword`로 나눌 수 있다 : `called -> call + ed`
2. **N-그램 토큰화** : N개의 연속된 단어 그룹을 토큰화한다. 
	- `the cat`, `he was` 등은 2-그램 or 바이그램 토큰화
3. 문자 수준 토큰화 : 각 문자가 1개의 토큰화이지만, 잘 안씀

- 텍스트 처리 모델에는 2가지가 있다.
1. `시퀀스 모델Sequence Model` : 단어의 순서를 고려함
2. `BoW 모델Bag-of-Words Model` : 순서를 무시하고 집합으로 다룸

- **시퀀스 모델은 단어 수준 토큰화**를 쓰고, **BoW 모델은 N-그램 토큰화**를 쓴다.
- N-그램은 인공적으로 모델에 국부적인 단어 순서에 대한 소량의 정보를 주입한다.

> "the cat sat on the mat" 
> 2그램 : `[the, the cat, cat, cat sat, sat, sat on, on, on the, the mat, mat]`
> 3그램 : `[the, the cat, cat, cat sat, the cat sat, sat, sat on, on, cat sat on, on the, sat on the, the mat, mat, on the mat]`
> 위 리스트 각각을 `N-그램 가방bag of N-gram`이라고 한다. `bag`은 시퀀스가 아니라 집합임을 의미한다.
> BoW는 순서가 없기 떄문에 얕은 학습 방법의 언어 처리 모델에 사용되는 경향이 있다.  
>  N-그램은 일정의 특성 공학으로, 딥러닝 시퀀스 모델은 이런 방식을 계층적인 특성 학습으로 대체한다.   
>  1D 컨브넷, RNN, 트랜스포머로 단어와 문자 그룹에 대한 특성을 학습할 수 있다. 그룹을 명시적으로 알려주지 않아도 된다!


### 어휘 사전 인덱싱
- 각 토큰을 수치 표현으로 인덱싱해야 한다.
- 토큰을 해싱할 수도 있지만 ,실전에서는 `훈련 데이터의 모든 토큰의 인덱스(어휘 사전)`을 만들어 각 항목에 고유한 정수를 할당하는 방식을 사용한다.
```python
# 각 토큰에 인덱스 할당 : 사전 생성
voca = {}
for text in dataset:
	tokens = tokenize(text)
	for token in tokens:
		if token not in voca:
		voca[token] = len(voca)

# 각 토큰의 인덱스를 원핫인코딩
def one_hot_eocnode_token(token):
	vector = np.zeros((len(voca), ))
	token_index = voca[token]
	vector[token_index] = 1
	return vector
```

훈련 데이터에서 **가장 많이 등장하는 2~3만 개 단어로만 어휘사전을 제한**하는 것이 일반적이다. 텍스트 데이터는 고유한 토큰이 매우 많기 때문에, 1~2번 등장하는 토큰을 인덱싱하는 건 특성 공간만 키우고 실제로는 별 쓸모가 없을 확률이 높다.  
앞의 예제에서 `num_words = 10000`으로 설정했던 걸 생각하면 됨.  
그런데 실제 데이터에서 훈련 데이터에 없는 토큰이 등장하면 어떻게 될까? 위 코드는 `dict`로 구성했기 때문에 `KeyError`가 발생할 것이다. 따라서 `예외 어휘Out Of Vocabulary` 인덱스를 사용하며, 일반적으로 `인덱스 1`을 사용한다.
- 일반적으로 사용하는 토큰이 2개 있다. 
	- `마스킹Masking 토큰` : 인덱스 0, 무시할 수 있는 토큰. **시퀀스 데이터를 패딩**할 때 사용한다.
		- 텍스트 길이는 다를 수 있는데 배치의 모든 시퀀스의 길이가 같아야 하니까, 거기에 넣는 인덱스라는 뜻임
	- `OOV 토큰` : 인덱스 1, **토큰이 인식할 수 없는 단어**

### TextVectorization 층 구현하기
- 위의 표준화 + 토큰화 + 인덱싱은 파이썬으로 쉽게 구현할 수 있다.
```python
# 토큰화 수동 구현
import string

class Vectorizer:
  def standardize(self, text):
    text = text.lower()
    return "".join(char for char in text if char not in string.punctuation)

  def tokenize(self, text):
    return text.split()

  def make_vocabulary(self, dataset):
    self.vocabulary = {"": 0, "[UNK]" : 1} # 패딩 토큰 0, 사전에 없는 토큰 1
    for text in dataset:
      text = self.standardize(text)
      tokens = self.tokenize(text)
      for token in tokens:
        if token not in self.vocabulary:
          self.vocabulary[token] = len(self.vocabulary)
    self.inverse_vocabulary = dict((v, k) for k, v in self.vocabulary.items())

  def encode(self, text):
    text = self.standardize(text)
    tokens = self.tokenize(text)
    return [self.vocabulary.get(token, 1) for token in tokens]

  def decode(self, int_sequence):
    return " ".join(self.inverse_vocalbulary.get(i, "[UNK]") for i in int_sequence)

vectorizer = Vectorizer()
dataset = [
    "I write, erase, rewrite",
    "Erase again, and then",
    "A poppy blooms.",
]
vectorizer.make_vocabulary(dataset)
```
```python
# 사용
test_sentence = "I write, rewrite, and still rewrite again"
encoded_sentence = vectorizer.encode(test_sentence)
print(encoded_sentence) # [2, 3, 5, 7, 1, 5, 6]
decoded_sentence = vectorizer.decode(encoded_sentence)
print(decoded_sentence) # i write rewrite and [UNK] rewrite again
```

- 케라스에는 훨씬 빠르고 효율적인 `TextVectorization` 층이 있다.
```python
from tensorflow.keras.layers import TextVectorization

# 정수 인덱스로 인코딩된 단어 시퀀스를 반환하며, 다른 모드도 있음
text_vectorization = TextVectorization(output_mode = "int", )
```
  
기본적으로 `TextVectorization` 층은 "소문자 & 구두점 제거 & 토큰화를 위해 공백으로 나눔"의 기능을 수행한다.   
그러나 사용자 정의 함수를 받아서 처리할 수도 있다.    
이렇게 들어가는 **사용자 정의 함수는 일반적인 파이썬 문자열을 인풋으로 받지 않고, `tf.string` 텐서를 인풋으로 받아야 한다.**

```python
import re
import string
import tensorflow as tf

def custom_standardization(string_tensor):
  lowercase_string = tf.strings.lower(string_tensor) # 문자열 소문자로 변경
  return tf.string.regex_replace(
      lowercase_string, f"[{re.escape(string.punctuation)}]", ""
  )

def custom_split(string_tensor):
  return tf.string.split(string_tensor)

text_vectorization = TextVectorization(
    output_mode = 'int',
    standardize = custom_standardization,
    split = custom_split
)

# 텍스트 말뭉치Corpus의 어휘사전 인덱싱 : Dataset 객체로 adapt() 메서드를 호출하면 된다.
dataset = [
    "I write, erase, rewrite",
    "Erase again, and then",
    "A poppy blooms.",
]
text_vectorization.adapt(dataset)

# 계산된 어휘사전 추출하기
text_vectorization.get_vocabulary()
# ['', '[UNK]', 'erase', 'write', 'then', 'rewrite', 'poppy', 'i', 'blooms', 'and', 'again', 'a']
```

- 예시 문장 인코딩과 디코딩
```python
# 예시 문장 인코딩과 디코딩
vocabulary = text_vectorization.get_vocabulary()
test_sentence = "I write, rewrite, and still rewrite again"
encoded_sentence = text_vectorization(test_sentence)
print(encoded_sentence)
# tf.Tensor([ 7 3 5 9 1 5 10], shape=(7,), dtype=int64)

inverse_vocab = dict(enumerate(vocabulary))
decoded_sentence = " ".join(inverse_vocab[int(i)] for i in encoded_sentence)
print(decoded_sentence)
# i write rewrite and [UNK] rewrite again
```

> `TextVectorization` 층은 딕셔너리 룩업(lookup) 연산으로, CPU에서만 실행된다.
> `TextVectorization` 층의 사용법은 크게 2가지이다.


1. `tf.data` 파이프라인에 넣기(GPU, TPU 사용 시 개추)
```python
int_sequence_dataset = string_dataset.map(
										  text_vectorization,
										  num_parallel_calls = 4
)
```
- `num_parallel_calls` : 여러 CPU 코어에서 `map` 메서드를 병렬화함

2. 모델의 일부로 넣기 - 결국 층이니까
```python
text_input = keras.Input(shape = (), dtype = 'string') # 문자열 기대 입력
vectorized_text = text_vectorization(text_input) # 텍스트 벡터화 층 적용
embedded_input = keras.layers.Embedding(...)(vectorized_text)
output = ...
model = keras.Model(text_input, output)
```

- 두 방법에는 차이점이 있다.
- 모델에 넣는 경우, 나머지 부분과 동기적으로 수행된다.
	- GPU에 있는 모델의 나머지 부분이 수행되기 위해, CPU층에 놓인 `TextVectorization` 층의 출력을 기다리게 된다.
- `tf.data` 파이프라인에 넣는 경우, CPU와 GPU 각각이 따로 처리될 수 있다.(비동기적 처리)
	- CPU는 데이터를 가공해서 GPU에 계속 전달하고, GPU는 GPU에 있는 모델 처리만 하면 됨.

- 따라서  **GPU나 TPU에서 훈련 시 `tf.data` 파이프라인에 넣는 방법이 더 좋은 성능**을 낸다.
- 단, 제품 환경에 배포 시 2번째 처럼 원시 문자열을 입력으로 받는 모델을 준비해야 한다. 그렇지 않으면 텍스트 표준화와 토큰화를 다시 구현해야 한다.
	- 전처리에 차이가 생기면 모델의 정확도를 손상시킬 위험이 있는데, `TextVectorization` 층은 모델에 텍스트 전처리를 포함시켜서 쉽게 배포할 수 있다.
	- 근데 이건 `tf.data` 파이프라인으로 해도 가능함.

## 단어 그룹 표현 방법 : 집합과 시퀀스
머신 러닝 모델이 개별 단어를 처리하는 방법은 별 논쟁이 없다. `단어는 범주형 특성(미리 정의된 집합에 있는 값)`이다. 단어 자체의 처리보다는 **단어 순서를 인코딩하는 방법이 더 중요**하다.  
시계열과 달리 문장의 단어는 자연스럽고 표준이 되는 순서가 없다. 언어가 다르면 비슷한 단어도 매우 다른 방식으로 나열한다. 순서는 중요할 수 있지만, 의미와의 관계가 얼마나 깊은가?는 애매한 점이 있음.  
**단어의 순서 표현은 여러 종류의 NLP 아키텍처를 낳는 핵심 질문**이다. 
>1. `BoW 모델` : 순서를 무시하고 텍스트를 집합으로 처리함.
>2. `시퀀스 모델` : 단어 순서를 고려함
	- 시계열의 타임 스텝처럼 등장하는 단어 하나하나를 처리
	- RNN
	- 하이브리드
	- 트랜스포머는 순서에 구애받지 않으나, 처리하는 표현에 위치 정보를 주입한다. 그래서 순서를 고려하면서 문장의 여러 부분을 동시에 볼 수 있다.(RNN과 다른 점!)

- 오늘날에도 두가지 방법 모두 사용한다.

### IMDB 영화 리뷰 데이터 준비
```sh
!curl -O https://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz
!tar -xf aclImdb_v1.tar.gz
!rm -r aclImdb/train/unsup # 필요없는 폴더 삭제

# 데이터 확인
!cat aclImdb/train/pos/4077_10.txt
```
- aclImdb 폴더는 train, test로 나뉘며, 각각 pos, neg로 또 나뉜다.
	- 각각 12500개씩 담겨 있음(4개 폴더에)

```python
# 훈련 텍스트 파일에서 20%를 aclImdb/val로 덜어 검증 세트 생성
import os, pathlib, shutil, random

base_dir = pathlib.Path("aclImdb")
val_dir = base_dir / "val"
train_dir = base_dir / "train"
for category in ("neg", "pos"):
  os.makedirs(val_dir / category) 
  files = os.listdir(train_dir / category)
  random.Random(1337).shuffle(files)

  # 20% 검증 세트
  num_val_samples = int(0.2 * len(files))
  val_files = files[-num_val_samples:]

  # 검증 데이터 옮기기
  for fname in val_files:
    shutil.move(train_dir / category / fname,
                val_dir / category / fname)
```

```python
# 디렉터리 -> 데이터셋 만들기
from tensorflow import keras 
batch_size = 32

train_ds = keras.utils.text_dataset_from_directory(
    "aclImdb/train", batch_size = batch_size
)
val_ds = keras.utils.text_dataset_from_directory(
    "aclImdb/val", batch_size = batch_size
)
test_ds = keras.utils.text_dataset_from_directory(
    "aclImdb/test", batch_size = batch_size
)
```

- 데이터셋 확인
```python
for inputs, targets in train_ds:
  print("inputs.shape : ", inputs.shape) # inputs.shape : (32,)
  print("inputs.dtype : ", inputs.dtype) # inputs.dtype : <dtype: 'string'>
  print("targets.shape : ", targets.shape) # targets.shape : (32,)
  print("targets.dtype : ", targets.dtype) # targets.dtype : <dtype: 'int32'>
  print("inputs[0] : ", inputs[0]) # 너무 길어 생략
  print("targets[0] : ", targets[0]) # targets[0] : tf.Tensor(0, shape=(), dtype=int32)
  break
```

### BoW 방식
텍스트 처리 인코딩에서 가장 간단한 방법은 순서를 무시하고 토콘의 집합으로 다루는 것이다. `개별 단어(유니그램)`를 사용하거나, `연속된 토큰 그룹(N-그램)`으로 순서 정보를 유지할 수 있다.


#### 이진 인코딩을 사용한 유니그램
- 개별 단어의 집합을 사용하면, 전체 텍스트를 **하나의 벡터**로 표현할 수 있다.
	- 벡터의 각 원소는 **해당 토큰이 사전에 있는지 여부**를 표시한다.
```python
text_vectorization = TextVectorization(
    max_tokens = 20000, # 2만개 단어만 사용
    output_mode = "multi_hot" # 멀티-핫 이진 벡터로 출력 토큰 인코딩
)

text_only_train_ds = train_ds.map(lambda x, y : x) # 레이블 없는 데이터셋
text_vectorization.adapt(text_only_train_ds) # 데이터셋의 어휘사전 인덱싱

# 데이터셋 전처리 : 다중 CPU 코어 활용
binary_1gram_train_ds = train_ds.map(
    lambda x, y : (text_vectorization(x), y),
    num_parallel_calls = 4
)
binary_1gram_val_ds = val_ds.map(
    lambda x, y : (text_vectorization(x), y),
    num_parallel_calls = 4
)
binary_1gram_test_ds = test_ds.map(
    lambda x, y : (text_vectorization(x), y),
    num_parallel_calls = 4
)

# 데이터셋 출력 확인
for inputs, targets in binary_1gram_train_ds:
  print("inputs.shape : ", inputs.shape)
  print("inputs.dtype : ", inputs.dtype)
  print("targets.shape : ", targets.shape)
  print("targets.dtype : ", targets.dtype)
  print("inputs[0] : ", inputs[0]) # [1. 1. 1. ... 0. 0. 0.]
  print("targets[0] : ", targets[0])
  break
```
- 입력 텐서에서 보이듯, 단어가 사전에 있는지 유무만 표시한다.
- 텍스트 분류에서 **2만 개의 어휘 사전 크기는 일반적으로 적당한 수준**임.

- 모델
```python
# 모델 생성 함수 : 모든 예제에서 쓸 거임
from tensorflow import keras
from tensorflow.keras import layers

def get_model(max_tokens = 20000, hidden_dim = 16):
  inputs = keras.Input(shape = (max_tokens, ))
  x = layers.Dense(hidden_dim, activation = 'relu')(inputs)
  x = layers.Dropout(0.5)(x)
  outputs = layers.Dense(1, activation = 'sigmoid')(x)
  model = keras.Model(inputs, outputs)
  model.compile(optimizer = 'rmsprop',
                loss = 'binary_crossentropy',
                metrics = ['accuracy'])
  return model
```

- 이진 유니그램 모델 훈련 및 테스트
```python
model = get_model()
model.summary()
callbacks = [
    keras.callbacks.ModelCheckpoint("binary_1gram.keras", save_best_only = True)
]
model.fit(binary_1gram_train_ds.cache(),
          validation_data = binary_1gram_val_ds.cache(),
          epochs = 10,
          callbacks = callbacks)

model = keras.models.load_model('binary_1gram.keras')
print(f"테스트 정확도 : {model.evaluate(binary_1gram_test_ds)[1]:.3f}") # 0.888
```

#### 이진 인코딩을 사용한 바이그램
한 개념이 여러 단어로 표현될 수 있어서, 단어 순서를 무시하는 것은 파괴적이다. `united states`에서 두 단어를 분리하는 것은 원래 의미와 많이 다르다.  
따라서 단일 단어가 아닌 **N-그램을 사용하여 국부적인 순서 정보를 BoW 표현에 추가**한다.
- **바이그램**을 가장 널리 사용한다.
- `TextVectorization`에서는 이렇게 쓸 수 있다.
```python
text_vectorization =TextVectorization(
									  ngrams = 2,
									  max_tokens = 20000,
									  output_mode = 'multi_hot'
)
```

- 그 다음 모델 훈련은 똑같음
```python
text_only_train_ds = train_ds.map(lambda x, y : x) # 레이블 없는 데이터셋
text_vectorization.adapt(text_only_train_ds) # 데이터셋의 어휘사전 인덱싱

# 데이터셋 전처리 : 다중 CPU 코어 활용
binary_2gram_train_ds = train_ds.map(
    lambda x, y : (text_vectorization(x), y),
    num_parallel_calls = 4
)
binary_2gram_val_ds = val_ds.map(
    lambda x, y : (text_vectorization(x), y),
    num_parallel_calls = 4
)
binary_2gram_test_ds = test_ds.map(
    lambda x, y : (text_vectorization(x), y),
    num_parallel_calls = 4
)

model = get_model()
model.summary()
callbacks = [
    keras.callbacks.ModelCheckpoint("binary_2gram.keras", save_best_only = True)
]
model.fit(binary_2gram_train_ds.cache(),
          validation_data = binary_2gram_val_ds.cache(),
          epochs = 10,
          callbacks = callbacks)

model = keras.models.load_model('binary_2gram.keras')
print(f"테스트 정확도 : {model.evaluate(binary_2gram_test_ds)[1]:.3f}") # 0.897
```

#### TF-IDF 를 사용한 바이그램
- 개별 단어, N-그램의 등장 횟수를 카운트한 정보를 추가할 수 있다.
- 텍스트에 대한 단어의 히스토그램을 쓴다고 표현해도 좋음.

- 각 토큰에 등장빈도(횟수)까지 같이 띄우기
```python
text_vectorization =TextVectorization(
									  ngrams = 2,
									  max_tokens = 20000,
									  output_mode = 'count'
)
```
- `output_mode`에 `count`를 전달하면 된다.

- 관사 같은 건 문서 종류에 무관하게 많이 등장하는데, 어떻게 제거할 수 있을까?
- **전체 단어 카운트를 정규화**할 수 있다. 평균을 빼고 분산으로 나누는 방식을 이용함.
- 이전 예제들은 텍스트 1개를 1개의 벡터로 만들었기 때문에 희소행렬이 아니지만, 
- 여기서는 **각 토큰이 1이 1개만 있는 벡터로 표기되므로 희소성**을 띈다. 그렇기 때문에 계산 부하를 줄이고, 과대적합의 위험을 감소시킬 수 있다.
- 그런데 평균을 빼면 희소성이 깨진다(0이 아니게 되니까). 그래서 나눗셈만 이용하는 정규화 방식을 선택하는데, 가장 좋은 방법이 `TF-IDF:TextFrequence InverseDocumentFrequency` 정규화이다.

> tf-idf 정규화
> 토큰의 중요도를 따질 때, 
> 1. 전체 문서에서 많이 등장하면 중요도가 떨어진다.
> 2. 어떤 문서에서 많이 등장하는 단어는 중요도가 증가한다.

- 파이썬으로는 이런 식으로 계산함
```python
def tfidf(term, document, dataset):
	term_freq = document.count(term) # 개별 문서에서 등장하는 단어 갯수
	doc_freq = math.log(sum(doc.count(term) for doc in dataset) + 1)
	return term_freq / doc_freq
```

**- `TextVectorizer`에서는 tf-idf를 이렇게 구현함.**
```python
text_vectorization =TextVectorization(
									  ngrams = 2,
									  max_tokens = 20000,
									  output_mode = 'tf-idf'
)
```

- 다시 훈련 ㄱ
```python
text_only_train_ds = train_ds.map(lambda x, y : x) # 레이블 없는 데이터셋
text_vectorization.adapt(text_only_train_ds) # 데이터셋의 어휘사전 인덱싱

# 데이터셋 전처리 : 다중 CPU 코어 활용
tfidf_2gram_train_ds = train_ds.map(
    lambda x, y : (text_vectorization(x), y),
    num_parallel_calls = 4
)
tfidf_2gram_val_ds = val_ds.map(
    lambda x, y : (text_vectorization(x), y),
    num_parallel_calls = 4
)
tfidf_2gram_test_ds = test_ds.map(
    lambda x, y : (text_vectorization(x), y),
    num_parallel_calls = 4
)

model = get_model()
model.summary()
callbacks = [
    keras.callbacks.ModelCheckpoint("tfidf_2gram.keras", save_best_only = True)
]
model.fit(tfidf_2gram_train_ds.cache(),
          validation_data = tfidf_2gram_val_ds.cache(),
          epochs = 10,
          callbacks = callbacks)

model = keras.models.load_model('tfidf_2gram.keras')
print(f"테스트 정확도 : {model.evaluate(tfidf_2gram_test_ds)[1]:.3f}") # 0.894
```

- 여기서는 눈에 띄는 성능 향상이 없지만, 일반적으로 **기본 이진 인코딩 대비 1% 정도의 성능을 높일 수 있다.**

> 원시 문자열을 처리하는 모델 내보내기  
> `tf.data` 파이프라인의 일부로 텍스트 표준화, 분할, 인덱싱을 수행했다. 그러나 이 파이프라인과 독립적으로 실행되는 모델을 내보내야 한다면 자체 텍스트 전처리를 사용해야 한다. 그렇지 않다면 제품 환경에서 다시 구현해야 하며, 이는 훈련 데이터와 제품 환경 데이터 간의 미묘한 차이가 생길 수도 있다.  
```python
inputs = keras.Input(shape = (1, ), dtype = 'string') # 1개의 문자열 입력 샘플
processed_inputs = text_vectorization(inputs) # 텍스트 전처리 수행
outputs = model(processed_inputs) # 이전 훈련 모델 적용
inference_model = keras.Model(inputs, outputs)

# 원시문자열 배치 처리
import tensorflow as tf
raw_text_data = tf.convert_to_tensor([
									  ["That was an excellent movie, I loved it."],
])
predictions = inference_model(raw_text_data)
print(f"긍정적인 리뷰일 확률 : {float(predictions[0]* 100):.2f} 퍼센트" )
```

### 시퀀스 모델 방식
`시퀀스 모델` : 순서 기반의 특성을 수동으로 만들지 않고, **원시 단어 시퀀스를 모델에 전달해서 스스로 특성을 학습하도록 한다.**  
- 이를 구현하려면 
1. 우선 입력을 정수 인덱스의 시퀀스로 표현해야 한다.
	- 1개의 정수가 1개의 단어를 의미한다.  
2. 이후 각 정수를 벡터로 매핑, 벡터 시퀀스를 얻는다.  
3. 마지막으로 1D 컨브넷, RNN, 트랜스포머 등 인접한 벡터의 특징을 비교할 수 있는 층에 전달한다.

- 2016~17년 사이 양방향 RNN이 최고의 성능을 냈으나, 오늘날의 `시퀀스 모델링`은 대부분 `트랜스포머`를 이용하여 수행된다.

#### 1번째 예제
```python
from tensorflow.keras import layers

max_length = 600
max_tokens = 20000

# 600개 단어를 넘는 데이터는 5%라서 합리적인 선택이래
text_vectorization = layers.TextVectorization(
    max_tokens = max_tokens, # 600개 단어 이후는 자른다 
    output_mode = 'int',
    output_sequence_length = max_length
)

text_vectorization.adapt(text_only_train_ds)

# 데이터셋 전처리 : 다중 CPU 코어 활용
int_train_ds = train_ds.map(
    lambda x, y : (text_vectorization(x), y),
    num_parallel_calls = 4
)
int_val_ds = val_ds.map(
    lambda x, y : (text_vectorization(x), y),
    num_parallel_calls = 4
)
int_test_ds = test_ds.map(
    lambda x, y : (text_vectorization(x), y),
    num_parallel_calls = 4
)
```

- 모델 생성 
- 정수 -> 벡터 시퀀스로 만드는 가장 간단한 방법은 `원-핫 인코딩`이다.
- `원-핫 벡터`위에 양방향 LSTM 층을 추가한다.
```python
import tensorflow as tf

inputs = keras.Input(shape = (None, ), dtype = 'int64') # 정수 시퀀스의 입력
embedded = tf.one_hot(inputs, depth = max_tokens) # 2만 차원 이진 벡터로 인코딩
x = layers.Bidirectional(layers.LSTM(32))(embedded) # 양방향 LSTM 층
x = layers.Dropout(0.5)(x)
outputs = layers.Dense(1, activation = 'sigmoid')(x) # 분류층
model = keras.Model(inputs, outputs)
model.compile(optimizer = 'rmsprop',
			 loss = 'binary_crossentropy',
			 metrics = ['accuracy'])
model.summary()

```

- 모델 훈련
```python
callbacks =[
			keras.callbacks.ModelCheckpoint("one_hot_bidir_lstm.keras",
											save_best_only = True)
]

model.fit(int_train_ds, validation_data = int_val_ds, epochs = 10, callbacks = callbacks)
model = keras.models.load_model("one_hot_bidir_lstm.keras")
print(f"테스트 정확도 : {model.evaluate(int_test_ds)[1]:.3f}")
```

- 결과 
> 1. 매우 느리다. 입력 크기가 (600, 20000) 크기의 행렬로 인코딩되어 있기 떄문에, **하나의 영화 리뷰는 1200만개의 부동 소수점으로 이뤄진다.** 따라서 양방향 LSTM이 할 일이 많다.
> 2. 테스트 정확도는 약 87% 정도로, 이진 유니그램 모델보다 속도도 느리고 성능도 떨어진다.

- 따라서 원핫인코딩으로 단어를 벡터로 바꾸는 것은 좋은 생각이 아니다. 대신 사용할 수 있는 것이 `단어 임베딩Word Embedding`이다.

#### 단어 임베딩
원핫인코딩 = 특성공학이지만, `인코딩 토큰은 모두 독립적`이라는 가정을 내포한다.  
그러나 원-핫 벡터는 모두 직교하므로, 이런 가정은 잘못되었다.  
단어는 구조적인 공간을 형성한다. 단어 사이에는 공유되는 정보가 있다.  
예를 들면, `movie`와 `film`은 비슷한 의미로 쓰이는 경우가 대부분이기 때문에 `movie` 벡터가 `film` 벡터와 직교하면 안된다. 두개의 벡터는 매우 비슷하거나 가까워야 한다.

- **두 단어 벡터 사이의 기하학적 관계는 단어 사이의 의미 관계를 반영**해야 한다.
	- 동의어는 비슷한 단어 벡터로 임베딩되어야 한다.
	- 이 떄, **두 단어 벡터 사이의 기하학적 거리(`코사인 거리 or L2 거리`)를 의미 거리**라고 생각할 수 있다.

- `단어 임베딩Word Embedding`은 위 아이디어를 반영한 벡터 표현으로, 사람의 언어를 구조적인 기하학 공간에 매핑한다.

- **단어임베딩은 저차원의 부동소수점** 벡터이다.
		- 단어 임베딩은 **많은 정보를 더 적은 차원으로 압축**한다.
- **단어 임베딩은 구조적인 표현이며, 데이터로부터 학습**된다. 비슷한 단어는 가까운 위치에 매핑된다.
	- ex) 2차원 공간에 `dog, wolf, cat, tiger`가 있다고 칠 때, `dog -> wolf` 벡터와 `cat -> tiger`로 가는 벡터를 같은 벡터라고 하면, 이 벡터를 "애완동물 -> 야생동물"이라고 해석할 수 있다.
- 의미 있는 기하학적 변환의 예시는 `성별` 벡터와 `복수Plural` 벡터이다.
	- `king`+ `female` -> `queen`
	- `king` + `plural` -> `kings`
	- 단어 임베딩 공간에는 **이러한 해석 가능하고 잠재적으로 유용한 수천 개의 벡터**가 있다.

- 단어 임베딩을 만드는 2가지 방법.
1. **현재 작업과 함께 단어 임베딩을 학습한다.** 랜덤한 단어 벡터로 시작, 신경망의 가중치를 학습하는 것과 같은 방식으로 단어 벡터를 학습한다.
2. 현재 작업과는 다른 머신 러닝 작업에서 미리 계산된 단어 임베딩을 모델에 로드한다. 이를 `사전 훈련된 단어 임베딩Pretrained Word Embedding`이라고 한다.

#### Embedding 층으로 단어 임베딩 학습하기
- 좋은 단어 임베딩 공간은, 문제에 따라 크게 달라진다.  
	- 영화 리뷰 감성 분석 모델을 위한 단어 임베딩 공간은, 법률 문서 문류 모델을 위한 단어 임베딩 공간과는 다를 것이다. 의미 관계의 중요성은 작업마다 다르기 때문이다.

- 따라서 새로운 작업에는 새로운 임베딩을 할당하는 게 타당하다.
```python
embedding_layer = layers.Embedding(input_dim = max_tokens, 
								   output_dim = 256)
```
- 2개의 매개변수가 필요하다. `토큰의 개수`와 `임베딩 차원`이다.
- **임베딩 층은 정수 인덱스를 밀집 벡터로 매핑하는 딕셔너리**로 이해하는 게 좋다.
	- 정수를 입력으로 받음 -> 내부 딕셔너리 -> 정수에 연관된 벡터를 찾아 반환.
	- = 딕셔너리 룩업
- `(batch_size, sequence_length)`의 크기를 갖는 랭크-2 정수 텐서를 입력으로 받는다. 
- `(batch_size, sequence_length, embedding_dimensionality)`인 랭크-3 부동 소수점 텐서를 반환한다.

- 가중치는 랜덤하게 초기화되며, 단어 벡터가 역전파로 점차 조정되고 후속 모델이 사용할 수 있도록 임베딩 공간을 구성한다. 훈련이 끝나면 임베딩 공간은 특정 문제에 전문화된 여러 구조를 갖는다.

- 단어 임베딩 층 포함한 모델과 훈련
```python
import tensorflow as tf

inputs = keras.Input(shape = (None, ), dtype = 'int64') # 정수 시퀀스의 입력
embedded = layers.Embedding(input_dim = max_tokens, 
								   output_dim = 256)(inputs)
x = layers.Bidirectional(layers.LSTM(32))(embedded) # 양방향 LSTM 층
x = layers.Dropout(0.5)(x)
outputs = layers.Dense(1, activation = 'sigmoid')(x) # 분류층
model = keras.Model(inputs, outputs)
model.compile(optimizer = 'rmsprop',
			 loss = 'binary_crossentropy',
			 metrics = ['accuracy'])
model.summary()

callbacks =[
			keras.callbacks.ModelCheckpoint("one_hot_bidir_lstm.keras",
											save_best_only = True)
]

model.fit(int_train_ds, validation_data = int_val_ds, epochs = 10, callbacks = callbacks)
model = keras.models.load_model("one_hot_bidir_lstm.keras")
print(f"테스트 정확도 : {model.evaluate(int_test_ds)[1]:.3f}") # 0.862
```
- 원-핫 모델보다 훨씬 빠르고 정확도는 비슷하지만, 바이그램보단 느리다.
	- 모델의 데이터가 적기 떄문이다. 바이그램 모델은 전체 데이터를 썼지만, 여기서는 600개의 토큰만을 사용했다.

#### 패딩과 마스킹 이해하기
- 입력 시퀀스가 0으로 가득 차있으면 모델 성능에 악영향을 미친다. `TextVectorization(output_sequence_length = max_length)`을 지정했기 때문이다. 
	- 토큰이 600개가 넘는다면 자르고, 600개보다 적다면 뒷쪽은 0으로 채워진다.

- **두 RNN이 병렬로 실행되는 양방향 RNN**을 사용한다. 한 층은 원래 순서대로 토큰을 처리하고, 다른 층은 동일한 토큰을 거꾸로 처리한다. 
	- 원래 순서로 토큰을 보는 RNN 층은 마지막에 패딩 인코딩 벡터만 처리한다. 짧은 문장이 많아진다면, RNN 내부 정보는 의미 없는 입력을 처리하면서 사라지게 된다.
	- 이러한 패딩을 건너뛰는 API로 `마스킹Masking`을 제공한다.

- `Embedding` 층은 입력 데이터에 상응하는 마스킹을 생성할 수 있다.
- 마스킹은 `1`과 `0`으로 이뤄진 `(batch_size, sequence_length)` 크기의 텐서이다. `mask[i, t]`는 샘플 i의 타임스텝 t를 건너뛸지 말지를 나타낸다. (0이면 건너뜀)
- 기본적으로 활성화되어 있지 않으나, `Embedding(mask_zero = True)`로 켤 수 있고, `Embedding.compute_mask()`로 마스킹을 추출할 수 있다.

- **실전에서는 수동으로 마스킹을 관리할 필요가 없다.** 케라스가 마스킹 처리할 수 있는 모든 층에 자동으로 전달하며, 마스킹을 사용해 RNN 층은 마스킹을 건너뛴다.
- 모델이 전체 시퀀스를 반환한다면, 손실 함수도 마스킹을 사용해 출력 시퀀스에서 마스킹 스텝을 건너뛴다.
```python
import tensorflow as tf

inputs = keras.Input(shape = (None, ), dtype = 'int64') # 정수 시퀀스의 입력
embedded = layers.Embedding(input_dim = max_tokens, 
								   output_dim = 256,
								   mask_zero = True)(inputs)
x = layers.Bidirectional(layers.LSTM(32))(embedded) # 양방향 LSTM 층
x = layers.Dropout(0.5)(x)
outputs = layers.Dense(1, activation = 'sigmoid')(x) # 분류층
model = keras.Model(inputs, outputs)
model.compile(optimizer = 'rmsprop',
			 loss = 'binary_crossentropy',
			 metrics = ['accuracy'])
model.summary()

callbacks =[
			keras.callbacks.ModelCheckpoint("embedding_bidir_lstm.keras",
											save_best_only = True)
]

model.fit(int_train_ds, validation_data = int_val_ds, epochs = 10, callbacks = callbacks)
model = keras.models.load_model("embedding_bidir_lstm.keras")
print(f"테스트 정확도 : {model.evaluate(int_test_ds)[1]:.3f}") # 0.874
```

#### 사전 훈련된 단어 임베딩 적용하기
- 훈련 데이터가 부족할 때 사용할 수 있다.
	- 이미지 데이터에서 사전 훈련된 컨브넷을 사용하는 것과 유사한 방식이다.
	- 충분한 데이터가 없어 좋은 특성을 학습하지 못하지만, 꽤 일반적인 특성이 필요할 때 쓴다. 

- 단어 임베딩은 단어 출현 통계를 사용하여 계산된다. 
	- `Word2vec` 알고리즘이 2013년에 개발되었고, 연구나 산업 애플리케이션에 적용되었다.
		- 성별 같은 구체적인 의미가 있는 속성을 잡아낸다.

- 케라스에서는 `Embedding` 층을 이해 내려받을 수 있는, 미리 계산된 단어 임베딩 데이터베이스가 여럿 있다. 
	- `Word2vec`
	- `GloVe` 
		- 단어의 동시 출현 통계를 기록한 행렬을 분해한다.
		- 위키피디아 데이터와 커먼 크롤 데이터에서 가져온 수백만 개의 영어 토큰 임베딩을 미리 계산해두었다.

##### `GloVe` 임베딩 적용하기
```python
# 2014년 영어 위키피디아 데이터셋에서 미리 계산된 GloVe 단어 임베딩
!wget http://nlp.stanford.edu/data/glove.6B.zip
!unzip -q glove.6B.zip
```
- **40만 개의 단어 및 단어가 아닌 토큰에 대한 100차원 임베딩 벡터**를 담는다.
- 비교) 직접 학습한 임베딩 층은 256차원이었다.

- `GloVe` 파싱하기
```python
import numpy as np

path_to_glove_file = "glove.6B.100d.txt"
embeddings_index = {}
with open(path_to_glove_file) as f:
	for line in f:
		word, coefs = line.split(maxsplit = 1)
		coefs = np.fromstring(coefs, "f", sep = " ")
		embedding_index[word] = coefs

print(f"단어 벡터 갯수 : {len(embedding_index)}") # 400000
```

- `Embedding` 행렬 만들기 
- 로드하기 위해서 행렬의 크기를 `(max_words, embedding_dim)`으로 만들어야 한다.
- `i`번째 원소는 토큰화로 만든 단어 인덱스의 `i`번째의 단어에 해당하는 `embedding_dim` 차원의 벡터이다.
```python
embedding_dim = 100

# 인덱싱 단어 추출
voca = text_vectorization.get_vocabulary()

# 어휘 사전의 단어와 인덱스 매핑
word_index = dict(zip(voca, range(len(voca))))

# GloVe 벡터를 담는 행렬 준비
embedding_matrix = np.zeros((max_tokens, embedding_dim)) 
for word, i in word_index.items():

	if i  < max_tokens:
		embedding_vector = embeddings_index.get(word)
		
	# 인덱스 i에 대한 단어 벡터로 행렬의 i번째 항목을 채운다
	# 임베딩 인덱스에 단어가 없다면 0
	if embedding_vector is not None:
		embedding_matrix[i] = embedding_vector

# 초기화 : 사전 훈련 임베딩 층에 넣기, 훈련 중 변경 X
embedding_layer = layers.Embedding(
								   max_tokens,
								   embedding_dim,
								   embeddings_initializer= keras.initializers.Constant(embedding_matrix),
								   trainable = False, # 훈련 중 가중치 변경 X
								   mask_zero = True
)
```

- 훈련
```python
inputs = keras.Input(shape = (None, ), dtype = 'int64') # 정수 시퀀스의 입력
embedded = embedding_layer(inputs)
x = layers.Bidirectional(layers.LSTM(32))(embedded) # 양방향 LSTM 층
x = layers.Dropout(0.5)(x)
outputs = layers.Dense(1, activation = 'sigmoid')(x) # 분류층
model = keras.Model(inputs, outputs)
model.compile(optimizer = 'rmsprop',
			 loss = 'binary_crossentropy',
			 metrics = ['accuracy'])
model.summary()

callbacks =[
			keras.callbacks.ModelCheckpoint("glove_embeddings_sequence_model.keras",
											save_best_only = True)
]

model.fit(int_train_ds, validation_data = int_val_ds, epochs = 10, callbacks = callbacks)
model = keras.models.load_model("glove_embeddings_sequence_model.keras")
print(f"테스트 정확도 : {model.evaluate(int_test_ds)[1]:.3f}") # 0.872
```

- 이 작업에서는 사전 훈련 임베딩의 효과가 높지 않은데, 작업 특화 임베딩을 학습하기 충분한 샘플이 데이터셋에 있기 때문에 그렇다.



## 트랜스포머 아키텍처
- 2017년 **`트랜스포머`가 자연어 처리 작업에서 `RNN`을 앞지르기 시작했다.**
- 트랜스포머는 `뉴럴 어텐션Neural Attention`이라고 부르는 간단한 매커니즘을 사용해, 강력한 시퀀스 모델을 만들 수 있다.

> - `셀프 어텐션Self-attention`을 사용해 트랜스포머 인코더를 만들고, IMDB에 이를 적용한다.

### 셀프 어텐션 이해하기
- 책을 보면서 어떤 부분은 주의깊게, 어떤 부분은 대충 훑고 지나갈 수 있다.
- 모델이 이런 식으로 동작하면 어떨까? 비슷한 개념이 이미 나온 적 있다.
	- 컨브넷에서 최대 풀링은 해당 공간에서 가장 중요한 특성만 선택한다. `전부 or Not = 어텐션`
	- TF-IDF 정규화는토큰이 전달하는 정보량에 따라 중요도를 할당한다. 중요도가 낮은 토큰은 무시된다. `연속적인 어텐션`
- 어텐션 개념은 `특성에 관한 중요도 점수`를 계산하는 것에서 시작된다.
- 점수를 계산하는 방법과, 점수로 수행하는 작업은 접근 방식에 따라 다르다.

- `문맥 인식Context-Aware` 특성에 쓰일 수 있다.
단어 임베딩에서, 단어 하나가 고정되는 순간 다른 단어들의 상대적인 위치도 모두 고정되는데, 이는 언어가 일반적으로 작동하는 방식이 아니다.   
`date`만 해도 날짜, 데이트, 대추라는 의미가 있다.   
`see`만 해도 살펴보다, 알겠다 등이 있다.

- 스마트한 임베딩 공간이라면 **주변 단어에 따라 단어의 벡터 표현이 달라지는데, 여기에 `셀프 어텐션`이 사용된다.**

> 예시 : `the train left the station on time`에서 `station`은 어떤 걸 의미할까? 라디오 방송국? 국제 우주 정거장?
> 1. **`station` 벡터와 문장의 모든 다른 단어 사이의 관련성 점수를 계산**한다. `어텐션 점수` 계산으로, 두 벡터 사이의 `점곱`을 이용한다.
>> 이미 두 단어 사이의 임베딩을 서로 관련시키는 표준적인 방법이었다. 이후 스케일링 함수와 소프트맥스를 거치나 이는 구현 세부사항일 뿐이다.
>
>2. **관련성 점수로 가중치를 두어, 문장에 있는 모든 단어 벡터의 합을 계산**한다. `station` 단어를 포함, `station` 단어와 밀접한 단어는 덧셈에 더 많이 기여할 것이다. 이 때 만들어진 벡터는 `station`의 새로운 표현이며 주변 문맥이 통합된 표현이다. 특히, `train`의 일부가 포함되므로 `train station`이라는 의미에 더 근접해진다.  

- 이를 넘파이 수도코드로 구현하면 이렇다
```python
def self_attention(input_sequence):
	output = np.zeros(shape = input_sequence.shape)
	for i, pivot_vector in enumerate(input_sequence):
		scores = np.zeros(shape = (len(input_sequence), ))
		for j, vector in enumerate(input_sequence):
			scores[j] = np.dot(pivot_vector, vector.T) # 토큰-토큰 사이의 점곱 계산
		
		# 스케일링 & 소프트맥스
		scores /= np.sqrt(input_sequence.shape[1])
		scores = softmax(scores)
		
		new_pivot_representation = np.zeros(shape = pivot_vector.shape)
		for j, vector in enumerate(input_sequence):
			new_pivot_representation += vector * scores[j] # 모든 토큰에 가중치 점수 부여 & 더함
		output[i] = new_pivot_representation
	
	return output
```

- 실전에서는 `MultiHeadAttention` 층을 제공하며, 이렇게 쓴다.
```python
num_heads = 4
embed_dim = 256
mha_layer = MultiHeadAttention(num_heads = num_heads, key_dim = embed_dim)
outputs = mha_layer(inputs, inputs, inputs)
```
> 궁금증
> 1. 입력을 3번씩 전달하는 이유?
> 2. 멀티 헤드가 뭐죠?

#### 일반화된 셀프 어텐션 : 쿼리 - 키 - 값 모델
- 트랜스포머 아키텍처는 원래 기계 번역을 위해 개발되었다.
- 기계 번역은 2개의 입력 시퀀스를 다룬다 
	- `소스 시퀀스Source Sequence` : 현재 번역하려는 것
	- `타깃 시퀀스Target Sequence` : 번역되어 나오는 결과물
- 트랜스포머는 한 시퀀스를 다른 시퀀스로 변환하기 위해 고안된 `시퀀스 투 시퀀스 Sequence-to-Sequence` 모델이다.

- 셀프 어텐션 매커니즘은 대충 이렇게 수행된다.
```
outputs = sum(inputs * pairwise_scores(inputs, inputs))
				C						A		B	
```
- "인풋 A의 모든 토큰이 인풋 B의 모든 토큰에 얼마나 관련되어 있는지 확인하고, 이 점수로 인풋 C에 있는 모든 토큰의 가중치 합을 계산한다" 이다.
- A, B, C가 모두 달라도 상관 없다. 
	- `A` = `쿼리(Query)`
	- `B` = `키(Key)`
	- `C` = `값(Value)`
- 즉, `쿼리`의 모든 원소가 `키`의 모든 원소와 얼마나 관련되어 있는지 계산 & 이 점수로 `값`의 모든 원소이 가중치 합을 계산한다.

> 검색 엔진, 추천 시스템에서 유래되었다. 
> 쿼리를 `dogs on the bench`라고 날리면
>  DB 내의 각 사진에는 `키워드, 키`가 연결되어 있다. `dogs, cat, party` 등
>  엔진은 쿼리와 키를 비교한 뒤, 매칭 강도에 따라 키의 순서를 매긴 후 상위 N개의 사진을 반환한다.

- 위 과정이 개념적으로 트랜스포머 스타일의 어텐션이 하는 일이다. `쿼리`, `키` 뿐만 아니라 지식의 본체인 `값`도 갖고 있다. 
- 실제로 `키`와 `값`은 같은 시퀀스인 경우가 많다.
	- 기계 번역에서는 
		- 타깃 시퀀스 = 쿼리
		- 소스 시퀀스 = 키, 값
	- 시퀀스 분류에서는 쿼리 = 키 = 값이다.

#### 멀티 헤드 어텐션
- `Attention is all you need`라는 논문에 소개된, 셀프 어텐션 매커니즘의 변형이다.
- **`멀티 헤드`란, 셀프 어텐션의 출력 공간이 독립적으로 학습되는 부분 공간으로 나뉜다**는 것이다.
- `(Dense(쿼리) + Dense(키) + Dense(값)) -> (어텐션) `을 거치는 과정을 `헤드`라고 한다.
	1. 3개의 인풋이 각각 3개의 벡터로 처리된다.
	2. 3개의 벡터는 뉴럴 어텐션으로 처리되어 1개의 출력 시퀀스로 연결된다.
- 멀티 헤드는 이런 `헤드` 과정이 병렬적으로 처리된 뒤, `Concatenate`되어 출력됨을 의미한다.
	- 그림에서는 동일한 쿼리, 키, 값이 다른 헤드에 들어가는 것으로 나와 있음.

- 이 작동 원리는 `깊이별 분리 합성곱`과 비슷하다.
	- 합성곱의 출력 공간이 독립적으로 학습되는 많은 부분 공간으로 나뉜다.
	- 컴퓨터 비전에는 이외에도 `그룹 합성곱Grouped Convolution`이 있음

### 트랜스포머 인코더
- `Dense` 투영을 추가하는 게 유용하다면, 어텐션 매커니즘의 출력에 추가하는 것도 생각해볼 수 있다.
	- 모델이 깊어지므로 `잔차 연결도 추가`한다.
	- 마찬가지로 정규화 층도 추가한다.

- 모델의 구성은 결국 이렇게 된다. 이를 `트랜스포머 인코더Transformer Encoder`라고 한다.
> 1. 멀티 헤드 어텐션에 M, K, V가 들어간다.
> 2. 멀티 헤드 어텐션의 출력에 M, K, V가 더해진다(잔차 연결)
> 3. 정규화(LayerNormalization)
> 4. 2개의 밀집 투영 층에 넣는다(`Dense -> Dense`)
> 5. `4.`의 출력에 `4.`의 입력을 더한다(잔차 연결)
> 6. 정규화(LayerNormalization)

```python
import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import layers

class TransformerEncoder(layers.Layer):
  def __init__(self, embed_dim, dense_dim, num_heads, **kwargs):
    super().__init__(**kwargs)
    self.embed_dim = embed_dim # 입력 토큰 벡터 크기
    self.dense_dim = dense_dim # 내부 밀집 층 크기
    self.num_heads = num_heads # 어텐션 헤드 개수
    self.attention = layers.MultiHeadAttention(
        num_heads = num_heads, key_dim = embed_dim
    )
    self.dense_proj = keras.Sequential(
        [
            layers.Dense(dense_dim, activation = 'relu'),
            layers.Dense(embed_dim),
        ]
    )
    self.layernorm_1 = layers.LayerNormalization()
    self.layernorm_2 = layers.LayerNormalization()

  # 연산 수행
  def call(self, inputs, mask = None):
    
    # Embedding층의 마스크는 2D이나, 어텐션층은 3D나 4D 기대
    if mask is not None: 
      mask = mask[:, tf.newaxis, :]

    attention_output = self.attention(inputs, inputs, attention_mask = mask)
    proj_input = self.layernorm_1(inputs + attention_output)
    proj_output = self.dense_proj(proj_input)
    return self.layernorm_2(proj_input + proj_output)

  # 모델 저장을 위한 직렬화 구현
  def get_config(self): 
    config = super().get_config()
    config.update({
        'embed_dim' : self.embed_dim,
        'num_heads' : self.num_heads,
        'dense_dim' : self.dense_dim
    })
    return config
```

- `BatchNormalziation` 층을 쓰지 않는 이유 : 시퀀스 데이터에는 잘 맞지 않는다.
	- 대신 배치의 각 시퀀스를 독립적으로 정규화하는 `LayerNormalization`을 쓴다.

- 둘을 pseudo code로 비교하면
```python
def layer_normalization(batch_of_sequences): #(batch_size, seq_length, embed_dim)
	mean = np.mean(batch_of_sequences, keepdims = True, axis = -1)
	variance = np.var(batch_of_sequences, keepdims = True, axis = -1)
	return (batch_of_sequences - mean) / variance

def batch_normalization(batch_of_images): # (batch_size, height, width, channels)
	mean = np.mean(batch_of_images, keepdims = True, axis = (0, 1, 2))
	variance = np.var(batch_of_images, keepdims = True, axis = (0, 1, 2))
	return (batch_of_images - mean) / variance
```

> 사용자 정의 층 저장하기 : `get_config()` 메서드를 구현해야 한다.
> 이 메서드는 층을 만들 때 사용한 생성자 매개변수 값을 담은 파이썬 딕셔너리를 반환한다.  
> 모든 케라스층은 직렬화 or 역직렬화할 수 있다.
> 직렬화 : `config = layer.get_config()`
> 역직렬화 : `new_layer = layer.__class__.from_config(config)` - config 자체는 가중치 값이 없어서 모든 가중치는 처음 상태로 초기화된다.
> 이런 모델을 저장한 다음 로드할 때, 설정 딕셔너리를 이해할 수 있도록 사용자 정의 클래스를 알려줘야 한다.
```python
model = keras.models.load_model(filename, custom_objects = {"PositionalEmbedding" : PositionalEmbedding})
```

- 트랜스포머 인코더로 텍스트 분류하기
```python
voca_size = 20000
embed_dim = 256
num_heads = 2
dense_dim = 32

inputs = keras.Input(shape = (None, ), dtype = 'int64')
x = layers.Embedding(voca_size, embed_dim)(inputs)
x = TransformerEncoder(embed_dim, dense_dim, num_heads)(x)
# 위 인코더는 전체 시퀀스를 반환하므로, 분류를 위해 전역 풀링층으로 각 시퀀스를 1개의 벡터로 만든다.
x = layers.GlobalMaxPooling1D()(x) 
x = layers.Dropout(0.5)(x)
outputs = layers.Dense(1, activation = 'sigmoid')(x)

model = keras.Model(inputs, outputs)
model.compile(optimizer = 'rmsprop', loss = 'binary_crossentropy', metrics = ['accuracy'])
model.summary()

# 훈련
callbacks = [
			 keras.callbacks.ModelCheckpoint("transformer.encoder.keras",
												 save_best_only = True)
]

model.fit(int_train_ds, validation_data = int_val_ds, epochs = 20, callbacks = callbacks)

# 불러오기 & 추론
model = keras.models.load_model("transformer.encoder.keras",
							   custom_objects = {"TransformerEncoder": TransformerEncoder}) # get_config을 지정한 경우, 불러올 때도 이걸 지정해주자.
print(f"테스트 정확도 : {model.evaluate(int_test_ds)[1]:.3f}") # 0.874
```

- 여기서 사용하는 **트랜스포머 인코더는 시퀀스 모델이 아니다!**
	- 시퀀스 토큰을 독립적으로 처리하는 `밀집 층`
	- 토큰을 집합으로 바라보는 `어텐션 층`으로 구성되어 있다.
	- 즉, 시퀀스 내의 토큰 순서를 바꿔도 동일한 어텐션 점수와 표현을 얻을 것이다.

- 트랜스포머가 순서를 고려하지 않는데, 어떻게 좋은 성능이 나올까?
- 트랜스포머는 기술적으로 순서에 구애받지 않지만, 모델이 처리하는 표현에 순서 정보를 수동으로 주입하는 하이브리드 방식이다. 이를 `위치 인코딩Positional Encoding`이라고 한다.

|               | 단어 순서 고려 | 문맥 고려(단어 간 상호작용) |
| ------------- | -------------- | --------------------------- |
| 유니그램 모델 | X              | X                           |
| 바이그램 모델 | 매우 제약됨    | X                           |
| RNN           | O              | X                           |
| 셀프 어텐션   | X              | O                           |
| 트랜스포머    | O              | O                            |

#### 위치 정보 주입하기 : 위치 인코딩
- 모델에 단어 순서 정보를 제공하기 위해, 문장의 단어 위치를 각 단어 임베딩에 추가한다.

- 입력 단어의 임베딩은 두 부분으로 구성된다.
	- 일반 단어 임베딩 : 특정 문맥에 독립적으로 단어 표현
	- 위치 벡터 : 현재 문장의 단어 위치 표현

- 단어 위치를 임베딩 벡터에 연결해서 구현할 수 있다.
- 위치가 매우 큰 정수가 되어 임베딩 벡터 값의 범위를 넘어갈 수 있다.

- 이를 위한 트릭으로, 위치에 따라 주기적으로 바뀌는 `[-1, 1]` 벡터를 코사인 함수를 사용해 단어 임베딩에 추가했다.
	- 이를 이용해 넓은 범위의 어떤 정수도 고유한 값으로 표현할 수 있다. 여기선 안 쓸 거임.

- 위치 임베딩 벡터를 학습하고, 위치 임베딩을 이에 해당하는 단어 임베딩에 추가하여 위치를 고려한 단어 임베딩을 만든다. 이를 `위치 임베딩`이라고 한다.
```python
class PositionalEmbedding(layers.Layer):

  # 위치 임베딩 : 시퀀스 길이를 미리 알아야 한다는 게 단점임
  def __init__(self, sequence_length, input_dim, output_dim, **kwargs): 
    super().__init__(**kwargs)
    self.token_embeddings = layers.Embedding(
        input_dim = input_dim, output_dim = output_dim
    )
    self.position_embeddings = layers.Embedding(
        input_dim = sequence_length, output_dim = output_dim
    )
    self.sequence_length = sequence_length
    self.input_dim = input_dim
    self.output_dim = output_dim

  def call(self, inputs):
    length = tf.shape(inputs)[-1]
    positions = tf.range(start = 0, limit = length, delta = 1)
    embedded_tokens = self.token_embeddings(inputs)
    embedded_positions = self.position_embeddings(positions)
    return embedded_tokens + embedded_positions

  # 입력 0 패딩을 무시하는 마스킹 생성. 
  # 프레임워크에 의해 자동으로 호출되며, 마스킹은 다음 층으로 전달된다.  
  def compute_mask(self, inputs, mask = None):
    return tf.math.not_equal(inputs, 0)

  def get_config(self):
    config = super().get_config()
    config.update({
        "output_dim" : self.output_dim,
        "sequence_length" : self.sequence_length,
        "input_dim" : self.input_dim
    })
    return config

```

- 트랜스포머 인코더와 위치 임베딩 합치고 훈련하기
```python
# 트랜스포머 인코더와 위치 임베딩 합치기

voca_size = 20000
sequence_length = 600
embed_dim = 256
num_heads = 2
dense_dim = 32

inputs = keras.Input(shape = (None, ), dtype = 'int64')
x = PositionalEmbedding(sequence_length, voca_size, embed_dim)(inputs) # 위치 임베딩 추가
x = TransformerEncoder(embed_dim, dense_dim, num_heads)(x)
x = layers.GlobalMaxPooling1D()(x) 
x = layers.Dropout(0.5)(x)
outputs = layers.Dense(1, activation = 'sigmoid')(x)

model = keras.Model(inputs, outputs)
model.compile(optimizer = 'rmsprop', loss = 'binary_crossentropy', metrics = ['accuracy'])
model.summary()

# 훈련
callbacks = [
			 keras.callbacks.ModelCheckpoint("full_transformer.encoder.keras",
												 save_best_only = True)
]

model.fit(int_train_ds, validation_data = int_val_ds, epochs = 20, callbacks = callbacks)

# 불러오기 & 추론
model = keras.models.load_model("full_transformer.encoder.keras",
							   custom_objects = {"TransformerEncoder": TransformerEncoder,
                            "PositionalEmbedding" : PositionalEmbedding})
print(f"테스트 정확도 : {model.evaluate(int_test_ds)[1]:.3f}")
```

- 가장 뛰어난 시퀀스 모델이지만, 여전히 BoW 방식이 더 좋은 성능을 낸다.

### 언제 시퀀스를 쓰고, 언제 BoW를 쓰나요?

- BoW 방법이 구식이고, 트랜스포머 기반 시퀀스 모델이 신식인가? 아니다.
- 이번 장에서 가장 뛰어난 성능을 내는 건 바이그램 BoW였기 때문이다.

#### 텍스트 분류라는 과제에 한해서,
- 텍스트 분류 작업 시도 시, 훈련 데이터의 샘플 갯수와 샘플의 평균 단어 갯수 비율 사이에 주의를 기울여야 한다.
$$
\frac{샘플 개수}{평균 샘플 길이}
$$
- 이 비율이 `<1500`이면 바이그램 BoW 모델의 성능이 더 낫다(더 빠르고, 많이 반복할 수 있다.)
- 비율이 `>1500`이면 시퀀스 모델이 낫다.
- 즉, **시퀀스 모델은 훈련 데이터가 많고, 샘플이 짧은 경우 잘 작동**한다.

- 예를 들면, 1000개의 단어 길이, 10만개의 문서가 있다면 비율이 100이므로 바이그램을 쓴다.
- 50만개의 트윗과 40개의 단어라면 트랜스포머를 쓴다. 비율이 12500이니까
- IMDB의 경우 샘플은 2만개, 평균은 233이다. 따라서 바이그램을 써야 한다.

- 직관적인 이해 
> 1. 데이터 갯수
> - 시퀀스 모델은 더 풍부하고 복잡한 공간을 표현, 더 많은 데이터가 필요하다.
> - 단어 집합은 단순한 공간이므로 수백 ~ 수천 개의 샘플로 로지스틱 회귀를 훈련할 수 있다.

> 2. 샘플 길이
> - **샘플이 짧을수록** 모델은 샘플의 정보를 무시할 수 없으며, **단어 순서가 더 중요**해진다.
`this movie is the bomb(이 영화 끝내줬다)`와 `this movie is a bomb(ㅆㄺ였다?)`를 비교하면, BoW 모델은 유니그램 표현이 비슷하므로 혼동이 있을 수 있지만 시퀀스 모델은 긍/부정을 구분할 수 있다.
>- **샘플이 길수록** 단어 통계를 더 신뢰할 수 있고, **단어 히스토그램만으로 주제나 감성을 잘 드러낼 수 있는 것**이다.

- 타이틀에도 있듯 "텍스트 분류"일 때만 저 규칙이 맞다.
- 예를 들면 기계 번역은 **매우 긴 문장에서 RNN보다 트랜스포머가 더 성능이 좋다.** (여기서는 텍스트가 길수록 RNN이 좋다고 했는데!)


## 텍스트 분류를 넘어 : 시퀀스 투 시퀀스 학습

- `시퀀스 투 시퀀스 모델`은 입력으로 시퀀스(문장, 문단 등)를 받아 다른 시퀀스로 바꾼다.
	- 기계 번역
	- 텍스트 요약
	- 질문 답변
	- 챗봇
	- 텍스트 생성
	- 기타

- 2가지 구조로 구성되어 있다. 훈련 중에는 다음 작업을 수행한다.
	- `인코더Encoder` : 모델이 소스 시퀀스를 중간 표현으로 바꾼다.
	- `디코더Decoder` : `0`~`i-1`까지의 이전 토큰과 인코딩된 소스 시퀀스를 보고 타깃 시퀀스에 있는 다음 토큰 `i`를 예측하도록 훈련된다.

- 추론 중에는 타깃 시퀀스를 못 쓰니까 처음부터 예측한다. 1번에 1개의 토큰만 생성한다.
	1.  인코더가 소스 시퀀스를 인코딩한다.
	2. 디코더가 인코딩된 소스 시퀀스`[start] 문자열 등`와 초기 시드 토큰을 사용해, 시퀀스의 첫 번째 토근을 예측한다.
	3. 예측된 시퀀스를 디코더에 다시 주입하고, 다음 도큰을 생성하는 식으로 종료 토큰`[end] 문자열 등`이 생성될 때까지 반복한다.

### 기계 번역 예제
- 데이터 받기
```
!wget http://storage.googleapis.com/download.tensorflow.org/data/spa-eng.zip
!unzip -q spa-eng.zip
```

- 데이터 확인하기 : 영어 문장 (탭) 스페인어 문장
```python
text_file = 'spa-eng/spa.txt'
with open(text_file) as f:
	lines = f.read().split("\n")[:-1]
text_pairs = []
for line in lines:
	english, spanish = line.split("\t")
	spanish = '[start] ' + spanish + ' [end]'
	text_pairs.append((english, spanish))

# text_pairs 내용
import random
print(random.choice(text_pairs))
```

- 데이터를 섞은 뒤, 훈련 / 검증 / 테스트로 나누기
```python
import random

random.shuffle(text_pairs)
num_val_samples = int(0.15 * len(text_pairs))
num_train_samples = len(text_pairs) - 2 * num_val_samples
train_pairs = text_pairs[:num_train_samples]
val_pairs = text_pairs[num_train_samples:num_train_samples + num_val_samples]
test_pairs = text_pairs[num_train_samples + num_val_samples : ]
```

- 영어, 스페인어를 위한 2개의 `TextVectorizer` 층을 준비한다.
	- `[start]`와 `[end]`를 추가했기 때문에, 전처리를 조금 다른 방식을 해야 함.
	- 구두점은 언어마다 다르다 : 구두점 문자를 삭제하려면 `?`의 역방향 문자도 삭제해야 한다.
	- 실제 번역 모델에서는 구두점이 들어간 문장을 만들어야 하므로, 구두점 문자를 별개의 토큰으로 다룬다. 여기서는 삭제함.
```python
import tensorflow as tf
import string
import re

strip_chars = string.punctuation + " ¿ "
strip_chars = strip_chars.replace("[", "")
strip_chars = strip_chars.replace("]", "")

def custom_standardization(input_string):
	lower_case = tf.strings.lower(input_string)
	return tf.strings.regex_replace(
	lower_case, f"[{re.escape(strip_chars)}]", "")

voca_size = 15000
sequence_length = 20

# 영어
source_vectorization = layers.TextVectorization(
												max_tokens = voca_size,
												output_mode = 'int',
												output_sequence_length = sequence_length
)

# 스페인어
target_vectorization = layers.TextVectorization(
												max_tokens = voca_size,
												output_mode = 'int',
												output_sequence_length = sequence_length + 1, # 훈련 중 한 스텝 앞선 문장이 필요하기 때문에 토큰 1개 추가
												standardize = custom_standardization
)

train_english_texts = [pair[0] for pair in train_pairs]
train_spanish_texts = [pair[1] for pair in train_pairs]
source_vectorization.adapt(train_english_texts)
target_vectorization.adapt(train_spanish_texts)
```

- 데이터를 `tf.data` 파이프라인으로 변환한다.
	- `(inputs, target)`의 튜플을 반환한다.
	- `inputs` : `encoder_inputs`와 `decoder_inputs` 키 2개를 가진 딕셔너리
	- `targets` : 한 스텝 앞의 스페인어 문장
```python
batch_size = 64

def format_dataset(eng, spa):
	eng = source_vectorization(eng)
	spa = target_vectorization(spa)
	return ({
		'english' : eng,
		'spanish' : spa[:, :, -1] # 입력 스페인어 문장은 마지막 토큰 포함 X라서 입출력 길이 동일
	}, spa[:, 1:]) # 타깃 스페인어 문장

def make_dataset(pairs):
	eng_texts, spa_texts = zip(*pairs)
	eng_texts = list(eng_texts)
	spa_texts = list(spa_texts)
	dataset = tf.data.Dataset.from_tensor_slices((eng_texts, spa_texts))
	dataset = dataset.batch(batch_size)
	dataset = dataset.map(format_dataset, num_parallel_calls = 4)
	return dataset.shuffle(2048).prefetch(16).cache() # 전처리 속도 높이기 위한 캐싱

train_ds = make_dataset(train_pairs)
val_ds = make_dataset(val_pairs)

# 크기 확인
for inputs, targets in train_ds.take(1):
	print(f"inputs['english'].shape : {inputs['english'].shape}")
	print(f"inputs['spanish'].shape : {inputs['spanish'].shape}")
	print(f"targets.shape : {targets.shape}")

# inputs['english'].shape : (64, 20) 
# inputs['spanish'].shape : (64, 20) 
# targets.shape : (64, 20)
```

### RNN 시퀀스투시퀀스 모델 만들기
- 트랜스포머 전, 시퀀스 투 시퀀스는 RNN을 사용했으며, 이는 기계 번역 시스템의 시초임.
- 2017년 경 구글은 7개의 대형 LSTM 층을 쌓은 모델을 사용했다.
- RNN으로 한 시퀀스를 다른 시퀀스로 바꾸는 가장 쉬운 방법은, 각 타임 스텝의 RNN 출력을 유지하는 것이다.
```python
inputs = keras.Input(shape = (sequence_length, ), dtype = 'int64')
x = layers.Embedding(input_dim = voca_size, output_dim = 128)(inputs)
x = layers.LSTM(32, return_sequences = True)(x)
outputs = layers.Dense(voca_size, activation = 'softmax')(x)
model = keras.Model(inputs, outputs)
```
- 그러나 2가지 단점이 있다.
1. `타깃 시퀀스의 길이 = 소스 시퀀스의 길이`여야 한다. 패딩을 추가해서 맞출 수는 있음.
2. RNN의 스텝별 처리 특징으로 인해, 모델이 타깃 시퀀스의 토큰 N을 예측하기 위해 **소스 시퀀스의 토큰 `0 ~ N`만을 참조**한다.
	- 이게 치명적이라서 기계 번역에 RNN이 적합하지 않다. 타깃 시퀀스의 `0 ~ N-1`을 참고할 수 없다는 문제다.

- 다른 방법으로는, `RNN(인코더)`을 사용해서 전체 소스 문장을 1개의 벡터나 벡터 집합으로 바꾼 뒤, 다른 `RNN(디코더)`의 초기 상태로 사용하는 방법이다.
```python
# GRU 기반 모델
from tensorflow import keras
from tensorflow.keras import layers

embed_dim = 256
latent_dim = 1024

# 인코더
source = keras.Input(shape = (None,), dtype = 'int64', name = 'english') # 영어 소스 문장. 이름 지정 시 입력 딕셔너리로 모델을 훈련할 수 있다.
x = layers.Embedding(voca_size, embed_dim, mask_zero = True)(source) # 마스킹 필수!
encoded_source = layers.Bidirectional(layers.GRU(latent_dim), merge_mode = 'sum')(x)

# 디코더
past_target = keras.Input(shape = (None,), dtype = 'int64', name = 'spanish')
x = layers.Embedding(voca_size, embed_dim, mask_zero = True)(past_target)
decoder_gru = layers.GRU(latent_dim ,return_sequences = True)
x = decoder_gru(x, initial_state = encoded_source)
x = layers.Dropout(0.5)(x)
target_next_step = layers.Dense(voca_size, activation = 'softmax')(x) # 다음 토큰 예측
seq2seq_rnn = keras.Model([source, past_target], target_next_step)

# 컴파일 & 훈련
seq2seq_rnn.compile(optimizer = 'rmsprop',
				   loss = 'sparse_categorical_crossentropy',
				   metrics = ['accuracy'])
seq2seq_rnn.fit(train_ds, epochs= 15, validation_data = val_ds)

```
- 훈련 중 디코더는 전체 타깃 시퀀스를 받는다.
- 하지만 RNN의 스텝별 처리 특징 때문에, 입력 토큰 0 ~ N만 사용해서 타깃 토큰 N을 예측한다.

- 위 모델은 64% 정확도를 달성한다 : 즉, 모델은 평균적으로 스페인 문장의 다음 단어를 64% 확률로 예측한다.
	- 그러나 `정확도`는 좋은 척도가 아니다. `N+1`번째 토큰을 예측할 때, `0 ~ N`번째 토큰이 일치하고 있다는 보장이 있는가?
- 실전에서는 `BLEU BiLingual Evaluation Understudy 점수`라는 지표를 쓸 가능성이 높다.
	- 생성된 전체 시컨스를 사용한다.
	- 모델이 생성한 번역과 사람이 생성한 참조 번역의 `n-gram`을 비교하는 식으로 계산된다. 1에 가까울수록 좋음.

- 위 모델로 추론하기
	- 테스트 세트에서 몇 문장을 선택 -> 어떻게 번역하는지 살펴본다.
	- `[start]`와 인코딩된 소스 문장을 디코더에 주입하고, 다음 토큰을 예측하고, 디코더에 반복적으로 다시 주입하는 방식이다. `[end]` 토큰이나 최대 문장길이가 될 떄까지 타깃 토큰을 생성한다.
```python
import numpy as np

# 예측 인덱스 문자열 토큰으로 변환하는 딕셔너리 준비
spa_voca = target_vectorization.get_vocabulary()
spa_index_lookup = dict(zip(range(len(spa_voca)), spa_voca))
max_decoded_sentence_length = 20

def decode_sequence(input_sentence):
	tokenized_input_setence = source_vectorization([input_sentence])
	decoded_sentence = "[start]"
	for i in range(max_decoded_sentence_length):
		tokenized_target_sentence = target_vectorization([decoded_sentence])
		
		# 다음 토큰 샘플링
		next_token_predictions = seq2seq_rnn.predict(
		[tokenized_input_sentence, tokenized_target_sentence])
		sampled_token_index = np.argmax(next_token_predictions[0, i, :])
		
		# 다음 토큰 -> 문자열로 변경 & 문장에 추가
		sampled_token = spa_index_lookup(sampled_token_index)
		decoded_sentence += " " + sampled_token
		if sampled_token == "[end]":
			break
	return decoded_sentence

test_eng_texts = [pair[0] for pair in test_pairs]
for _ in range(20):
	input_sentence = random.choice(test_eng_texts)
	print("-")
	print(input_sentence)
	print(decode_sequence(input_sentence))
```
- 이런 추론은 간단하지만 효율적이진 않다. 
- 전체 소스 시퀀스와 지금까지 생성된 전체 타깃 시퀀스를 새로운 단어를 샘플링할 때마다 모두 다시 처리해야 한다.
- **실전 앱에서는 인코더와 디코더를 별개의 모델로 나누고, 토큰 샘플링 반복마다 이전 내부 상태를 재사용해서 디코더가 1스텝만 진행된다.**

> 위 예제 개선 방법  
> 1. 순환층 여러 개 쌓기(인코더, 디코더. 디코더는 관리가 복잡해질 수 있다.)
> 2. GRU 대신 LSTM 쓰기

- 그러나 근본적인 제약이 있다.
	1. 소스 시퀀스가 인코더 상태 벡터나 벡터 집합으로 완전하게 표현되어야 한다. **번역할 수 있는 문장의 크기, 복잡도에 큰 제약이 된다.** 
		- 사람으로 치면 소스 시퀀스를 1번만 보고 완전히 기억만으로 문장을 번역하는 것과 동일
	2. **RNN은 오래된 과거를 잊어버려서, 매우 긴 문장 처리에는 문제가 있다.** 어떤 시퀀스든 100번째 토큰에 도달하면 시작 정보는 거의 없다.

### 트랜스포머 시퀀스 투 시퀀스 모델
사람의 번역을 생각해보면, 단어 단위로 문장을 읽어 의미를 기록한 다음 단어 단위로 번역된 문장을 생성하지는 않는다. 대신 **소스 문장 <-> 진행 중인 번역 사이를 오갈 것**이다. 
- 위 과정을 뉴럴 어텐션과 트랜스포머로 구현할 수 있다.
	- `트랜스포머 인코더`는 셀프 어텐션을 사용해서 입력 시퀀스의 토큰에 대해 문맥을 고려한 표현을 만든다고 했다.
		- RNN 인코더와 달리, 트랜스포머 인코더는 시퀀스 형태로 인코딩된 형태를 유지한다.
	- `트랜스포머 디코더`가 나머지 절반을 차지한다. 타깃 시퀀스의 `0~N`을 읽고 `N+1`을 예측한다.
		- 중요한 건, 뉴럴 어텐션을 사용해서 인코딩된 소스 문장에서, **어떤 토큰이 현재 예측하려는 타깃 토큰과 가장 관련이 높은지를 식별한다**는 것이다.

- `쿼리-키-값` 모델에서, 트랜스포머 디코더의 타깃 시퀀스는 소스 시퀀스에 있는 다른 부분에 더 주의를 기울이려 사용하는 `어텐션 쿼리` 역할을 한다. (소스 시퀀스는 키, 값 역할)

#### 트랜스포머 디코더
```python
class TransformerDecoder(layers.Layer):
	def __init__(self, embed_dim, dense_dim, num_heads, **kwargs):
		super().__init__(**kwargs)
		self.embed_dim = embed_dim
		self.dense_dim = dense_dim
		self.num_heads = num_heads
		self.attention_1 = layers.MultiHeadAttention(num_heads = num_heads,
		key_dim = embed_dim)
		self.attention_2 = layers.MultiHeadAttention(num_heads = num_heads,
		key_dim = embed_dim)
		self.attention_3 = layers.MultiHeadAttention(num_heads = num_heads,
		key_dim = embed_dim)
		self.dense_proj = keras.Sequential([layers.Dense(dense_dim, activation = 'relu'),
		layers.Dense(embed_dim),])
		self.layer_norm_1 = layers.LayerNormalization()
		self.layer_norm_2 = layers.LayerNormalization()
		self.layer_norm_3 = layers.LayerNormalization()
		self.supports_masking = True # 입력 마스킹을 출력으로 전달하게 함
		# layer.compute_mask()는 위 속성이 False이면 에러를 반환한다.
	
	def get_config(self):
		config = super().get_config()
		config.update({
		'embed_dim' : self.embed_dim,
		'num_heads' : self.num_heads,
		'dense_dim' : self.dense_dim})
		return config
```
![[20230725_190356.jpg]]
- `call` 메서드는 위 구조를 거의 구대로 구현한다.
- 추가적으로 `코잘 패딩Casual Padding`을 고려해야 한다.  
	- 미래 타임스텝의 데이터를 활용하지 못하도록 0으로 만드는 것을 의미함
	- **RNN은 한번에 한 스텝의 입력을 보는 반면, `TransformerDecoder`는 순서에 구애받지 않고 한번에 타깃 시퀀스 전체를 볼 수 있다.**
		- 전체 입력을 사용하도록 둔다면 N+1의 입력 스텝을 출력 위치 N에 복사하는 방법을 학습할 것이다.
		- 훈련 정확도가 정확하지만, 추론 시에는 N 이상의 입력 스텝이 있을 수 없기 때문에 쓸모없다.

- 코잘 패딩을 적용하는 방법은, 어텐션 행렬의 위쪽 절반을 마스킹하는 것이다.
- 이를 위해 클래스에 `get_casual_mask(self, inputs)` 메서드를 추가해 `MultiHeadAttention` 층에 전달하는 어텐션 행렬을 만든다.

- 전체 클래스 구성
	- `get_casual_attention_mask`와 `call`이 추가되었다.
```python
class TransformerDecoder(layers.Layer):
	def __init__(self, embed_dim, dense_dim, num_heads, **kwargs):
		super().__init__(**kwargs)
		self.embed_dim = embed_dim
		self.dense_dim = dense_dim
		self.num_heads = num_heads
		self.attention_1 = layers.MultiHeadAttention(num_heads = num_heads,
		key_dim = embed_dim)
		self.attention_2 = layers.MultiHeadAttention(num_heads = num_heads,
		key_dim = embed_dim)
		self.attention_3 = layers.MultiHeadAttention(num_heads = num_heads,
		key_dim = embed_dim)
		self.dense_proj = keras.Sequential([layers.Dense(dense_dim, activation = 'relu'),
		layers.Dense(embed_dim),])
		self.layer_norm_1 = layers.LayerNormalization()
		self.layer_norm_2 = layers.LayerNormalization()
		self.layer_norm_3 = layers.LayerNormalization()
		self.supports_masking = True # 입력 마스킹을 출력으로 전달하게 함
		# layer.compute_mask()는 위 속성이 False이면 에러를 반환한다.
	def get_config(self):
		config = super().get_config()
		config.update({
		'embed_dim' : self.embed_dim,
		'num_heads' : self.num_heads,
		'dense_dim' : self.dense_dim
		  })
		return config
	
	# 코잘 마스킹 : 미래 타임스텝의 데이터를 사용하지 못하게 한다
	def get_casual_attention_mask(self, inputs):
		input_shape= tf.shape(inputs)
		batch_size, sequence_length = input_shape[0], input_shape[1]
		i = tf.range(sequence_length)[:, tf.newaxis]
		j = tf.range(sequence_length)
		mask = tf.cast(i >= j, dtype = 'int32') # 
		mask = tf.reshape(mask, (1, input_shape[1], input_shape[1]))
		
		mult = tf.concat([tf.expand_dims(batch_size, -1), 
						  tf.constant([1, 1], dtype = tf.int32)], axis = 0)
		return tf.tile(mask, mult)
		
	def call(self, inputs, encoder_outputs, mask = None):
		casual_mask = self.get_casual_attention_mask(inputs)
		
		if mask is not None:
		  padding_mask = tf.cast(
			  mask[:, tf.newaxis, :], dtype = 'int32'
		  )
			padding_mask = tf.minimum(padding_mask, casual_mask)
		
		attention_output_1 = self.attention_1(
			query = inputs,
			value = inputs,
			key = inputs,
			attention_mask = casual_mask
		)
		attention_output_1 = self.layernorm_1(inputs + attention_output_1)
		
		attention_output_2 = self.attention_2(
			query = attention_output_1,
			value = encoder_outputs,
			key = encoder_outputs,
			attention_mask = padding_mask, # 합친 마스킹을 소스 + 타깃 시퀀스를 연결시키는 2번째 어텐션 층에 전달
		)
		attention_output_2 = self.layernorm_2(
			attention_output_1 + attention_output_2
		)
		
		proj_output = self.dense_proj(attention_output_2)
		return self.layernorm_3(attention_output_2 + proj_output)
```

- 엔드 투 엔드 트랜스포머
	-  소스 시퀀스와 타깃 시퀀스를 한 스텝 앞의 타깃 시퀀스에 매핑
	- `PositionalEmbedding`, `TransformerEncoder`, `TransformerDecoder` 층을 합치면 된다.

```python
# 엔드 투 엔드 트랜스포머
embed_dim = 256
dense_dim = 2048
num_heads = 8

encoder_inputs = keras.Input(shape = (None,), dtype = 'int64', name = 'english')
x = PositionalEmbedding(sequence_length, voca_size, embed_dim)(encoder_inputs)
encoder_outputs = TransformerEncoder(embed_dim, dense_dim, num_heads)(x)

decoder_inputs = keras.Input(shape = (None, ), dtype = 'int64', name = 'spanish')
x = PositionalEmbedding(sequence_length, voca_size, embed_dim)(decoder_inputs)
x = TransformerDecoder(ebmed_dim, dense_dim, num_heads)(x, encoder_outputs)
x = layers.Dropout(0.5)(x)
decoder_outputs = layers.Dense(vocas_size, activation = 'softmax')(x)

transformer = keras.Model([encoder_inputs, decoder_inputs], decoder_outputs)

transformer.compile(
					 optimizer = 'rmsprop',
					 loss = 'sparse_categorical_crossentropy',
					 metrics = ['accuracy']
)
transformer.fit(train_ds, epochs = 30, validation_data = val_ds)
```

- 모델로 새로운 문장 번역하기
```python
import numpy as np

spa_voca = target_vectorization.get_vocabulary()
spa_index_lookup = dict(zip(range(len(spa_voca)), spa_voca))
max_decoded_sentence_length = 20

def decode_sequence(input_sentence):
	tokenized_input_sentence = source_vectorization([input_sentence])
	decoded_sentence = "[start]"
	for i in range(max_decoded_sentence_length):
		tokenized_target_sentence = target_vectorization(
		[decoded_sentence])[:, :-1]
		predictions = transformer([tokenized_input_sentence, tokenized_target_sentence])
		sampled_token_index = np.argmax(predictions[0, i, :])
		
		# 다음 토큰 예측 문자열로 바꾸고 생성된 문장에 추가
		sampled_token = spa_index_lookup[sampled_token_index] 
		decoded_sentence += " " + sampled_token
		if sampled_token == "[end]":
			break
	return decoded_sentence

test_eng_texts = [pair[0] for pair in test_pairs]
for _ in range(20):
	input_sentence = random.choice(test_eng_Texts)
	print("-")
	print(input_sentence)
	print(decode_sequence(input_sentence))
```