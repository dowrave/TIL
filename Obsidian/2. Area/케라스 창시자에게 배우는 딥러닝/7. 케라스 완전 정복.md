1. [[#다양한 워크플로우|다양한 워크플로우]]
2. [[#케라스 모델을 만드는 여러 방법|케라스 모델을 만드는 여러 방법]]
	1. [[#케라스 모델을 만드는 여러 방법#Sequential|Sequential]]
	1. [[#케라스 모델을 만드는 여러 방법#함수형 API|함수형 API]]
		1. [[#함수형 API#다중 입력, 다중 출력 모델|다중 입력, 다중 출력 모델]]
		2. [[#함수형 API#함수형 API의 장점 : 층 연결 구조 활용하기|함수형 API의 장점 : 층 연결 구조 활용하기]]
	2. [[#케라스 모델을 만드는 여러 방법#Model 서브클래싱|Model 서브클래싱]]
		1. [[#Model 서브클래싱#주의사항 : 서브클래싱 모델이 지원하지 않는 것|주의사항 : 서브클래싱 모델이 지원하지 않는 것]]
	3. [[#케라스 모델을 만드는 여러 방법#여러 방식 혼합해서 사용하기|여러 방식 혼합해서 사용하기]]
	4. [[#케라스 모델을 만드는 여러 방법#작업에 가장 적합한 도구|작업에 가장 적합한 도구]]
3. [[#내장 훈련 루프와 평가 루프 사용하기|내장 훈련 루프와 평가 루프 사용하기]]
	1. [[#내장 훈련 루프와 평가 루프 사용하기#사용자 정의 지표 만들기|사용자 정의 지표 만들기]]
	2. [[#내장 훈련 루프와 평가 루프 사용하기#콜백 사용하기|콜백 사용하기]]
	3. [[#내장 훈련 루프와 평가 루프 사용하기#사용자 정의 콜백 만들기|사용자 정의 콜백 만들기]]
	4. [[#내장 훈련 루프와 평가 루프 사용하기#텐서보드 모니터링과 시각화|텐서보드 모니터링과 시각화]]
4. [[#사용자 정의 훈련, 평가 루프 만들기|사용자 정의 훈련, 평가 루프 만들기]]
	1. [[#사용자 정의 훈련, 평가 루프 만들기#훈련 vs 추론|훈련 vs 추론]]
	2. [[#사용자 정의 훈련, 평가 루프 만들기#완전한 훈련과 평가 루프|완전한 훈련과 평가 루프]]
		1. [[#완전한 훈련과 평가 루프#tf.function 으로 성능 높이기|tf.function 으로 성능 높이기]]
		2. [[#완전한 훈련과 평가 루프#fit() 메서드를 사용자 정의 루프로 활용하기|fit() 메서드를 사용자 정의 루프로 활용하기]]


- [실습 코드 : 여기의 7장](https://github.com/dowrave/TIL/tree/main/NotObsidian/%EC%BC%80%EB%9D%BC%EC%8A%A4%20%EC%B0%BD%EC%8B%9C%EC%9E%90%EC%97%90%EA%B2%8C%20%EB%B0%B0%EC%9A%B0%EB%8A%94%20%EB%94%A5%EB%9F%AC%EB%8B%9D)
- 컴퓨터 비전, 시계열 예측, 자연어 처리, 생성 딥러닝 등을 알기 위해선, `Sequential` 모델과 기본 `fit()` 루프 이상을 알아야 한다.

## 다양한 워크플로우
- 케라스 API는 복잡성의 `단계적 공개Progressive Disclosure` 원칙을 따른다.
	- 시작은 간단하게, 필요할 때마다 학습해서 복잡한 것을 처리

## 케라스 모델을 만드는 여러 방법

### Sequential
- 케라스 모델을 만드는 가장 간단한 방법

```python
model = keras.Sequential([
	layers.Dense(64, activation = 'relu'),
	layers.Dense(10, activation = 'softmax')
])
```

- `add` 메서드를 이용해 점진적으로 만들 수도 있다.
```python
model = keras.Sequential() 
model.add(layers.Dense(64, activation = 'relu'))
model.add(layers.Dense(10, activation = 'softmax'))
```

- 위 모델은 아직 가중치가 없으며, **가중치를 지정**하려면 데이터로 호출하거나 입력 크기를 지정하여 **`build()` 메서드를 호출**해야 한다.
```python
model.build(input_shape = (None, 3))

# 모델 구조 출력 : build 이전에는 출력 X
model.summary()
```
- 크기 `3`인 샘플을 기대하며, `None`은 어떠한 배치 크기도 가능하다는 의미이다

- 케라스의 `Sequential`이나 `layers`에는 `name` 파라미터를 지정할 수 있다.

- `build()` 없이 가중치를 지정하려면, `input` 클래스를 사용할 수 있다
	- `keras.Input(shape = (3, ))`으로 구현할 수도 있고
	- `model.add(layers.Dense(..., input_shape(3, )))`으로 구현할 수도 있다.

### 함수형 API
- `Sequential` 모델은 적용할 수 있는 곳이 제한적이다.
	- 1개의 인풋, 1개의 아웃풋만 가지며 순서대로 쌓은 경우만 표현할 수 있다.

- 실제 문제에서는 다중 입력(이미지, 이미지의 메타데이터)이나 다중 출력(여러 항목을 예측), 비선형적 구조 등이 있을 수 있다.
- 이럴 때 함수형 API를 사용할 수 있다.

```python
inputs = keras.Input(shape = (3, ), name = "my_input") #(None, 3), dtype = float32
features = layers.Dense(64, activation='relu')(inputs) #(None, 64)
outputs = layers.Dense(10, activation = 'softmax')(features)
model = keras.Model(inputs = inputs, outputs = outputs)
```
- `inputs` 클래스 객체 정의로 시작한다. 
	- 모델이 처리할 데이터 크기, `dtype` 정보가 있다.
	- 이러한 객체를 `심볼릭 텐서Symbolic Tensor`라고 한다. 실제 데이터가 없지만 사용할 때 모델이 보게 될 데이터 텐서의 사양이 인코딩되어 있다.

- 이전 층을 다음 층의 인풋으로 받는 방식으로 구현한다.
- 마지막에 모델 전체를 정의할 때는 모델 전체의 인풋과 아웃풋을 `Model`에 전달하면 끝.

#### 다중 입력, 다중 출력 모델
- 대부분의 딥러닝 모델은 리스트가 아니라 그래프 형태이다. 
	- 다중 입력 or 다중 출력

> 예제 : 고객 이슈 티켓에 우선 순위를 지정하고 적절한 부서로 전달하는 시스템 만들기
> 입력 : 제목(텍스트), 본문(텍스트), 사용자 추가 태그(범주 : 원-핫 인코딩)  
> 출력 : 티켓의 우선순위 점수(0~1 사이의 시그모이드), 이슈 처리 부서

```python
voca_size = 10000
num_tags = 100
num_departments = 4

title = keras.Input(shape = (voca_size,), name = 'title')
text_body = keras.Input(shape = (voca_size,), name = 'text_body')
tags = keras.Input(shape = (num_tags, ), name = 'tags')

features = layers.Concatenate()([title, text_body, tags]) # 입력 특성을 1개의 텐서 feature로 연결
features = layers.Dense(64, activation = 'relu')(features)

priority = layers.Dense(1, activation = 'relu', name = 'priority')(features)
department = layers.Dense(num_departments, activation = 'softmax', name = 'department')(features)

model = keras.Model(inputs = [title, text_body, tags], outputs = [priority, department])
```

```python
# 다중 입출력 모델 훈련
import numpy as np

num_samples = 1280

# 더미 입력 데이터

title_data = np.random.randint(0, 2, size = (num_samples, voca_size))
text_body_data = np.random.randint(0, 2, size = (num_samples, voca_size))
tags_data = np.random.randint(0, 2, size = (num_samples, num_tags))

  

# 더미 타깃 데이터

priority_data = np.random.random(size = (num_samples, 1))
department_data = np.random.randint(0, 2, size = (num_samples, num_departments))

model.compile(optimizer = 'rmsprop',
              loss = ['mean_squared_error', 'categorical_crossentropy'],
              metrics = [['mean_absolute_error'], ['accuracy']])

model.fit([title_data, text_body_data, tags_data],
          [priority_data, department_data],
          epochs = 1)

model.evaluate([title_data, text_body_data, tags_data],
               [priority_data, department_data])

priority_preds, department_preds = model.predict([title_data, text_body_data, tags_data])
```

- 순서에 신경쓰고 싶지 않다면(입출력이 많은 경우) `Input` 객체와 출력 층에 부여한 **이름을 활용,** **데이터를 딕셔너리에 전달할 수 있다.**

```python
model.compile(optimizer = 'rmsprop',
			 loss = {'priority' : 'mean_squared_error', 'department' : 'categorical_crossentropy'},
			 metrics = {"priority" : ["mean_absolute_error"], "department" : ['accuracy']})

model.fit({'title' : title_data, 'text_body' : text_body_data, 'tags' : tags_data},
		 {'priority' : priority_data, 'department' : department_data},
		 epochs = 1)

model.evaluate({'title' : title_data, 'text_body' : text_body_data, 'tags' : tags_data},
		 {'priority' : priority_data, 'department' : department_data})

priority_preds, department_preds = model.predict({'title' : title_data, 'text_body' : text_body_data, 'tags' : tags_data})
```

#### 함수형 API의 장점 : 층 연결 구조 활용하기
- `모델 시각화`와` 특성 추출`이라는 중요한 2가지 기능이 가능하다.

- 모델의 토폴로지 시각화
```python
keras.utils.plot_model(model, 'ticket_classifier.png', show_shapes = True)
```

![[Pasted image 20230722165023.png]]
-  `None`은 배치 크기를 의미한다 : 어떠한 배치 크기에도 이 모델은 사용 가능하다.

```python
model.layers # 모든 층의 리스트
model.layers[3].input
model.layers[3].output
```

- `특성 추출Feature Extraction` : 다른 모델에서 중간 특성을 재사용할 수 있다.
```python
features = model.layers[4].output # 중간 Dense층
difficulty = layers.Dense(3, activation = 'softmax', name = 'difficulty')(features)
new_model = keras.Model(
      inputs = [title, text_body, tags],
      outputs = [priority, department, difficulty]
)

keras.utils.plot_model(new_model, 'ticket_classifier.png', show_shapes = True)
```
![[Pasted image 20230722165338.png]]

### Model 서브클래싱
- 가장 고급 방법

> 1. `__init__()` 메서드로 모델이 사용할 층을 정의한다.
> 2. `call()` 메서드에서 앞서 만든 층을 사용해 모델의 정방향 패스를 정의한다.
> 3. 서브클래스의 객체를 만들고, 데이터와 호출하여 가중치를 만든다.

- 위 모델 다시 서브클래싱으로 만들기
 ```python
 class CustomerTicketModel(keras.Model):
  def __init__(self, num_departments):
    super().__init__() # 필수! 부모 클래스의 생성자 호출
    
    # 층은 생성자에서 정의된다
    self.concat_layer = layers.Concatenate()
    self.mixing_layer = layers.Dense(64, activation = 'relu')
    self.priority_scorer = layers.Dense(1, activation = 'sigmoid')
    self.department_classifier = layers.Dense(num_departments, activation = 'softmax')

  # 정방향 패스 정의

  def call(self, inputs):
    title = inputs['title']
    text_body = inputs['text_body']
    tags = inputs['tags']
    features = self.concat_layer([title, text_body, tags])
    features = self.mixing_layer(features)
    priority = self.priority_scorer(features)
    department = self.department_classifier(features)
    return priority, department

model = CustomerTicketModel(num_departments = 4)
priority, department = model({
    'title' : title_data,
    'text_body' : text_body_data,
    'tags' : tags_data
})

priority_preds, department_preds = model.predict({"title" : title_data,
           "text_body" : text_body_data,
           "tags" : tags_data})
```

-  3장의 `Layers` 클래스와 매우 유사함
- `Model` 클래스는 `fit(), predict(), evaluate()` 메서드를 가졌다는 것만 차이가 있다.

#### 주의사항 : 서브클래싱 모델이 지원하지 않는 것
- `함수형`과 크게 차이가 있다.
- 함수형은 명시적인 데이터 구조인 층의 그래프이다. 출력, 조사, 수정이 가능하다.
- 서브클래싱 모델은 파이썬 클래스라서 제약 사항이 생긴다.
	- 층의 연결 방식이 `call()` 메서드 내에 감춰져서 이 정보를 활용할 수 없다.
	- `summary()` 메서드가 층의 연결 구조를 출력할 수없다.
	- `plot_model()`로 모델 구조를 그래프로 그릴 수 없다.
	- 객체 생성 후 정방향 패스는 완전한 블랙박스가 된다.

### 여러 방식 혼합해서 사용하기
- 어떤 한 방법을 택한다고 해서 다른 패턴의 사용이 제한되지는 않는다. 
- 서브클래스로 모델을 만드는 데 그 안에 함수형으로 모델을 넣어도 무방함. 반대도 가능.

### 작업에 가장 적합한 도구
- `함수형 API`가 사용성과 유연성 사이의 가장 적절한 절충점이다.
- 책의 예제는 함수형 API를 쓰며, 서브클래싱 층도 중간중간에 넣는다. 
- 서브클래싱 층을 넣은 함수형 api는 함수형 api의 장점을 유지하면서 높은 개발 유연성을 제공할 수 있다.

## 내장 훈련 루프와 평가 루프 사용하기
```python
from tensorflow.keras.datasets import mnist

def get_mnist_model():
  inputs = keras.Input(shape = (28 * 28),)
  features = layers.Dense(512, activation=  'relu')(inputs)
  features = layers.Dropout(0.5)(features)
  outputs = layers.Dense(10, activation = 'softmax')(features)
  model = keras.Model(inputs, outputs)
  return model

(images, labels), (test_images, test_labels) = mnist.load_data()
images = images.reshape((60000, 28 * 28)).astype("float32") / 255
test_images = test_images.reshape((10000, 28 * 28)).astype("float32") / 255
train_images, val_images = images[10000:], images[:10000]
train_labels, val_labels = labels[10000:], labels[:10000]

model = get_mnist_model()
model.compile(optimizer = 'rmsprop',
              loss = 'sparse_categorical_crossentropy',
              metrics = ['accuracy'])
model.fit(train_images, train_labels, epochs = 3, validation_data = (val_images, val_labels))

test_metrics = model.evaluate(test_images, test_labels)
predictions = model.predict(test_images)
```

- 워크플로우 커스터마이징 하기
	- 사용자 정의 측정 지표 전달
	- `fit()` 메서드에 콜백을 전달해 특정 시점에 수행되는 행동 예약

### 사용자 정의 지표 만들기
- 평균 제곱근 오차 계산하는 사용자 정의 지표
```python
class RootMeanSqauredError(keras.metrics.Metric):
  def __init__(self, name = 'rmse', **kwargs):
    super().__init__(name = name, **kwargs)
    self.mse_sum = self.add_weight(name = 'mse_sum', initializer = 'zeros')
    self.total_samples = self.add_weight(name = 'total_samples', initializer = 'zeros', dtype = 'int32')

  def update_state(self, y_true, y_pred, sample_weight = None):
    y_true = tf.one_hot(y_true, depth = tf.shape(y_pred)[1]) # 정수 레이블 0~9 원핫인코딩
    mse =tf.reduce_sum(tf.square(y_true - y_pred))
    self.mse_sum.assign_add(mse)
    num_samples = tf.shape(y_pred)[0]
    self.total_samples.assign_add(num_samples)

  # 현재 지표값 반환
  def result(self):
    return tf.sqrt(self.mse_sum / tf.cast(self.total_samples, tf.float32))

  # 객체 재생성하지 않고 초기화
  def reset_state(self):
    self.mse_sum.assign(0.)
    self.total_samples.assign(0)

# metrics 지표에 추가만 해주면 됨 (전체 실행은 위에 있음)
model.compile(optimizer = 'rmsprop',
              loss = 'sparse_categorical_crossentropy',
              metrics = ['accuracy', RootMeanSqauredError()])
```

```
loss: 0.0998 - accuracy: 0.9721 - rmse: 7.4349
```

### 콜백 사용하기
- 케라스 콜백은 `model.fit()` 호출을 스스로 판단하고 동적으로 결정하게 만든다.
	- 모델 체크포인트 저장
	- 조기 종료
	- 훈련 중 하이퍼파라미터 값 조정
	- 훈련, 검증 지표를 기록하거나 학습한 표현이 업데이트될 때마다 시각화

> 예시
> keras.callbacks.ModelCheckpoint
> keras.callbacks.EarlyStopping
> keras.callbacks.LearningRateScheduler
> keras.callbacks.ReduceLROnPlateau
> keras.callbacks.CSVLogger


- 콜백은 이렇게 사용함
```python
callbacks_list[
			   keras.callbacks.EarlyStopping(monitor = 'val_accuracy',
											patience = 2) # 2번 동안 향상 X 시 중지
				# 에포크 끝에서 현재 가중치 저장 : 값이 더 좋아질 때만 저장한다
			   keras.callbacks.ModelCheckpoint(filepath = 'checkpoint_path.keras',
											   monitor = 'val_loss',
											   save_best_only = True)
]


# 전달
model.fit(...,
		 callbacks = callbacks_list)


# 모델 로드하기
model = keras.models.load_model("checkpoint_path.keras")
```


### 사용자 정의 콜백 만들기

> `on_epoch_begin(epoch, logs)` : 에포크 시작 시 호출
> `on_epoch_end(epoch, logs)` : 에포크 종료 시 호출
> `on_batch_begin(epoch, logs)` : 배치 처리 시작 전에 호출
> `on_batch_end(epoch, logs)` : 배치 처리 후에 호출
> `on_train_begin(logs)` : 훈련 시작 시 호출
> `on_train_end(logs)` : 훈련 종료 시 호출

```python
# 사용자 정의 콜백

class LossHistory(keras.callbacks.Callback):
  def on_train_begin(self, logs):
    self.per_batch_losses  = []
  def on_batch_end(self, batch, logs):
    self.per_batch_losses.append(logs.get("loss"))
  def on_epoch_end(self, epoch, logs):
    plt.clf()
    plt.plot(range(len(self.per_batch_losses)), self.per_batch_losses,
             label = "Training Loss For Each Batch")
    plt.xlabel(f"Batch epoch {epoch}")
    plt.ylabel(f"Loss")
    plt.legend()
    plt.savefig(f"plot_at_epoch_{epoch}")
    self.per_batch_losses = []

model = get_mnist_model()

model.compile(optimizer = 'rmsprop', loss = 'sparse_categorical_crossentropy', metrics = ['accuracy'])

model.fit(train_images,  train_labels, epochs = 10, callbacks = [LossHistory()], validation_data = (val_images, val_labels))
```
![[Pasted image 20230722185712.png]]


### 텐서보드 모니터링과 시각화
- 좋은 연구나 좋은 모델을 개발하려면 실험하는 모델 내부에서 일어나는 일의 피드백이 필요하다.
- 아이디어 발생 -> 검증할 실험 계획 -> 정보 가공 -> 다음 아이디어 영감 -> ...

- 실험 결과의 처리를 도와주는 게 `텐서보드`다
	- 측정 지표 모니터링
	- 모델 구조 시각화
	- 활성화 출력, 그래디언트의 히스토그램
	- 임베딩 3D 표현

```python
tensorboard = keras.callbacks.TensorBoard(log_dir = '/full_path_to_your_log_dir')
model.fit(train_images,  
		  train_labels, 
		  epochs = 10, 
		  callbacks = [LossHistory()], 
		  validation_data = (val_images, val_labels))
```

```python
# 로컬 (없다면 pip install tensorboard) 이후 실행
tensorboard --logdir /full_path_to_your_log_dir

# 코랩 노트북
%load_ext tensorboard
%tensorboard --logdir /full_path_to_your_log_dir
```


## 사용자 정의 훈련, 평가 루프 만들기
- 대부분 `fit()` 함수를 쓰겠지만, 모든 것을 지원하지는 못한다.
- `fit()`은 지도학습에만 초점이 맞춰져 있다.
	- 생성 학습
	- 자기 지도 학습 - 타깃을 입력에서 얻음
	- 강화 학습
- 등 명시적인 타깃이 없는 경우가 있음.
- 이러한 경우 자신만의 훈련 로직을 직접 작성해야 한다.

### 훈련 vs 추론
- `Dropout()` 층 등 **일부 층은 훈련과 추론에서 동작이 다르다.**
	- 이러한 층은 `call()` 메서드에 `training` Bool 매개변수를 제공한다.
	- 정방향 패스는 `predictions = model(inputs, training = True)`가 된다.

- 모델 가중치 그래디언트는 `tape.gradients(loss, model.trainable_weights)`을 사용해야 한다.
	- 훈련 가능 가중치 : `Dense`의 커널, 바이어스 등 역전파로 업데이트 되는 애들
	- 훈련 불가능 가중치 : 정방향 패스 동안 업데이트됨
		- 예시) 배치를 세는 카운트 수
```python
def train_step(inputs, targets):
  with tf.GradientTape() as tape:
    predictions = model(inputs, training = True)
    loss = loss_fn(targets, predictions)
  gradients = tape.gradients(loss, model.trainable_weights)
  optimizer.apply_gradients(zip(model.trainable_weights, gradients))
```

- 케라스 내장 가중치 중, 훈련 불가 가중치를 가진 층은 `BatchNormalization` 층 밖에 없다. 

### 완전한 훈련과 평가 루프
```python
model = get_mnist_model()

loss_fn = keras.losses.SparesCategoricalCrossentropy()
optimizer = keras.optimizers.RMSprop()
metrics = [keras.metrics.SparseCategoricalAccuracy()]
loss_tracking_metric = keras.metrics.Mean()

  

# 정방향 패스, 역방향 패스, 지표 추적을 fit()과 유사한 훈련 스텝 함수로 연결
def train_step(inputs, targets):

  with tf.GradientTape() as tape:
    predictions = model(inputs, training = True)
    loss = loss_fn(targets, predictions)
  gradients = tape.gradients(loss, model.trainable_weights)
  optimizer.apply_gradients(zip(model.trainable_weights, gradients))
  logs = {}

  for metric in metrics:
    metric.update_state(targets, predictions)
    logs[metric.name] = metric.result()

  loss_tracking_metric.update_state(loss)
  logs['loss'] = loss_tracking_metric.result()
  return logs

  

# 매 에포크 시작과 평가 전에 지표 상태 재설정
def reset_metrics():
  for metric in metrics:
    metric.reset_state()
  loss_tracking_metric.reset_state()

# 훈련 루프 자체
training_dataset = tf.data.Dataset.from_tensor_slices((train_images, train_labels))
training_dataset = training_dataset.batch(32)
epochs = 3
for epoch in range(epochs):
  reset_metrics()
  for inputs_batch, targets_batch in training_dataset:
    logs = train_step(inputs_batch, targets_batch)
  print(f"{epoch}번째 에포크 결과")
  for key, value in logs.items():
    print(f"...{key}: {value:.4f}")


# 테스트 스텝
def test_step(inputs, targets):
  predictions = model(inputs, training = False) # 테스트니까 False로 전달
  loss = loss_fn(targets, predictions)
  logs = {}
  for metric in metrics:
    metric.update_state(targets, predictions)
    logs["val_" + metric.name] = metric.result()
  loss_tracking_metric.update_state(loss)
  logs['val_loss'] = loss_tracking_metric.result()
  return logs

  
val_dataset = tf.data.Dataset.from_tensor_slices((val_images, val_labels))
val_dataset = val_dataset.batch(32)
reset_metrics()
for inputs_batch, targets_batch in val_dataset:
  logs= test_step(inputs_batch, targets_batch)
print("평가 결과 : ")
for key, value in logs.items():
  print(f"...{key}: {value:.4f}")
```
- 이렇게 구현하면 `fit()`이랑 `evaluate()`을 구현한 게 된다.


#### tf.function 으로 성능 높이기
- 텐서플로우 코드는 `즉시 실행eager execution`이다. 이는 성능 측면에선 최적이 아니다.
- 텐서플로우 코드는 `계산 그래프Computation Graph`로 컴파일하는 것이 더 성능이 좋다.
	- 전역적인 최적화가 가능하기 때문이다.

```python
@tf.function # 함수 위에 이 데코레이터만 추가하면 된다.
def test_step(inputs, targets):
	# ...
```

- 디버깅할 때는 즉시 실행을 활용하고, 성능을 높이고 싶을 때는 `@tf.function`을 달아주면 된다.

#### fit() 메서드를 사용자 정의 루프로 활용하기
- `fit()`도 써보고, 밑바닥부터 구현도 해봤다.
- 그 중간 지점이 있는데, 사용자 정의 훈련 스텝 함수를 제공하고 나머지 처리는 프레임워크에 위임할 수 있다.

> 1. `keras.Model`을 상속한 클래스를 만든다.
> 2. `train_step(self, data)` 메서드를 오버라이드 한다.
> 3. `Metrics` 객체들을 반환하는 `metrics` 속성을 구현한다.

```python
loss_fn = keras.losses.SparseCategoricalCrossentropy()
loss_tracker = keras.metrics.Mean(name = 'loss')

class CustomModel(keras.Model):
	def train_step(self, data): # 메서드 오버라이드
		inputs, targets = data
		with tf.GradientTape() as tape:
			predictions = self(inputs, training = True) # 모델 = 클래스이므로 self 사용
		    loss = loss_fn(targets, predictions)
		gradients = tape.gradient(loss, self.trainable_weights)
		self.optimizer.apply_gradients(zip(gradients, self.trainable_weights))
		loss_tracker.update_state(loss) # 손실 평균 추적 업데이트
		return {"loss" : loss_tracker.result()}
	
	@property
	def metrics(self): # 에포크마다 재설정할 지표 나열함
		return [loss_tracker] 

inputs = keras.Input(shape = (28 * 28))
features = layers.Dense(512, activation = 'relu')(inputs)
features = layers.Dropout(0.5)(features)
outputs = layers.Dense(10, activation = 'softmax')(features)
model = CustomModel(inputs, outputs)

# train을 바꿨지만 fit으로 훈련할 수 있다
model.compile(optimizer = keras.optimizers.RMSprop())
model.fit(train_images, train_labels, epochs = 3)
```

- 주의 사항
	- 어떤 방식(Sequential, 함수형, 서브클래싱)으로 모델을 만들든 사용 가능함
	- 프레임워크가 알아서 처리한다 : 오버라이딩 시 `@tf.function`을 쓸 필요는 없다.

- `compile()` 메서드에서 참조할 수 있는 것들
	- `self.compiled_loss` : `complie()`에 전달한 손실함수
	- `self.compiled_metrics` : 지표 목록. `.update_state()`를 호출해서 모든 지표 동시 업데이트 가능.
	- `self.metrics` : `compile()` 메서드에 전달한 실제 지표 목록.

```python
class CustomModel(keras.Model):
	def train_step(self, data): 
		inputs, targets = data
		with tf.GradientTape() as tape:
			predictions = self(inputs, training = True) 
		    loss = self.compiled_loss(targets, predictions) # 손실 계산
		gradients = tape.gradient(loss, self.trainable_weights)
		self.optimizer.apply_gradients(zip(gradients, self.trainable_weights))
		self.compiled_metrics.update_state(targets, predictions) # 지표 업데이트 
		return {m.name : m.result() for m in self.metrics} # 반환
```