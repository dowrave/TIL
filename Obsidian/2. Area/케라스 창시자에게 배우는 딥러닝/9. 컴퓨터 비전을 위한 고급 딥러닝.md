
1. [[#3가지 주요 컴퓨터 비전 작업|3가지 주요 컴퓨터 비전 작업]]
2. [[#이미지 분할 예제|이미지 분할 예제]]
3. [[#최신 컨브넷 아키텍처 패턴|최신 컨브넷 아키텍처 패턴]]
	1. [[#최신 컨브넷 아키텍처 패턴#모듈화 - 계층화 - 재사용(Modularity - Hierarchy - Reuse : MHR)|모듈화 - 계층화 - 재사용(Modularity - Hierarchy - Reuse : MHR)]]
	2. [[#최신 컨브넷 아키텍처 패턴#잔차 연결(Residual Connection)|잔차 연결(Residual Connection)]]
	3. [[#최신 컨브넷 아키텍처 패턴#배치 정규화Batch Normalization|배치 정규화Batch Normalization]]
	4. [[#최신 컨브넷 아키텍처 패턴#깊이별 분리 합성곱Depthwise Separable Convolution|깊이별 분리 합성곱Depthwise Separable Convolution]]
	5. [[#최신 컨브넷 아키텍처 패턴#Xception 유사 모델에 모두 적용하기|Xception 유사 모델에 모두 적용하기]]
4. [[#컨브넷이 학습한 것 해석하기|컨브넷이 학습한 것 해석하기]]
	1. [[#컨브넷이 학습한 것 해석하기#중간층 활성화 시각화|중간층 활성화 시각화]]
	2. [[#컨브넷이 학습한 것 해석하기#컨브넷 필터 시각화하기|컨브넷 필터 시각화하기]]
	3. [[#컨브넷이 학습한 것 해석하기#클래스 활성화의 히트맵 시각화하기|클래스 활성화의 히트맵 시각화하기]]


## 3가지 주요 컴퓨터 비전 작업
- `이미지 분류Image Classification` : 이미지에 1개 이상의 레이블을 할당함
	- 단일 레이블 분류 : 이미지 당 1개의 레이블 할당
	- 다중 레이블 분류 : 이미지가 속한 모든 레이블 할당
	- ex) 구글 포토 앱 키워드 검색 시, 대규모 다중 레이블 분류 모델이 실행됨.
- `이미지 분할Image Segmentation` : 이미지를 나누거나 분할하는 것이 목표이며, 각 영역은 1개의 범주를 가짐
	- ex) 줌 등에서 배경과 얼굴을 분리함
- `객체 탐지Object Detection` : 관심 객체 주변에 사각형 그리는 게 목적.

- 이외에도
	- `이미지 유사도 평가 Image Similarity Scoring` : 두 이미지가 시각적으로 비슷한 정도 출력
	- `키포인트 감지Keypoint Detection` : 이미지에서 관심 속성을 정확히 짚기
	- `포즈 추정Pose Estimation`
	- `3D 메시 추정mesh estimation`

- 객체 탐지는 이 책에서 다루지 않으며, `RetinaNet` 예제가 있으니 참고 [링크](https://keras.io/examples/vision/retinanet/)
	- 밑바닥부터 객체 탐지 모델을 만들고 훈련한다.

## 이미지 분할 예제
- 모델을 사용해 이미지 안의 각 픽셀에 클래스를 할당한다.
	- `배경, 전경`, `도로, 자동차, 보도` 등으로 구분한다.

- 2가지 종류가 있다.
1. `시맨틱 분할Semantic Segmentation` 
	- 각 픽셀이 독립적으로 1개의 의미를 가진 범주로 구분된다.
	- 2개의 고양이가 있다면, 동일한 범주로 분류된다.
2. `인스턴스 분할Instance Segmentation`
	- 이미지 픽셀을 범주 이외에도 개별 객체 인스턴스로 구분한다.
	- `cat 1`, `cat 2`

- 여기 예제는 `시맨틱 분할`이며, 주 피사체를 배경에서 분리하는 방법을 쓴다
- 데이터셋 : Oxford-IIIT Pets 데이터셋(https://www.robots.ox.ac.uk/~vgg/data/pets)을 사용한다.
	- 다양한 품종의 7390개의 개, 고양이 사진
	- 각 사진의 전경-배경 분할 마스크 포함

- `분할 마스크Segmentation Mask` 
	- 이미지 분할에서 레이블에 해당
	- 입력 이미지와 동일한 크기
	- 1개의 컬러 채널

- 따라서 `1(전경)`, `2(배경)`, `3(윤곽)`으로 구분된다.

- 데이터셋 다운로드 & 압축 해제
```sh
!wget http://www.robots.ox.ac.uk/~vgg/data/pets/data/images.tar.gz
!wget http://www.robots.ox.ac.uk/~vgg/data/pets/data/annotations.tar.gz
!tar -xf images.tar.gz
!tar -xf annotations.tar.gz
```

- 입력 사진은 `images/` 폴더에 `jpg` 파일로 저장된다.
- 분할 마스크는 `annotations/trimaps/` 폴더에 `png`로 저장된다. 이름은 동일.

- 입력 파일 경로, 분할 마스크 파일 경로 리스트로 구성
```python
import os

input_dir = 'images/'
target_dir = 'annotations/trimaps/'

input_img_paths = sorted([os.path.join(input_dir, fname) for fname in os.listdir(input_dir) if fname.endswith('.jpg')])
target_img_paths = sorted([os.path.join(target_dir, fname) for fname in os.listdir(target_dir) if fname.endswith('.png') and not fname.startswith('.')])
```

- 입력 시각화
```python
import matplotlib.pyplot as plt
from tensorflow.keras.utils import load_img, img_to_array

plt.axis('off')
plt.imshow(load_img(input_img_paths[9]))
```
![[Pasted image 20230724132604.png]]

- 분할마스크 시각화
```python
def display_target(target_array):
  normalized_array = (target_array.astype('uint8') - 1) * 127 # 원래 레이블 1, 2, 3을 0, 127, 254로 만듦
  plt.axis('off')
  plt.imshow(normalized_array[:, :, 0])

img = img_to_array(load_img(target_img_path[9], color_mode = 'grayscale'))
display_target(img)
```
![[Pasted image 20230724132844.png]]

- 입력, 타깃을 2개의 넘파이 배열로 로드
```python
import numpy as np
import random

img_size = (200, 200)
num_imgs = len(input_img_paths)

# 1337은 시드 : 품종별로 정렬되어 있는 파일 경로를 섞는다.
# 섞는 방법이 일치하도록 동일한 시드를 이용한다.
random.Random(1337).shuffle(input_img_paths)
random.Random(1337).shuffle(target_img_paths)

def path_to_input_image(path):
  return img_to_array(load_img(path, target_size = img_size))

def path_to_target(path):
  img = img_to_array(
      load_img(path, target_size = img_size, color_mode = 'grayscale')
  )
  img = img.astype('uint8') - 1 # 레이블을 0, 1, 2가 되도록 수정
  return img

input_imgs = np.zeros((num_imgs,) + img_size + (3,), dtype = 'float32')
targets = np.zeros((num_imgs,) + img_size + (1,), dtype = 'uint8')

for i in range(num_imgs):
  input_imgs[i] = path_to_input_image(input_img_paths[i])
  targets[i] = path_to_target(target_img_paths[i])
```

- 데이터 세트 나누기
```python
num_val_samples = 1000
train_input_imgs = input_imgs[:-num_val_samples]
train_targets = targets[:-num_val_samples]
val_input_imgs = input_imgs[-num_val_samples:]
val_targets = targets[-num_val_samples:]
```

- 모델 만들기
```python
# 모델 정의
from tensorflow import keras
from tensorflow.keras import layers

def get_model(img_size, num_classes):
  inputs = keras.Input(shape = img_size + (3, ))
  x = layers.Rescaling(1./255)(inputs) # 입력 이미지 0 ~ 1 스케일링 : 필수!
  x = layers.Conv2D(64, 3, strides = 2, activation = 'relu', padding = 'same')(x)
  x = layers.Conv2D(64, 3, activation = 'relu', padding = 'same')(x)
  x = layers.Conv2D(128, 3, strides = 2, activation = 'relu', padding = 'same')(x)
  x = layers.Conv2D(128, 3, activation = 'relu', padding = 'same')(x)
  x = layers.Conv2D(256, 3, strides = 2, activation = 'relu', padding = 'same')(x)
  x = layers.Conv2D(256, 3, activation = 'relu', padding = 'same')(x)

  x = layers.Conv2DTranspose(256, 3, activation = 'relu', padding = 'same')(x)
  x = layers.Conv2DTranspose(256, 3, activation = 'relu', padding = 'same', strides = 2)(x)
  x = layers.Conv2DTranspose(128, 3, activation = 'relu', padding = 'same')(x)
  x = layers.Conv2DTranspose(128, 3, activation = 'relu', padding = 'same', strides = 2)(x)
  x = layers.Conv2DTranspose(64, 3, activation = 'relu', padding = 'same')(x)
  x = layers.Conv2DTranspose(64, 3, activation = 'relu', padding = 'same', strides = 2)(x)

  outputs = layers.Conv2D(num_classes, 3, activation = 'softmax', padding = 'same')(x)
  model = keras.Model(inputs, outputs)
  return model

model = get_model(img_size = img_size, num_classes = 3)
model.summary()
```
- 전반부의 목적은 **이미지를 작은 특성맵으로 인코딩하는 것으로, 공간상의 각 픽셀은 원본 이미지의 더 큰 영역에 대한 정보를 담고 있다.**

- 전반부는 이미지 분류에서 했던 컨브넷과 비교했을 때 `다운샘플링`에 `MaxPooling2D`를 쓰는 대신 `Conv2D(strides = 2)`를 쓰고 있다.
	- 왜냐하면 이미지 분할은 **픽셀별 타깃 마스크**를 생성해야 하므로, 정보상의 공간 위치에 더 많은 관심을 두기 때문이다.
	- `MaxPooling2D`을 사용하면 풀링 윈도우 내의 위치 정보가 완전히 삭제된다.
		- 2x2개의 값 중 1개만 선택하는데, 그 1개가 어떤 위치에서 왔는지 알 수 없다!
		- 따라서 최대 풀링은 **분류 작업에는 맞지만 분할 작업에는 해**를 끼칠 수 있는 것이다.
	- 반면 `Conv2D(strides=)`은 위치 정보를 유지하면서 다운샘플링한다. 이는 `생성 모델`처럼 특성의 위치를 고려하는 경우에도 사용한다.

- 후반부는 `Conv2DTranspose`층이다. 모델의 최종 출력은 타깃 마스크의 크기인 `(200, 200, 3)`과 동일해야 한다(즉, **인풋으로 들어온 이미지와 같아야** 함)
- 따라서 지금까지 적용한 변환을 거꾸로 적용하는데, 이를 `업샘플링Upsampling`이라고 한다.
- `Conv2DTranspose` 층은 **업샘플링을 학습하는 합성곱 층**이라고 생각할 수 있다.

> 참고 : 이 모델 학습은 GPU를 켜도 매우 오래 걸리는 편임 (에포크당 1분 정도?)

- 시각화
```python
def visualize(history):
  epochs = range(1, len(history.history['loss'])+1)
  loss = history.history['loss']
  val_loss = history.history['val_loss']
  plt.figure()
  plt.plot(epochs, loss, 'bo', label = 'training loss')
  plt.plot(epochs, val_loss, 'b', label = 'validation loss')
  plt.title("training and validation loss")
  plt.legend()

visualize(history)
```
![[Pasted image 20230724142835.png]]
- 분할 마스크 예측
```python
from tensorflow.keras.utils import array_to_img

model = keras.models.load_model('oxford_segmentation.keras')

i = 4
test_image = val_input_imgs[i]
plt.axis('off')
plt.imshow(array_to_img(test_image))
mask = model.predict(np.expand_dims(test_image, 0))[0]

def display_mask(pred):
  mask = np.argmax(pred, axis = -1)
  mask *= 127
  plt.axis('off')
  plt.imshow(mask)

display_mask(mask)
```
![[Pasted image 20230724142831.png]]
> 코랩 용량 때문에 최대적합까지 실행 못했음.

- 그러나 실제로 개발한 컨브넷은 위 모델보다 훨씬 복잡하다.
- 최고 수준의 모델을 구성하기 위한 멘탈 모델과 사고 과정이 부족한데, `아키텍처 패턴Architecture Pattern`을 배워야 한다.

## 최신 컨브넷 아키텍처 패턴
- 모델의 `아키텍처`는 모델을 만드는 데 사용된 일련의 선택이다.
	- 층, 층의 설정, 연결하는 방법 등
	- 이러한 선택 모델의 `가설 공간Hypothesis Space`을 정의한다.
- 좋은 가설 공간은 현재 문제와 솔루션에 대한 `사전 지식Prior Knowledge`을 인코딩한다.
	- 예를 들면 합성곱을 쓴다 = 패턴이 이동 불변성(다른 위치에 동일한 패턴이 나타날 수 있음)임을 미리 알고 있다는 뜻이다.

- **데이터에서 효율적으로 학습하기 위해 찾고 있는 것에 대한 가정이 필요**하다.
	- 모델 아키텍처는 경사 하강법이 해결할 문제를 간단하게 만드는 것이다.

- 모델 아키텍처는 `직관성`이 핵심이지만, 전부는 아니다. 여기서도 모범 사례는 있음.
- 컨브넷 아키텍처의 모범 사례 
	- `잔차 연결Residual Connection`
	- `배치 정규화Batch Normalization`
	- `분리 합성곱Separable Convolution`

### 모듈화 - 계층화 - 재사용(Modularity - Hierarchy - Reuse : MHR)
- 복잡한 시스템을 단순하게 만들려면 `모듈화 -> 계층화 -> 재사용`을 일반적으로 적용할 수 있다. 이제부터 `MHR`이라고 함.
	- 여기서 `재사용 = 추상화Abstraction`

- 소프트웨어 엔지니어라면 알고 있는 원칙 : 효율적인 코드는 `MHR`이 되어 있음. 이런 원칙으로 코드를 리팩터링하면 `소프트웨어 아키텍처`를 수행했다고 한다.
- 인기 있는 모든 컨브넷 아키텍쳐는 층으로만 구성되지 않고, 반복되는 층 그룹(블록, 모듈)으로 구성되어 있다.
	- `VGG16`도 `합성곱 - 합성곱 - 최대풀링`이 반복되는 구조이다.

- 계층 구조가 깊으면 특성 재사용과 추상화를 장려하므로 본질적으로 좋다. 그러나 `그래디언트 소실Gradient Vanishing` 문제 떄문에 층을 쌓는 정도에 한계가 있다. 
- 여기서 나온 아이디어가 `잔차 연결`이다.

### 잔차 연결(Residual Connection)
$$y = f_4(f_3(f_2(f_1(x))))$$
- `f4`의 출력을 기반으로 각 함수의 파라미터를 조정한다.
	- `f1`을 조정하려면, `f2, f3, f4`에 오차 정보를 통과시켜야 한다.
	- 그러나 연속적인 각 함수에는 일정량의 잡음이 있는데, **함수 연결이 너무 깊으면 잡음이 그래디언트를 압도**한다. 이를 `그래디언트 손실Gradient Vanishing`이라고 한다.

- 해결법은 이전 입력에 담긴 잡음 없는 정보를 유지시키는 것이다.

>- 대표적인 게 `잔차 연결Residual Connection`이다.
	- **층이나 블록의 입력을 출력에 더하기만 하면 된다.**
	- 쉽게 말하면 $x -> f(x)$였던 것을 $x -> f(x) + x$로 만드는 것이다.

- 잔차 연결 구현
```python
x = ... # 입력 텐서
residual = x # 원본 입력 저장 : 잔차
x = block(x) # 잡음이 있을 수 있는 블록
x = add([x, residual]) # 최종 출력은 '항상' 원본 입력의 전체 정보를 보존한다.
```

- $x$와 $f(x)$의 크기가 다른 경우는 어떻게 더해야 할까?
	- 이런 경우 `1x1 Conv2D`층을 사용해서 잔차를 원하는 출력 크기로 선형적으로 투영할 수 있다.
	- 이 때 합성곱 층은 패딩 때문에 다운샘플링되지 않도록 `padding = 'same'`을 사용한다.

- 필터 개수가  변경되는 잔차 블록
```python
from tensorflow import keras
from tensorflow.keras import layers

inputs= keras.Input(shape = (32, 32, 3))
x = layers.Conv2D(32, 3, activation = 'relu')(inputs)
residual = x
x = layers.Conv2D(64, 3, activation = 'relu', padding = 'same')(x)
residual = layers.Conv2D(64, 1)(residual)
x = layers.add([x, residual])
```
- 잔차는 32개의 필터만 있지만 `Conv2D`층을 거친 x는 64개의 필터가 있다. 따라서 `1x1 Conv2D`를 사용하여 적절한 크기로 투영한다.
- 컨브넷 자체에서 다운샘플링이 되지 않도록 `padding = 'same'`을 지정한다.

- 최대풀링층에 잔차 연결
```python
inputs= keras.Input(shape = (32, 32, 3))
x = layers.Conv2D(32, 3, activation = 'relu')(inputs)
residual = x
x = layers.Conv2D(64, 3, activation = 'relu', padding = 'same')(x)
x = layers.MaxPooling2D(64, 1)(x)
residual = layers.Conv2D(64, 1, strides = 2)(residual)
x = layers.add([x, residual])
```

- 여러 블록으로 이뤄진 간단한 컨브넷 예시
```python
inputs= keras.Input(shape = (32, 32, 3))
x = layers.Rescaling(1./255)(inputs)

def residual_block(x, filters, pooling = False):
	residual = x
	x = layers.Conv2D(filters, 3, activation = 'relu', padding = 'same')(x)
	x = layers.Conv2D(filters, 3, activation = 'relu', padding = 'same')(x)
	if pooling:
		x = layers.MaxPooling2D(2, padding = 'same')(x)
		residual = layers.Conv2D(filters, 1, strides = 2)(residual) # 1.
	elif filters != residual.shape[-1]: # 2.
		residual = layers.Conv2D(filters, 1)(residual) 
	x = layers.add([x, residual])
	return x

x = residual_block(x, filters = 32, pooling = True)
x = residual_block(x, filters = 64, pooling = True)
x = residual_block(x, filters = 128, pooling = False) # 3.

x = layers.GlobalAveragePooling2D()(x)
outputs = layers.Dense(1, activation = 'sigmoid')(x)

model = keras.Model(inputs = inputs, outputs = outputs)
model.summary()
```
1. 최대 풀링 사용 시 스트라이드 합성곱을 추가한다.
	- 스케일 맞춰주는 거인 듯? `strides = 2`도 이미지 크기를 1/4로 쪼개니까.
2. 채널 수가 바뀐 경우에만 잔차를 투영한다. 
3. 바로`전역 평균 풀링GlobalAveragePooling2D`을 사용하므로 `pooling = False`를 지정한다. 
	- 전역 평균 풀링 :`sample, height, width, channel` -> `sample, channel`을 반환함

> 여기서 `residual`은 그냥 `x -> f(x)`일 때 스케일이 바뀌었다면, 그 스케일을 맞춰주기 위한 텐서 변환임.


### 배치 정규화Batch Normalization
 - `정규화Normalization` 
	 - 머신러닝 모델에 주입되는 샘플을 균일하게 만드는 광범위한 방법
	 - 일반화를 돕는다

- `Z-score 정규화`: 평균을 뺴고 표준 편차로 나눔
- `minmax 정규화` 

위 정규화는 데이터를 넣기 전에 실행되는데, 데이터가 나온 후에도 정규화가 필요할 수 있다.  다음 층에 들어가기 전의 데이터가 정규화되어 있다고 가정할 수 없기 때문이다. 이 때 `배치 정규화`가 그 역할을 한다.

- 배치 정규화는 훈련하는 동안 평균과 분산이 바뀌더라도 이에 적응하여 데이터를 정규화한다.
	- 훈련 중 **현재 배치 데이터의 평균, 분산을 이용하여 데이터를 정규화**한다.
	- 추론 시 훈련에서 본 배치 데이터의 평균과 분산의 지수 이동 평균을 쓴다.

- 지수 이동 평균
$$
v = v \times momentum + v_{new} \times (1-momentum)
$$

- 배치 정규화가 왜 도움이 되는가? 
	- 정확히 아는 사람은 없다. 원본 논문이 `내부 공변량 변화를 감소시키기 때문`이라고 했음에도 말이다.
	- 딥러닝은 과학이라기보단 **계속 변하고 경험적으로 추구되는 엔지니어링 모범 사례의 집합**이다.
	- 책들이 어떻게 작동하는지는 알려주지만, 왜 그게 잘 작동하는지는 만족스럽게 설명하지 못하는 점에서 느낄 수 있음
	- 이 책은 만족스러운 설명이 있으면 언급하지만, 배치 정규화는 그럴만한 설명이 없는 듯 하다(한국어로 나온 시점이 22년 8월임)

- 배치 정규화는 그래디언트의 전파를 도와주는 역할을 하는 것으로 보이며, 더 깊은 네트워크를 구성하는데 도움을 준다.
	- 매우 깊은 네트워크라면 여러 개의 `BatchNormalization` 층을 써야 한다.
	- `ResNet50, EfficientNet, Xception` 등이 배치 정규화를 이용한다.

- `BatchNormalization` 층은 **어떤 층 다음에도 사용할 수 있다.**

> 참고 : `Dense`, `Conv2D`층 모두 `Bias`가 들어가기 때문에 편향 벡터를 가진다. 그런데 배치 정규화는 평균을 0에 맞추는 작업을 하므로, 편향 벡터가 더 이상 필요없어진다.  
> 따라서 `use_bias = False` 옵션으로 편향을 제외, 층을 더 가볍게 만들 수 있다.

- 주의 : `BatchNormalization`층을 쓸 때, 이전 층에는 **활성화 함수를 비활성화한 다음 놓는 게 일반적이다.(논란의 여지가 있음)**

```python
x = layers.Conv2D(32, 3, use_bias = False)(x) # 활성화 함수 X
x = layers.BatchNormalization()(x) # 정규화 후에
x = layers.Activation('relu')(x) # 활성화 함수가 온다
```
- 배치 정규화가 입력 평균을 0으로 만들지만 활성화 함수는 0을 기준으로 값을 통과시키거나 삭제하기 때문이다. 근데 항상 그런 결과가 나온다는 얘기도 아님.

- **배치 정규화 층이 있는 모델을 `미세 조정`한다면 배치 정규화 층들을 동결시키는 것이 좋다.** 내부 평균과 분산이 계속 업데이트되어 `Conv2D` 층에 적용할 매우 작은 업데이트를 방해할 수 있기 때문이다.

### 깊이별 분리 합성곱Depthwise Separable Convolution
- `Conv2D`를 대체하면서 더작고 더 가볍고 모델 성능을 높일 수 있는 방법이 있다.
- `깊이별 분리 합성곱Depthwise Separable Convolution`이다.
	- 케라스에는 `SeparableConv2D`로 구현됨.
- 입력 채널별로 공간 방향의 합성곱을 수행한 뒤, 점별 합성곱(1x1)으로 출력 채널을 합친다.
![[Pasted image 20230724150619.png]]
- 이는 **공간 특성의 학습과 채널 방향 특성의 학습을 분리**하는 효과를 낸다.
	- 가정 : 공간상의 위치는 높은 상관관계를 갖지만 채널 간에는 매우 독립적이다
	- 이 가정은 일반적으로 맞음.

- 일반 `Conv2D`에 비해 더 적은 개수의 파라미터를 쓰고, 더 적은 연산을 하며, 유사한 표현 능력을 지닌다.
	- 제한된 데이터로 밑바닥부터 작은 모델을 훈련할 때 특히 중요하다.
	- `Xception` 구조 기반일 떄 분리 합성곱을 사용했다.

> `3x3 윈도우, 64개의 입, 출력 채널`에서 훈련가능 파라미터는 `36864`개이다. (바이어스 제외). 이를 깊이별 분리 합성곱으로 구현하면 `3x3x64 + 64x64 = 4672`개이다.
> 당연히 후자가 훨씬 빨라야 하지만, 하드웨어의 최적화가 이쪽으로 이뤄지지 않고 있다. `Conv2D`에 최적화 되고 있음.
> 그렇다고 깊이별 분리 합성곱의 장점이 죽는 건 아니다. 속도가 눈에 띄게 빨라지진 않아도, 적은 파라미터 개수 과대적합의 위험이 줄어든다. 채널 사이에 상관관계가 없다는 가정 하에 모델 수렴이 빨라지고 더 강력한 표현을 만든다.


### Xception 유사 모델에 모두 적용하기
- 이번 장의 컨브넷 아키텍처 조직 원칙
	- 모델은 반복되는 층인 `블록`으로 정의된다.
	- 특성 맵의 공간이 줄어듦에 따라 채널(필터) 수는 증가한다
	- 깊고 좁은 아키텍처가 넓고 얕은 것보다 낫다
	- `잔차 연결`은 깊은 네트워크 훈련에 도움이 된다.
	- 합성곱 층 다음 `배치 정규화 `층도 도움이 된다.
	- `Conv2D` -> `Separable2D` 층으로 바꾸는 게 도움이 된다.

---
- 8장에서 했던 데이터 준비부터 다시 해야 함
```python
# 1.
from google.colab import files
files.upload()

# 2.
!cp kaggle.json ~/.kaggle/
!chmod 600 kaggle.json

! kaggle competitions download -c dogs-vs-cats

!unzip --qq dogs-vs-cats.zip
!unzip -qq train.zip
```
```python
import matplotlib.pyplot as plt
from tensorflow.keras.utils import load_img, img_to_array
from tensorflow import keras
from tensorflow.keras import layers
import os, shutil, pathlib

original_dir = pathlib.Path("train")
new_base_dir = pathlib.Path("cats_vs_dogs_small")


def make_subset(subset_name, start_index, end_index):
  for category in ('cat', 'dog'):
    dir = new_base_dir / subset_name / category
    os.makedirs(dir)
    fnames = [f"{category}.{i}.jpg" for i in range(start_index, end_index)]
    for fname in fnames:
      shutil.copyfile(src = original_dir / fname, dst = dir / fname)

make_subset("train", start_index = 0, end_index = 1000)
make_subset("validation", start_index = 1000, end_index = 1500)
make_subset("test", start_index = 1500, end_index = 2500)

from tensorflow.keras.utils import image_dataset_from_directory
train_dataset = image_dataset_from_directory(
    new_base_dir / "train",
    image_size = (180, 180),
    batch_size = 32
)

validation_dataset = image_dataset_from_directory(
    new_base_dir / "validation",
    image_size = (180, 180),
    batch_size = 32
)

  

test_dataset = image_dataset_from_directory(
    new_base_dir / "test",
    image_size = (180, 180),
    batch_size = 32
)

```
---


- 모델 적용 예시
```python
data_augmentation = keras.Sequential(
    [
        layers.RandomFlip("horizontal"),
        layers.RandomRotation(0.1),
        layers.RandomZoom(0.2)
    ]
)

inputs = keras.Input(shape = (180, 180, 3))
x = data_augmentation(inputs)
x = layers.Rescaling(1./255)(x)

# 1.
x = layers.Conv2D(filters = 32, kernel_size = 5, use_bias = False)(x)

for size in [32, 64, 128, 256, 512]:
  residual = x
  x = layers.BatchNormalization()(x)
  x = layers.Activation('relu')(x)
  x = layers.SeparableConv2D(size, 3, padding = 'same', use_bias = False)(x)

  x = layers.BatchNormalization()(x)
  x = layers.Activation('relu')(x)
  x = layers.SeparableConv2D(size, 3, padding = 'same', use_bias = False)(x)  

  x = layers.MaxPooling2D(3, strides=2, padding = 'same')(x)
  
  residual = layers.Conv2D(size, 1, strides = 2, padding = 'same', use_bias = False)(residual)
  x = layers.add([x, residual])

x = layers.GlobalAveragePooling2D()(x) # 2.
x = layers.Dropout(0.5)(x)
outputs = layers.Dense(1, activation = 'sigmoid')(x)
model = keras.Model(inputs = inputs, outputs = outputs)
```
1. 1번째 `Conv2D`층은 `Separable`로 구성하지 않는다.
	- 왜냐하면 `각 채널이 독립적이다` 라는 가정이, `RGB` 이미지에서는 통하지 않기 때문이다. 실제로 높은 상관관계를 가짐
	- 따라서 1번째는 `Conv2D` 층을 이용해 분리시킨다. 이 아웃풋으로 나온 채널은 RGB 정보가 아니니까, 이후는 `SeparableConv2D`를 이용할 수 있다.

2. 원래 모델은 `Dense` 층 이전에 `Flatten()`을 이용했음. `GlobalAveragePooling2D`를 썼다는 것을 참고하자.

- 컴파일 & 훈련
```python
model.compile(optimizer = 'rmsprop', loss = 'sparse_categorical_crossentropy')
callbacks = [
    keras.callbacks.ModelCheckpoint('oxford_segmentation.keras', save_best_only = True)
]

history = model.fit(train_dataset,
                    epochs = 15,
                    callbacks = callbacks,
                    batch_size = 64,
                    validation_data = validation_dataset)
```

- 시각화
```python
def visualize(history):
  accuracy = history.history['accuracy']
  val_accuracy =history.history['val_accuracy']
  loss = history.history['loss']
  val_loss = history.history['val_loss']

  epochs = range(1, len(accuracy) + 1)

  plt.plot(epochs, accuracy, "bo", label = "Training Accuracy")
  plt.plot(epochs, val_accuracy, "bo", label = "Validation Accuracy")
  plt.title("Training and Validation Accuracy")
  plt.legend()

  plt.figure()
  plt.plot(epochs, loss, "bo", label = "Training Loss")
  plt.plot(epochs, val_loss, "b", label = "Validation Loss")
  plt.legend()
  plt.show()

visualize(history)
```

![[Pasted image 20230724161208.png]]
![[Pasted image 20230724161214.png]]

- 이전에 했던 모델은 `83%`대의 성능이었는데, 여기선 `90%`대까지 나온다.
- 모범 사례를 따르면 훨씬 나은 성능을 얻을 수 있음.
- 여기서 더 올리고 싶다면 파라미터를 튜닝해야 하는데, 이는 13장에서 다룬다.
- 이런 모범 사례는 이미지 분류 뿐만 아니라 컴퓨터 비전과 관계가 있다.
	- `Xception`은 이미지 분할 모델인 `DeepLabV3`의 표준 합성곱을 기반으로 한다.

## 컨브넷이 학습한 것 해석하기
- 근본적인 문제는 `해석 가능성Interpretability`이다. 
- 딥러닝을 `블랙박스` 같다고 얘기하는데, 모델의 표현을 사람이 이해하기 어렵기 때문이다.
	- 그러나 **컨브넷에서 해당하는 말은 아니다.**

- 가장 쉬운 3가지
	- **컨브넷의 중간층 출력을 시각화**
		- 연속된 컨브넷 층이 입력을 어떻게 변형시키는 지 이해하고, 개별적인 컨브넷 필터의 의미를 파악한다.
	- **컨브넷 필터를 시각화**
		- 컨브넷의 필터가 찾으려는 시각적인 패턴과 개념을 이해한다.
	- **클래스 활성화에 대한 히트맵을 시각화**
		- 어떤 클래스에 속하는 데 이미지의 어느 부분이 기여했는지 이해하고, 이미지에서 객체의 위치를 추정하는 데 도움이 된다.

### 중간층 활성화 시각화


--- 
- 8장에서 다룬 데이터 증식 사용 Conv2D 모델을 이용함
```python
inputs = keras.Input(shape = (180, 180, 3))
x = data_augmentation(inputs)
x = layers.Rescaling(1./255)(x)
x = layers.Conv2D(filters = 32, kernel_size = 3, activation = 'relu')(x)
x = layers.MaxPooling2D(pool_size = 2)(x)
x = layers.Conv2D(filters = 64, kernel_size = 3, activation = 'relu')(x)
x = layers.MaxPooling2D(pool_size = 2)(x)
x = layers.Conv2D(filters = 128, kernel_size = 3, activation = 'relu')(x)
x = layers.MaxPooling2D(pool_size = 2)(x)
x = layers.Conv2D(filters = 256, kernel_size = 3, activation = 'relu')(x)
x = layers.MaxPooling2D(pool_size = 2)(x)
x = layers.Conv2D(filters = 256, kernel_size = 3, activation = 'relu')(x)
x = layers.Flatten()(x)
x = layers.Dropout(0.5)(x)
outputs = layers.Dense(1, activation = 'sigmoid')(x)

model = keras.Model(inputs = inputs, outputs = outputs)
model.compile(loss = 'binary_crossentropy',
                optimizer = 'rmsprop',
                metrics = ['accuracy'])

callbacks = [
  keras.callbacks.ModelCheckpoint(
  filepath = "convnet_from_scratch_with_augmentation.keras",
  save_best_only = True,
  monitor = 'val_loss'
  )
]

history = model.fit(
    train_dataset,
    epochs = 100,
    validation_data = validation_dataset,
    callbacks= callbacks
)
```
---

- 모델 불러오기
```python
model = keras.models.load_model(
    "convnet_from_scratch_with_augmentation.keras"
)
model.summary()
```

- 완전한 별도의 이미지를 입력 이미지로 선택
```python
# 훈련에 쓰이지 않은 이미지 하나를 입력으로 선택

img_path = keras.utils.get_file(
    fname = 'cat.jpg',
    origin = 'https://img-datasets.s3.amazonaws.com/cat.jpg'
)

def get_img_array(img_path, target_size):

  # 이미지 로드 & 크기 변경
  img = keras.utils.load_img(
      img_path, target_size = target_size
  )

  # 배열 크기 & 타입 변환
  array = keras.utils.img_to_array(img)

  # 배열을 단일 이미지의 배치로 변환 -> 차원 추가
  array = np.expand_dims(array, axis = 0)
  return array

img_tensor = get_img_array(img_path, target_size = (180, 180))
```

- 이미지 출력
```python
# 이미지 출력
import matplotlib.pyplot as plt

plt.axis('off')
plt.imshow(img_tensor[0].astype('uint8'))
plt.show()
```

- 합성곱, 풀링층 활성화를 출력하는 모델
```python
# 특성맵 추출하기
layer_outputs = []
layer_names = []
for layer in model.layers:

  # 모든 Conv2D와 MaxPooling2D 층의 출력을 하나의 리스트에 추가한다.
  # isinstance(a, b) : a가 b타입에 속하면 True
  if isinstance(layer, (layers.Conv2D, layers.MaxPooling2D)):
    layer_outputs.append(layer.output)
    layer_names.append(layer.name)

# 입력이 주어졌을 때 출력을 반환하는 모델 생성
activation_model = keras.Model(inputs = model.input,
                               outputs = layer_outputs)

# 새로운 이미지에 대한 층 활성화 계산
activations = activation_model.predict(img_tensor)

# 층 활성화 시각화
first_layer_activation = activations[0]
print(first_layer_activation.shape)
```

- 아무 필터나 하나 잡고 시각화
```python
import matplotlib.pyplot as plt

plt.matshow(first_layer_activation[0, :, :, 2], cmap = 'viridis')
plt.axis('off')
plt.show()
```
![[Pasted image 20230724163522.png]]

- 모든 활성화 시각화하기
```python
# 모든 활성화 시각화하기
images_per_row = 16
for layer_name, layer_activation in zip(layer_names, activations):
  n_features = layer_activation.shape[-1]
  size = layer_activation.shape[1]
  n_cols = n_features // images_per_row
  display_grid = np.zeros(((size + 1) * n_cols - 1,
                           images_per_row * (size + 1) - 1))
  for col in range(n_cols):
    for row in range(images_per_row):
      channel_index = col * images_per_row + row
      channel_image = layer_activation[0, :, :, channel_index].copy() # 각 채널(특성) 이미지
      if channel_image.sum() != 0:
        channel_image -= channel_image.mean()
        channel_image /= channel_image.std()
        channel_image *= 64
        channel_image += 128
      channel_image = np.clip(channel_image, 0, 255).astype('uint8')
      display_grid[
          col * (size + 1): (col + 1) * size + col,
          row * (size + 1): (row + 1) * size + row
      ] = channel_image # 빈 그리드에 채널행렬 저장

	scale = 1. / size
	plt.figure(figsize = (scale * display_grid.shape[1], scale * display_grid.shape[0]))
	plt.title(layer_name)
	plt.grid(False)
	plt.axis('off')
	plt.imshow(display_grid, aspect = 'auto', cmap = 'viridis')
```

![[Pasted image 20230724164127.png]]![[Pasted image 20230724164143.png]]
![[Pasted image 20230724164147.png]]
![[Pasted image 20230724164150.png]]
![[Pasted image 20230724164153.png]]
![[Pasted image 20230724164155.png]]
![[Pasted image 20230724164158.png]]
![[Pasted image 20230724164202.png]]

>1. **1번째 층**은 여러 종류의 에러 감지기가 모여 있는 것처럼, **초기 이미지의 거의 모든 정보가 유지된다.**
>2. **층이 깊어질수록** 활성화가 추상적으로 되고, 이해하기 어려워진다. 고양이 귀나 눈 등 **고수준 개념을 인코딩**하기 시작한다.
	- 깊은 층의 표현은 시각적 컨텐츠의 정보가 줄고, **클래스에 관한 정보가 증가**한다.
>3. **비어 있는 활성화가 층이 깊어지면서 증가**한다. 
	- 필터에 인코딩된 패턴이 입력 이미지에 나타나지 않았을 때 발생한다.

- 사람과 동물이 세상을 인지하는 방식이 비슷하다.  어떤 장면을 보고 어떤 물체가 있었는지는 기억하지만, 구체적인 모양은 기억하지 못한다.
- 왜냐하면 우리 뇌는 시각적 입력에서 관련성이 적은 요소를 필터링 한 후 고수준 개념으로 변환하기 때문이다.

### 컨브넷 필터 시각화하기
- 각 필터가 반응하는 시각적 패턴을 그릴 수 있다.

- 모델 가져오기
```python
model = keras.applications.xception.Xception(
    weights = 'imagenet',
    include_top = False
)
```

- `Conv2D, SeparableConv2D`층 이름 얻기
```python
for layer in model.layers:
  if isinstance(layer, (keras.layers.Conv2D, keras.layers.SeparableConv2D)):
    print(layer.name)
```

- 특성 추출 모델 만들기
```python
layer_name = "block3_sepconv1"
layer = model.get_layer(name = layer_name)
feature_extractor = keras.Model(inputs = model.input, outputs = model.output)

# 모델 사용
activation = feature_extractor(
    keras.applications.xception.preprocess_input(img_tensor)
)
```

```python
# 입력 이미지가 층의 필터를 얼마나 활성화하는지를 정량화된 스칼라 값으로 반환함
# 즉 이게 손실함수 역할
def compute_loss(image, filter_index):
  activation = feature_extractor(image)
  filter_activation = activation[:, 2:-2, 2:-2, filter_index]
  return tf.reduce_mean(filter_activation)
```

> `model.predict(x)` vs `model(x)`  
> 둘 다 f(x)를 의미하지만, 완전히 동일한 건 아니다.
> `predict(x)`는 배치로 데이터를 순회 & 넘파이 배열로 추출한다. 매우 큰 배열도 처리할 수 있음. 
> 그러나 미분 불가능이어서, `GradientTape` 범위 내에서 이 메서드를 호출해도 그래디언트를 구할 수 없다.
> `model(x)`는 메모리 내 처리 & 확장성이 없다. 
> 따라서 모델 호출의 그래디언트를 계산하려면 `model(x)`를, 출력 값만 필요하다면 `predict(x)`를 사용한다. **저수준 경사 하강법 루프를 작성하는 게 아니라면, `predict()` 메서드를 쓰자.**


- 경사 상승법 부드럽게 사용하기 : 그래디언트 텐서를 `L2 Norm`으로 나눠 정규화하기.
```python
import tensorflow as tf

@tf.function
def gradient_ascent_step(image, filter_index, learning_rate):
  with tf.GradientTape() as tape:
    tape.watch(image) # 이미지 텐서는 텐서플로우 변수가 아니라 명시적으로 지정한다.
    loss = compute_loss(image, filter_index) # 필터 활성화 : 스칼라 손실 계산
  grads = tape.gradient(loss, image)
  grads = tf.math.l2_normalize(grads) # 그래디언트 정규화 트릭
  image += learning_rate * grads
  return image
```

```python
# 필터 시각화 생성 함수
img_width = 200
img_height = 200

def generate_filter_pattern(filter_index):
  iterations = 30
  learning_rate = 10.

  # 초기화
  image = tf.random.uniform(
      minval = 0.4,
      maxval = 0.6,
      shape = (1, img_width, img_height, 3)
  )

  for i in range(iterations):
    image = gradient_ascent_step(image, filter_index, learning_rate)
    
  return image[0].numpy()
```

```python
# 텐서 -> 이미지 변환 유틸리티 함수
def deprocess_image(image):
  image -= image.mean()
  image /= image.std()
  image *= 64
  image += 128

  image = np.clip(image, 0, 255).astype('uint8')
  image = image[25:-25, 25:-25, :]
  return image

plt.axis('off')
plt.imshow(deprocess_image(generate_filter_pattern(filter_index = 2)))
plt.show()
```
![[Pasted image 20230724170943.png]]

- 층이나 모델의 모든 필터 시각화
```python
layer_names = ['block2_sepconv1', 'block6_sepconv2', 'block13_sepconv1']

for layer_name in layer_names:
	all_images = []
	for filter_index in range(64):
	  print(f"{filter_index}번 필터 처리")
	  image = deprocess_image(
	      generate_filter_pattern(filter_index)
	  )
	  all_images.append(image)
	
	margin = 5
	n = 8
	cropped_width = img_width - 25 * 2
	cropped_height = img_height - 25 * 2
	width = n * cropped_width + (n - 1) * margin
	height = n * cropped_height + (n - 1) * margin
	stitched_filters = np.zeros((width, height, 3))
	
	for i in range(n):
	  for j in range(n):
	    image = all_images[i * n + j]
	    stitched_filters[
	        (cropped_width + margin) * i : (cropped_width + margin)* i + cropped_width,
	        (cropped_height + margin) * j : (cropped_height + margin) * j + cropped_height,
	        :,
	    ] = image
	
	keras.utils.save_img(f"filters_for_layer_{layer_name}.png", stitched_filters)
```

1. 모델의 1번째 층의 필터는 `간단한 대각선 방향의 선이나 색깔, 혹은 둘 다`를 인코딩한다.
2. 나중에 있는 층의 필터는 `에지, 색깔의 조합으로 만들어진 간단한 질감`을 인코딩한다.
3. 더 뒤로 갈수록 깃털, 눈, 나뭇잎 등 `자연적인 이미지에서 찾을 수 있는 질감`을 인코딩한다.

### 클래스 활성화의 히트맵 시각화하기
- 이미지의 어느 부분이 컨브넷의 최종 분류에 기여하는지 이해할 수 있음
- `모델 해석 가능성Model Interpretability`
- 또한, 이미지에 특정 물체가 있는 위치를 파악할 수도 있다.

- 이를 `클래스 활성화 맵 Class Activation Map, CAM` 시각화라고 부른다.
	- 입력 이미지에 대한 클래스 활성화의 히트맵을 만드는 방법이다.
- 특정 출력 클래스에 대해, 입력 이미지의 모든 위치를 계산한 2D 점수 그리드이다.
	- ex) 강아지 vs 고양이에 이미지를 넣으면, CAM 시각화는 고양이 클래스에 대한 히트맵을 생성, 이미지에서 고양이와 비슷한 부분을 알려준다.

- 이 챕터에서의 구현은`Grad-CAM : Visaul Explanations from Deep Networks via Gradient-based Localization`에 있음.
- `Grad-CAM`은 입력 이미지가 주어지면 합성곱 층에 있는 특성 맵의 출력을 추출한 다음, 특성 맵의 모든 채널 출력에 채널에 대한 클래스의 그래디언트 평균을 곱한다.

```python
model = keras.applications.xception.Xception(weights = 'imagenet') # 최상위 밀집 연결층 포함
```

```python
# Xception 모델에 맞게 이미지 전처리
img_path = keras.utils.get_file(
    fname = "elephant.jpg",
    origin = "https://img-datasets.s3.amazonaws.com/elephant.jpg"
)

def get_img_array(img_path, target_size):
  img = keras.utils.load_img(img_path, target_size = target_size)
  array = keras.utils.img_to_array(img) # 299, 299, 3 / float32
  array = np.expand_dims(array, axis = 0) # 1, 299, 299, 3
  array = keras.applications.xception.preprocess_input(array)
  return array

img_array = get_img_array(img_path, target_size = (299, 299))
```

```python
# 사전 훈련 네트워크 실행 & 예측 벡터 디코딩
preds = model.predict(img_array)
print(keras.applications.xception.decode_predictions(preds, top = 3)[0])

np.argmax(preds[0]) # 아프리카 코끼리 인덱스
```

- `Grad-CAM` 처리 구현
```python
# 마지막 합성곱층 반환하는 모델
last_conv_layer_name = 'block14_sepconv2_act'
classifier_layer_names = [
    'avg_pool',
    'predictions'
]
last_conv_layer = model.get_layer(last_conv_layer_name)
last_conv_layer_model = keras.Model(model.inputs, last_conv_layer.output)

# 마지막 합성곱층 활성화 -> 최종 클래스 예측 매핑 모델
classifier_input = keras.Input(shape = last_conv_layer.output.shape[1:])
x = classifier_input
for layer_name in classifier_layer_names:
  x = model.get_layer(layer_name)(x)
classifier_model = keras.Model(classifier_input, x)

# 마지막 합성곱 층 활성화에 대한, 최상위 예측클래스의 그래디언트
with tf.GradientTape() as tape:
  last_conv_layer_output = last_conv_layer_model(img_array)
  tape.watch(last_conv_layer_output)
  preds = classifier_model(last_conv_layer_output)
  top_pred_index = tf.argmax(preds[0])
  top_class_channel = preds[:, top_pred_index]

grads = tape.gradient(top_class_channel, last_conv_layer_output)

# 그래디언트 평균 & 중요도 가중치 적용 : 클래스 활성화 히트맵
pooled_grads = tf.reduce_mean(grads, axis = (0, 1, 2)).numpy()
last_conv_layer_output = last_conv_layer_output.numpy()[0]
for i in range(pooled_grads.shape[-1]):
  last_conv_layer_output[:, :, i] *= pooled_grads[i]
heatmap = np.mean(last_conv_layer_output, axis = -1)
```

```python
# 히트맵과 원본 그림을 겹친 이미지

import matplotlib.cm as cm

img = keras.utils.load_img(img_path)
img = keras.utils.img_to_array(img)
heatmap = np.uint8(255 * heatmap)

# 색 변경
jet = cm.get_cmap('jet')
jet_colors = jet(np.arange(256))[:, :3] 
jet_heatmap = jet_colors[heatmap]

jet_heatmap = keras.utils.array_to_img(jet_heatmap)
jet_heatmap = jet_heatmap.resize((img.shape[1], img.shape[0]))
jet_heatmap = keras.utils.img_to_array(jet_heatmap)

superimposed_img = jet_heatmap * 0.4 + img
superimposed_img = keras.utils.array_to_img(superimposed_img)

save_path = 'elephant_cam.jpg'
superimposed_img.save(save_path)
```
![[Pasted image 20230724174255.jpg]]
- 이 시각화 기법은 다음을 설명해준다.
	1. 왜 네트워크가 이 이미지에 코끼리가 있다고 생각하는지
	2. 코끼리가 사진 어디에 있는지

- 특히 새끼 코끼리의 귀 부분이 활성화가 강한데, 코끼리 중에서도 왜 아프리카 코끼리인지를 구분할 수 있는 지표가 된다.

