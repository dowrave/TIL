[[#1. 텐서플로우|1. 텐서플로우]]
[[#2. 케라스|2. 케라스]]
[[#3. 텐서플로우 + 케라스의 역사|3. 텐서플로우 + 케라스의 역사]]
[[#4. 환경 설정 : 넘어감(코랩에서 실습 진행)|4. 환경 설정 : 넘어감(코랩에서 실습 진행)]]
[[#5. 텐서플로우 시작하기|5. 텐서플로우 시작하기]]
	 [[#5. 텐서플로우 시작하기#1. 상수 텐서와 변수|1. 상수 텐서와 변수]]
		 [[#1. 상수 텐서와 변수#tf.Variable|tf.Variable]]
	 [[#5. 텐서플로우 시작하기#2. GradientTape|2. GradientTape]]
	 [[#5. 텐서플로우 시작하기#4. 엔드투엔드 예제 : 텐서플로우 선형 분류기|4. 엔드투엔드 예제 : 텐서플로우 선형 분류기]] 
 [[#6. Keras API 이해하기|6. Keras API 이해하기]]
	 [[#6. Keras API 이해하기#1. 층|1. 층]]
		 [[#1. 층#케라스의 Layer 클래스|케라스의 Layer 클래스]]
		 [[#1. 층#동적으로 층 만들기|동적으로 층 만들기]]
	 [[#6. Keras API 이해하기#2. 층에서 모델로|2. 층에서 모델로]]
	 [[#6. Keras API 이해하기#3. 컴파일 단계|3. 컴파일 단계]]
	 [[#6. Keras API 이해하기#4. 손실 함수 선택하기|4. 손실 함수 선택하기]]
	 [[#6. Keras API 이해하기#5. fit() 이해하기|5. fit() 이해하기]]
	 [[#6. Keras API 이해하기#6. 검증 데이터에서 손실, 측정 지표 모니터링하기|6. 검증 데이터에서 손실, 측정 지표 모니터링하기]]
	 [[#6. Keras API 이해하기#7. 추론 : 훈련 모델 사용하기|7. 추론 : 훈련 모델 사용하기]]


## 1. 텐서플로우
- 구글에서 만든 파이썬 기반의 무료 오픈소스 머신러닝 플랫폼
- 목적 : 연구자가 수치 텐서에 대한 수학적 표현을 적용
- 넘파이의 기능을 넘어서는 점들
	- 미분 가능하다면 자동으로 그래디언트 계산 가능
	- GPU, TPU 실행 가능
	- 분산 연산 쉬움
	- 텐서플로우 프로그램은 C++, JS, 텐서플로 라이트 등 다른 런타임에 알맞게 변환할 수 있고, 실전 환경에 쉽게 배포할 수 있다.
- 라이브러리 이상의 플랫폼임.
	- TF-Agent : 강화학습 연구
	- Tensorflow-Serving : 제품 배포
	- TensorFlow Hub : 사전 훈련된 모델의 저장소
- 확장하기도 쉬움
	- 알파제로
	- 기상예보 모델

## 2. 케라스
- 텐서플로우 위에 구축된 딥러닝용 API
	- 어떤 종류의 딥러닝 모델도 쉽게 만들고 훈련할 수 있는 방법 제공
	- 텐서플로우 위에 구축되므로, 다양한 HW 위에서 실행 가능하고 분산도 가능함
- 개발자 경험을 중요하게 생각한다. -> 배우기 쉽고, 전문가에게도 생산성이 높다.
	- 일관되고 간단한 워크플로우
	- 일반적인 사용에 필요한 작업 횟수 최소화
	- 에러에 대한 명확하고 실행 가능한 피드백
- 케라스 사용 예시 : 구글, 넷플릭스, 우버, CERN, NASA, Yelp, 인스타카트, 스퀘어
	- 캐글에서도 대부분의 딥러닝 사용자는 케라스를 이용함
- 모델을 구축하고 훈련하는 데 하나의 표준 방식을 따르지 않음
	- 고수준 ~ 저수준까지 다양한 워크플로우 가능
	- 사이킷런처럼 `fit()`을 호출하고 프레임워크가 알아서 처리하게 할 수도 있고
	- 넘파이처럼 모든 세부 내용을 제어할 수도 있음
	- 즉, 시작하면서 배운 모든 것이 전문가가 되어서도 유용하다는 것.
		- 이런 점은 파이썬 : 여러 사용 방식을 제공하고 섞어 써도 잘 작동하는 멀티 패러다임 언어와 비슷하다.

## 3. 텐서플로우 + 케라스의 역사
- 케라스가 2015년 3월, 텐서플로우가 11월에 나옴
	- 케라스는 원래 Theano 라이브러리였음
- 텐서플로우 릴리즈 후, 케라스는 멀티백엔드 구조로 리팩터링됨.
	- 케라스를 씨아노나 텐서플로우와 함께 쓸 수 있음
	- 환경변수를 바꿔 두 라이브러리 사이를 쉽게 전환 가능
	- 이런 식으로 여러 라이브러리 위에서 쓸 수 있게 되다가 결국 텐서플로우 위로만 고정.

## 4. 환경 설정 : 넘어감(코랩에서 실습 진행)


## 5. 텐서플로우 시작하기
- 신경망 훈련은 **저수준**과 **고수준**으로 나눠서 진행됨.
	- 저수준 (텐서플로우)
		- 텐서
		- 역전파
	- 고수준(케라스)
		- 모델, 층
		- 손실함수
		- 옵티마이저
		- 측정 지표
		- 훈련 루프

### 1. 상수 텐서와 변수
```python
x = tf.ones(shape = (2, 1))
x = tf.zeros(shape = (2, 1))
x = tf.random.normal(shape = (3, 1), mean = 0, stddev = 1.) # 정규분포에서 뽑은 각 값을 3행 1열로 만듦
x = tf.random.uniform(shape = (3, 1), minval = 0, maxval = 1.) # 균등분포에서 뽑은 각 값을 3행 1열로 만듦
```

- 넘파이 배열과 텐서 사이의 가장 큰 차이점: **텐서에는 값을 할당할 수 없다.**
```python
x = np.ones(shape = (2, 2))
y = tf.ones(shape = (2, 2))

x[0, 0] = 0.
y[0, 0] = 0. # 에러 발생
```

#### tf.Variable
- 모델 훈련에서 값 할당이 안되면 변수는 어떻게 업데이트되나요? : `tf.Variable`이어야만 내부 값을 바꿀 수 있다.
```python
v = tf.Variable(initial_value = tf.random.normal(shape = (3, 1)))

# 값 할당하기
v.assign(tf.ones((3, 1))) # 전체
v[0, 0].assign(3.) # 일부

# 각각 +=, -=
v.assign_add(tf.ones((3, 1))) # 모든 원소에 1을 더함
```

- 수학 계산 : `tf.ones, tf.square, tf.sqrt, tf.matmul` 등등

### 2. GradientTape
- 넘파이에서 할 수 없음. 
- 텐서의 경우 미분가능하다면 그래디언트 계산이 가능하다.

```python
var = tf.Variable(initial_Value = 3.)
with tf.GradientTape() as tape:
	result = tf.square(input_var)
gradient = tape.gradient(result, input_var)
```

- 상수 텐서는 tape 블록 내에 `tape.watch(const_tensor)`로 추적한다는 것을 알려줘야 한다.
	- 자원 낭비를 막기 위해 감시할 대상을 수동으로 지정하기도 한다.

- 이계 그래디언트 계산하기
```python
time = tf.Variable(0.)
with tf.GradientTape() as outer_tape:
	with tf.GradientTape() as inner_tape:
		position = 4.9 * time ** 2
	speed = inner_tape.gradient(position, time)
acceleration = outer_tape.gradient(speed, time)
```

### 3. 엔드투엔드 예제 : 텐서플로우 선형 분류기

1. 구분되는 데이터 만들기 
- 특정 평균과 공분산 행렬을 가진 랜덤 분포에서 좌표값 뽑기
```python
import matplotlib.pyplot as plt
import tensorflow as tf
import numpy as np

num_samples_per_class = 1000

negative_samples = np.random.multivariate_normal(
												 mean = [0, 3],
												 cov = [[1, 0.5], [0.5, 1]],
												 size = num_samples_per_class
)

positive_samples = np.random.multivariate_normal(
												 mean = [3, 0],
												 cov = [[1, 0.5], [0.5, 1]],
												 size = num_samples_per_class
)

# (1000, 2)인 2개의 샘플들을 수직 연결
inputs = np.vstack((negative_samples, positive_samples)).astype(np.float32)

# 타깃 레이블 생성
targets = np.vstack((np.zeros((num_samples_per_class, 1), dtype = "float32"),
					np.ones((num_samples_per_class, 1), dtype = "float32")
					))

# 시각화
plt.scatter(inputs[:, 0], inputs[:, 1], c = targets[:, 0])
plt.show()
```


2. 선형 분류기 만들기

- 선형 분류기는 1개의 아핀 변환 : y = W x + b 임
```python
input_dim = 2
output_dim = 1
W = tf.Variable(initial_value = tf.random.uniform(shape = (input_dim, output_dim)))
b = tf.Variable(initial_value = tf.zeros(shape = (output_dim,)))

def model(inputs):
	return tf.matmul(inputs, W) + b

# 손실함수
def square_loss(targets, predictions):
	per_sample_losses = tf.square(targets - predictions)
	return tf.reduce_mean(per_sample_losses) # 샘플 당 손실값을 1개의 손실값으로 평균함

lr = 0.1

def training_step(inputs, targets):
	with tf.GradientTape() as tape:
		predictions = model(inputs)
		loss = square_loss(targets, predictions)
	grad_loss_wrt_W, grad_loss_wrt_b = tape.gradient(loss, [W, b])
	W.assign_sub(grad_loss_wrt_W * lr)
	b.assign_sub(grad_loss_wrt_b * lr)
	return loss

# 전체 데이터 사용
# 미니배치 대비 실행 시간이 오래 걸리나 손실 감소에 더 효율적이고, 학습률을 크게 할 수 있음
for step in range(40):
	loss = training_step(inputs, targets)
	print(f"{step}번째 손실 : {loss:.4f}")

# 예측 & 시각화
predictions = model(inputs)
plt.scatter(inputs[:, 0], inputs[:, 1], c = predictions[:, 0] > 0.5)
# 직선 긋기
x = np.linspace(-1, 4, 100) # -1 ~ 4 사이에 100개의 숫자 생성. 간격 일정함
y = -W[0] / W[1] * x + (0.5 - b) / W[1] # 직선의 방정식
plt.plot(x, y, "-r")

plt.show()
```
- 직선의 방정식
$$
prediction = w_1 \cdot x + w_2 \cdot y + b
$$
- 클래스 0은 이 값이 0.5보다 작다, 1은 0.5보다 크다 임
![[Pasted image 20230720223135.png]]
- 이게 선형 분류기의 전부임

## 6. Keras API 이해하기

### 1. 층 
- 상태가 없는 층도 있으나, 대부분의 층은 가중치를 가진다.
	- 상태가 없는 층 : `flatten`, `pooling`, `dropout` (다 쓰는 거네?)
- 층마다 적절한 텐서 포맷과 데이터 처리 방식이 다르다
	- 벡터 데이터는 밀집 연결층으로 처리함`Dense`
	- 시퀀스 데이터는 순환층`LSTM 등`과 1D 합성곱 층`Conv1D`으로 처리함
	- 이미지 데이터는 `Conv2D` 층으로 처리함

#### 케라스의 Layer 클래스
- 케라스의 모든 것은  `Layer`이거나 이와 상호작용하는 무언가이다.
```python
from tensorflow import keras

class SimpleDense(keras.layers.Layer): # 모든 층은 Layer 클래스를 상속받음
	def __init__(self, units, activation = None):
		super().__init__()
		self.units = units
		self.activation = activation
	
	def build(self, input_shape): # 가중치를 생성함
		input_dim = input_shape[-1]
		self.W = self.add_weight(shape = (input_dim, self.units),
								initializer = "random_normal")
		self.b = self.add_weight(shape = (self.units, ),
								initializer = "zeros")

	def call(self, inputs): # 정방향 패스 계산을 정의함
		y = tf.matmul(inputs, self.W) + self.b
		if self.activation is not None:
			y = self.activation(y)
```

- 사용 예시
```python
my_dense = SimpleDense(units = 32, activation = tf.nn.relu)
input_tensor = tf.ones(shape = (2, 784))
output_tensor = my_dense(input_tensor)
print(output_tensor.shape)
```
- `SimpleDense`를 그대로 호출했음 : 즉 `__call__()` 메서드로 호출됐는데, 굳이 안에서 `call` 함수를 정의할 필요가 있을까?
	- **때에 맞춰 가중치를 생성**해야 하기 때문이다.

#### 동적으로 층 만들기
- `층 호환Layer Compatibility` : **모든 층은 특정 크기의 입력 텐서만 받고, 특정 크기의 출력 텐서만 반환**한다.
	- 모델에 연속적으로 추가하는 경우 앞의 사이즈를 명시하지 않아도 **동적으로 맞춰서 만들어짐.** 인풋으로 받는 크기만 명시해주면 된다.

- 기본 `Layer` 클래스의 `__call__()` 메서드는 아래와 같다.
```python
def __call__(self, inputs):
	if not self.built:
		self.build(inputs.shape)
		self.built = True
	return self.call(inputs)
```
- `build` 메서드가 자동으로 호출되는 것을 주목하자.

### 2. 층에서 모델로
- 모델은 층으로 구성된 그래프이다.
- 모델은 만드는 방법은 2가지이다.
	1. 직접 `Model` 클래스의 서브 클래스를 만든다
	2. `함수형 API`를 이용한다.
- 모델은 가설 공간을 정의한다. 모델을 정의하면 입력 -> 출력 데이터로 매핑하는 텐서 연산이 제한되며, 우리가 여기서 가중치의 좋은 값을 찾는 것이다.
- **데이터 학습을 위해선 데이터에 대한 가정을 해야 한다.**
	- 가정에서 학습할 수 있는 것이 정의된다.
	- 따라서 모델의 구조는 매우 중요하다.
	- ex) `Dense`층이 활성화 함수 없이 2개의 클래스를 분류하는 문제에 쓰였다면 두 클래스가 선형적으로 분류될 수 있다고 가정하는 것이다.
- 네트워크 구조 선택은 모범 사례와 원칙이 있지만, 연습이 필요하다.
- 선택을 위한 원리는 앞으로 소개할 것임

### 3. 컴파일 단계
- 모델 구조 정의 후..
1. **손실 함수** : 훈련 함수에서 최소화할 값
2. **옵티마이저** : 손실 함수를 기반으로 네트워크의 업데이트 방법 결정(SGD의 일종을 고른다)
3. **측정 지표** : 모니터링할 성능의 척도. 예를 들면 정확도. 
	- 측정 지표는 미분 불가능해도 됨
- 이 3개를 정하면 `compile()`과 `fit()`으로 모델 훈련을 시작할 수 있다.

```python
model = keras.Sequential([keras.layers.Dense(1)])
model.compile(optimizer = 'rmsprop', 
			  loss = 'mean_squared_error', 
			  metrics = ['accuracy'])
```
- 실제 객체로 지정해도 무방하지만, 이게 편하죠?
	- 단, 바꾸고 싶은 게 있다면 객체에서 수정한 다음 그 객체를 집어넣어줘야 한다.

- 옵티마이저
	- SGD(모멘텀 선택 가능)
	- RMSProp
	- Adam
	- Adagrad
	- 기타
- 손실 함수
	- CategoricalCrossentropy
	- SparseCategoricalCrossentropy
	- BinaryCrossentropy
	- MeanSqaureError
	- KLDivergence
	- ConsineSimilarity
	- 기타
- 측정 지표
	- CategoricalAccuracy
	- SparseCategoricalAccuracy
	- BinaryAccuracy
	- AUC
	- Precision
	- Recall
	- 기타

### 4. 손실 함수 선택하기
- **분류, 회귀, 시퀀스 예측 등의 일반적인 문제에는 올바른 손실 함수를 선택하는 간단한 가이드라인이 있다.**
	- 2개의 클래스 분류: BinaryCrossentropy
	- 다중 클래스 분류 : CategoricalCrossentropy
	- 어떤 새로운 연구를 할 때만 자기만의 손실 함수를 만들게 될 것임

### 5. fit() 이해하기
- `fit()`은 훈련 루프를 구현한다.
	- 훈련할 **데이터** : 넘파이 배열이나 Dataset 객체로 전달됨
	- 훈련할 **에포크 횟수** : 전달한 데이터에서 훈련 루프 반복 횟수
	- 각 에포크에서 사용할 **배치 크기** 

```python
history = model.fit(inputs,
					targets,
					epochs = 5,
					batch_size = 128)
```

- `fit()` 호출 시 `History` 객체가 반환된다.
- 이 객체는 `"loss"`, 혹은 특정 지표 이름의 키나 에포크 값의 리스트를 매핑한다.

### 6. 검증 데이터에서 손실, 측정 지표 모니터링하기
- `sklearn`의 `train_test_spilt`이 바로 생각나는데, 여기선 랜덤한 인덱스를 전달해서 분류하고 있음
```python
# 1. 입력과 타깃을 섞음
indices_permutation = np.random.permutation(len(inputs))
shuffled_inputs = inputs[indices_permutation]
shuffled_targets = targets[indices_permutation]

# 2. 30%의 데이터를 검증 데이터로 사용함
num_validation_samples = int(0.3 * len(inputs))
val_inputs = shuffled_inputs[:num_validation_samples]
val_targetss = shuffled_targets[:num_validation_samples]

# ...

model.fit(training_inputs, training_targets, epochs= 5, batch_size = 16,
		 validation_data = (val_inputs,val_targets)) # 검증 데이터 전달
```

- 검증 손실 및 측정 지표 확인하기
```python
loss_and_metrics = model.evaluate(val_inputs, val_targets, batch_size = 128)
```

### 7. 추론 : 훈련 모델 사용하기
```python
# 모든 입력을 한번에 처리하므로 데이터가 많으면 부적절
predictions = model(new_inputs) # 배열/텐서 인풋, 텐서 아웃풋

# 작은 배치로 순회하여 넘파이 배열로 예측 반환
predictions = model.predict(new_inputs, batch_size = 128)
```