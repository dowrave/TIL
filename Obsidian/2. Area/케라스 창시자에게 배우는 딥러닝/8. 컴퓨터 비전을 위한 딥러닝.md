


- 딥러닝 초창기에 가장 큰 성공을 거둠
- `합성곱 신경망CNN`이 이미지 분류 대회에서 좋은 결과를 얻었음
	- `CNN`은 대부분의 컴퓨터 비전 앱에 쓰인다.

## 합성곱 신경망 소개
```python
from tensorflow import keras
from tensorflow.keras import layers

inputs = keras.Input(shape = (28, 28, 1))
x = layers.Conv2D(filters = 32, kernel_size = 3 , activation = 'relu')(inputs)
x = layers.MaxPooling2D(pool_size = 2)(x)
x = layers.Conv2D(filters = 64, kernel_size = 3 , activation = 'relu')(x)
x = layers.MaxPooling2D(pool_size = 2)(x)
x = layers.Conv2D(filters = 128, kernel_size = 3 , activation = 'relu')(x)
x = layers.Flatten()(x)
outputs = layers.Dense(10, activation = 'softmax')(x)

model = keras.Model(inputs = inputs, outputs = outputs)
```
- 입력을 `height, width, channel`의 텐서로 받는 것에 주목! 
- `MaxPooling2D` 출력은 채널은 유지하되 너비와 높이는` pool_size` 만큼 나눈다.

```python
model.summary()
input_2 (InputLayer) [(None, 28, 28, 1)] 0 
conv2d_3 (Conv2D) (None, 26, 26, 32) 320 
max_pooling2d_2 (MaxPooling (None, 13, 13, 32) 0 2D) 
conv2d_4 (Conv2D) (None, 11, 11, 64) 18496 
max_pooling2d_3 (MaxPooling (None, 5, 5, 64) 0 2D) 
conv2d_5 (Conv2D) (None, 3, 3, 128) 73856 
flatten (Flatten) (None, 1152) 0 
dense (Dense) (None, 10) 11530 
```
- `Conv2D`층은 너비와 높이를 줄이되(stride 때문) 채널은 늘린다.
- `Maxpooling2D`층은 너비와 높이를 1/2로 줄인다.
- `Flatten`은 이미지를 분류하기 위해 `Dense`로 연결해야 하기 때문에 1D 텐서로 펼치는 역할을 한다.

- 훈련
```python
from tensorflow.keras.datasets import mnist

(train_images, train_labels), (test_images, test_labels) = mnist.load_data()
train_images = train_images.reshape((60000, 28, 28, 1))
train_images = train_images.astype('float32') / 255

test_images = test_images.reshape((10000, 28, 28, 1))
test_images = test_images.astype('float32') / 255

  

model.compile(optimizer = 'rmsprop',
              loss = 'sparse_categorical_crossentropy',
              metrics = ['accuracy'])
model.fit(train_images, train_labels, epochs = 5, batch_size = 64)
```

- 평가
```python
test_loss, test_acc = model.evaluate(test_images, test_labels)
print(f"테스트 정확도 : {test_acc:.3f}")
```

- Dense 대비 Conv 층이 훨씬 잘 작동한다.

### 합성곱 연산
- `Dense`는 입력 특성 공간의 전역 패턴을 학습한다.
	- MNIST로 치면, 모든 픽셀에 거친 패턴을 학습한다.
- **`Conv2D`층은 지역 패턴을 학습한다.** 
	- 위 예제에서 3x3 크기의 패턴을 찾는다

- 이 특징은 아래의 성질을 제공한다.

1. **학습된 패턴은 `평행 이동 불변성Translation Invariant`을 가진다.** 
	- 어떤 위치에서 학습된 패턴이 다른 위치에서 나타나면 인식할 수 있다.
	- `Dense`는 새로운 위치는 새로운 패턴으로 학습한다.

2. **패턴의 공간적 계층 구조를 학습할 수 있다.**
	- 1번째 Conv2D 층이 작은 지역 패턴을 학습한다.
	- 2번째 Conv2D 층은 1번째 층의 특성으로 구성된, 더 큰 패턴을 학습한다.
	- 위 과정을 반복하면 Conv2D 층은 매우 복잡하고 추상적인 시각적 개념을 효과적으로 학습할 수 있다. 우리가 보는 세상이 공간적 계층 구조를 가졌기 때문이다.
		- `직선, 질감`이 모여 `눈이나 귀`를 만들고, 이것들이 모여 다시 `얼굴`을 형성한다.

- 합성곱 연산은 `특성 맵feature map`인 랭크-3 텐서에 적용된다.
	- 공간(높이, 너비) 및 깊이로 구성된다.

- 컬러는 3개의 채널을 가지므로 깊이 차원이 3, 흑백은 깊이 차원 1을 가진다.
- 출력 특성 맵의 경우, 채널은 컬러를 의미하지 않고 `필터filter`를 의미하게 된다.
	- **필터는 입력 데이터의 어떤 특성을 인코딩**한다.
	- ex) 고수준으로 보면 한 필터가 "입력에 얼굴이 있는가?"를 인코딩할 수 있음.

- 위 예제에서 `(28, 28, 1) -> (26, 26, 32)`가 됐는데, 32개의 (26 x 26) 배열을 `필터의 응답 맵response map`이라고 한다. 입력의 각 위치에서 필터의 응답을 나타냄.

**특성 맵**
- 깊이 축의 차원 = 하나의 특성(필터)
- 랭크 2 텐서 : 입력의 응답을 나타내는 2D 공간의 맵

#### 합성곱의 파라미터
- 입력으로부터 뽑아낼 **패치 크기** : 전형적으로 **3x3, 5x5**를 쓴다.
- **특성 맵의 출력 깊이** : 필터 갯수를 의미한다.

- 3D 입력 특성 맵 위를 윈도우(3x3, 5x5)가 슬라이딩하며 모든 위치에서 3D 특성 패치를 추출한다.
- 3D 패치는 합성곱 커널이라는 하나의 학습된 가중치 행렬과의 텐서 곱셈을 통해, `(output_depth, )` 크기의 1D 텐서로 변환된다.

#### 경계 문제와 패딩
- 윈도우의 크기와 이미지의 크기 때문에 출력 데이터의 너비, 높이 값은 입력 데이터보다 작아질 수 있다.

- **입력 데이터와 같은 너비, 높이를 출력 데이터에서도 유지하려면 `패딩`을 이용한다.**
	- 이미지의 가장 바깥 부분을 0으로 채운다.
- `Conv2D(padding = 'valid' or 'same')`
	- `valid` : 패딩을 사용하지 않는다.
	- `same` : 입력 데이터와 동일한 높이, 너비를 갖게끔 출력을 만든다

#### 합성곱 스트라이드
- `스트라이드` : 2번의 연속적인 윈도우 사이의 거리, 디폴트값은 1이다.
- 2를 쓴 경우, 특성 맵의 너비와 높이가 2의 배수로 다운샘플링된다.
	- 일부 유형의 모델에서 유용하다.
- **다운샘플링의 경우, 스트라이드보다는 맥스풀링 연산**을 쓰는 경우가 많다.

### 최대 풀링 연산
- 강제적으로 특성 맵을 다운샘플링하는 것이 최대 풀링의 역할이다.
- `윈도우 = 2x2` 크기를 쓰고, `스트라이드 = 2`를 쓴다. 
- 합성곱 : 들어온 값들을 모두 더해서 가운데에 넣는 연산
- 맥스 풀링 : 들어온 값 중 최댓값을 가운데에 넣는 연산

#### 왜 다운샘플링을 하는가?
- **처리할 특성 맵의 가중치 개수를 줄일 수 있다.**
	- 위 예제에서 Flatten에 들어가는 파라미터 수는 1152개지만, MaxPooling2D 층을 제거하고 모델을 만들면 61952개이다.

- 합성곱을 계속 거치면, 같은 3x3 윈도우를 통해 보더라도 뒤에서 보는 윈도우가 훨씬 많은 정보를 보게 된다. 
	- 다운샘플링이 없다면, 3번째로 보는 3x3 윈도우는 1번째 인풋의 7x7 윈도우만을 보게 되는데, 28x28 이미지에서 7x7만 보고 이미지 분류를 할 수 있는가?

- `평균 풀링average pooling`도 있지만, 맥스 풀링이 더 잘 작동하는 편이다.
	- 왜냐하면 특성이 특성 맵의 각 타일에서 패턴, 개념의 존재를 인코딩하는 경향이 있기 때문이다.

- 가장 좋은 서브샘플링 전략은 **합성곱으로 조밀한 특성 맵 만들기(Stride가 없는 Conv2D) -> 작은 패치에 최대로 활성화된 특성을 고르는 것(MaxPooling2D이다.**
	- 스트라이드, 평균 풀링보다 이게 좋대

## 소규모 데이터셋에서 밑바닥부터 컨브넷 훈련하기

- 매우 적은 데이터를 사용해서 이미지 분류 모델을 사용하는 일은 흔하다.
	- 수백 ~ 수만 개의 이미지를 작은 샘플이라고 한다.

> 기본적인 문제 해결 전략 : 소규모 데이터셋으로 모델 훈련하기
> 1. 규제 없이 훈련, 기준이 되는 기본 성능 만들기
> - 이 때 이슈는 과대적합
> 2. 과대적합을 줄이기 위한 방법 : `데이터 증식Data Augmentation`
> 3-1) 사전 훈련된 네트워크로 특성 추출하기
> 3-2) 사전 훈련된 네트워크를 세밀하게 튜닝하기

- 이 3가지 전략(**처음부터 작은 모델 훈련, 사전 훈련 모델로 특성 추출, 사전 훈련 모델 튜닝**)은 작은 데이터셋의 이미지 분류 문제에서 도구 상자에 포함되어 있어야 한다.


### 사전 준비
- `강아지 vs 고양이 데이터셋`은 캐글에서 받자. 코랩의 경우 아래를 따라간다.
```python
# kaggle.json 업로드
from google.colab import files
files.upload() 
```
```sh
# 경로 잘 확인하고 입력
!chmod 600 kaggle.json

# 데이터 다운로드
! kaggle competitions download -c dogs-vs-cats
# 403 forbidden이 뜬 경우 www.kaggle.com/c/dogs-vs-cats/rules 에서 약관 동의 눌러주자

# 압축 해제
!unzip --qq dogs-vs-cats.zip
!unzip -qq train.zip
```

- 데이터 설명
	- 데이터를 3개의 서브셋이 들어 있는 데이터셋을 만든다.
		- 클래스마다 1000개의 샘플로 이뤄진 훈련 세트
		- 클래스마다 500개 검증세트
		- 클래스마다 1000개 테스트 세트
	- 이렇게 연습하는 이유는 실전에서는 수만개의 이미지보단 수천개의 이미지를 만날 것이며, 데이터가 많을수록 문제가 쉬워지기 때문이다.
- 전체 구조는 train, validation, test 폴더 밑에 각각 cat, dog가 있는 형태임

```python
# 이미지 -> 훈련, 검증, 테스트 디렉터리로 복사하기
import os, shutil, pathlib
original_dir = pathlib.Path("train")
new_base_dir = pathlib.Path("cats_vs_dogs_small")

def make_subset(subset_name, start_index, end_index):
  for category in ('cat', 'dog'):
    dir = new_base_dir / subset_name / category
    os.makedirs(dir)
    fnames = [f"{category}.{i}.jpg" for i in range(start_index, end_index)]
    for fname in fnames:
      shutil.copyfile(src = original_dir / fname, dst = dir / fname)

make_subset("train", start_index = 0, end_index = 1000)
make_subset("validation", start_index = 1000, end_index = 1500)
make_subset("test", start_index = 1500, end_index = 2500)
```

### 모델 만들기
- 컨브넷 구조는 동일하되, 이미지가 더 크고 복잡하다.
- 따라서 `Conv2D` 층과 `MaxPooling2D` 단계를 추가하여, `Flatten`에 들어가는 ㅍ파라미터의 수를 줄일 수 있다.
```python
# 소규모 모델 만들기

from tensorflow import keras
from tensorflow.keras import layers

inputs = keras.Input(shape = (180, 180, 3)) # 가로세로 180의 RGB 이미지를 인풋으로 기대
x = layers.Rescaling(1./255)(inputs) # 입력을 [0, 1]로 스케일을 조정함
x = layers.Conv2D(filters = 32, kernel_size = 3, activation = 'relu')(x)
x = layers.MaxPooling2D(pool_size = 2)(x)
x = layers.Conv2D(filters = 64, kernel_size = 3, activation = 'relu')(x)
x = layers.MaxPooling2D(pool_size = 2)(x)
x = layers.Conv2D(filters = 128, kernel_size = 3, activation = 'relu')(x)
x = layers.MaxPooling2D(pool_size = 2)(x)
x = layers.Conv2D(filters = 256, kernel_size = 3, activation = 'relu')(x)
x = layers.MaxPooling2D(pool_size = 2)(x)
x = layers.Conv2D(filters = 256, kernel_size = 3, activation = 'relu')(x)
x = layers.Flatten()(x)
outputs = layers.Dense(1, activation = 'sigmoid')(x)

model = keras.Model(inputs = inputs, outputs = outputs)

model.compile(loss = 'binary_crossentropy',
              optimizer = 'rmsprop',
              metrics = ['accuracy'])
```
- `Flatten()`에 전달하기 직전, `Maxpooling2D`가 아니라 `Conv2D`에서 전달한다.

### 데이터 전처리
- 데이터는 부동 소수점 타입의 텐서로 전처리되어야 한다.
- `JPEG` 파일을 네트워크에 주입하려면 아래 과정을 거친다.

> 1. 사진 파일을 읽음
> 2. JPEG -> RGB 픽셀값으로 디코딩
> 3. 부동소수점의 텐서로 변환
> 4. 동일한 크기의 이미지(180x180)로 바꿈
> 5. 배치로 묶음

- 케라스는 이를 자동으로 처리하는 유틸리티 : `image_dataset_from_directory()` 함수를 제공한다.
- `image_dataset_from_directory(directory)` 호출 시
	1. 서브 디렉터리를 찾는다. 각 서브디렉터리는 한 클래스에 해당하는 이미지가 있다고 가정한다.
	2. 각 서브 디렉터리의 파일을 인덱싱한다.
	3. 파일을 읽고, 순서를 섞고, 텐서로 디코딩하고, 동일 크기로 변경하고, 배치로 묶는 `tf.data.Dataset` 객체를 반환한다.

```python
from tensorflow.keras.utils import image_dataset_from_directory
train_dataset = image_dataset_from_directory(
    new_base_dir / "train",
    image_size = (180, 180),
    batch_size = 32
)
validation_dataset = image_dataset_from_directory(
    new_base_dir / "validation",
    image_size = (180, 180),
    batch_size = 32
)

test_dataset = image_dataset_from_directory(
    new_base_dir / "test",
    image_size = (180, 180),
    batch_size = 32
)
```

---
> Dataset 객체 이해하기  
> 머신러닝 파이프라인을 위한 `tf.data` API 중 핵심 클래스.  
> `Dataset` 객체는 반복자(iterator)이다. `for` 루프에 쓸 수 있고, 입력 데이터와 레이블의 배치를 반환한다. `Dataset` 객체를 `fit()`에 바로 전달하는 것도 가능하다.
> 직접 구현하기 어려운 여러 기능을 처리해준다. `비동기 데이터 프리페칭`이라고 하는, 이전 배치를 모델이 처리하는 동안 다음 배치 데이터를 전처리하는 작업도 가능하다.
> `Dataset` 클래스는 데이터셋을 조작하기 위한 함수형 API도 제공한다.

```python
import numpy as np
import tensorflow as tf
random_numbers = np.random.normal(size = (1000, 16))

# 넘파이 배열, 튜플, 딕셔너리 -> Dataset 생성 가능
dataset = tf.data.Dataset.from_tensor_slices(random_numbers)

# 1개의 샘플 반환
for i, element in enumerate(dataset):
	print(element.shape)
	if i>=2:
		break

# 배치 반환
batched_dataset = dataset.batch(32)
for i, element in enumerate(batched_dataset):
	print(element.shape)
	if i>=2:
		break
```

> 유용한 메서드
> - `.shuffle(buffer_size)` : 버퍼 내의 원소를 섞는다.
> - `.prefetch(buffer_size)` : 장치 활용도를 높이고자 GPU 메모리에 로드할 데이터를 미리 준비
> - `.map(callable)` : 임의의 변환을 데이터셋의 각 원소에 적용한다.(자주 쓴다)

- `.map` 예제 : 원소 크기 변환
```python
reshaped_dataset = dataset.map(lambda x : tf.reshape(x, (4, 4)))
for i, element in enumerate(reshaped_dataset):
	print(element.shape)
	if i >= 2:
		break
```
---
다시 본문으로 돌아와서, 데이터셋이 반환하는 데이터, 레이블 크기 확인
```python
for data_batch, labels_batch in train_dataset:
  print("데이터 배치 크기 : ", data_batch.shape)
  print("레이블 배치 크기 : ", labels_batch.shape)
  break
# 데이터 배치 크기 : (32, 180, 180, 3) 
# 레이블 배치 크기 : (32,)
```

```python
# 모델 훈련
callbacks = [keras.callbacks.ModelCheckpoint(
    filepath = "convnet_from_scratch.keras",
    save_best_only = True,
    monitor = "val_loss"
)]
history = model.fit(train_dataset,
                    epochs= 30,
                    validation_data = validation_dataset,
                    callbacks = callbacks)
```

```python
# 그래프 그리기

import matplotlib.pyplot as plt

accuracy = history.history['accuracy']
val_accuracy =history.history['val_accuracy']
loss = history.history['loss']
val_loss = history.history['val_loss']

epochs = range(1, len(accuracy) + 1)

plt.plot(epochs, accuracy, "bo", label = "Training Accuracy")
plt.plot(epochs, val_accuracy, "bo", label = "Validation Accuracy")
plt.title("Training and Validation Accuracy")
plt.legend()
plt.figure()

plt.plot(epochs, loss, "bo", label = "Training Loss")
plt.plot(epochs, val_loss, "b", label = "Validation Loss")
plt.legend()
plt.show()
```
![[Pasted image 20230723022402.png]]
![[Pasted image 20230723022428.png]]

```python
# 테스트 세트에서 모델 평가하기 : 과대적합 전 최대 성능 상태를 로드한다.
test_model = keras.models.load_model("convnet_from_scratch.keras")
test_loss, test_acc = test_model.evaluate(test_dataset)
print(f"테스트 정확도 : {test_acc:.3f}")
# 테스트 정확도 : 0.724
```

- 샘플 수가 적기 때문에 과대적합이 중요한 문제이다.
- `드롭 아웃`이나 `L2 규제` 등이 있지만, **여기서는 이미지에서 일반적으로 사용되는 `데이터 증식Data Augmentation`을 이용한다.**

### 데이터 증식 이용하기
