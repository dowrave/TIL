1. [[#합성곱 신경망 소개|합성곱 신경망 소개]]
	1. [[#합성곱 신경망 소개#합성곱 연산|합성곱 연산]]
		1. [[#합성곱 연산#합성곱의 파라미터|합성곱의 파라미터]]
		2. [[#합성곱 연산#경계 문제와 패딩|경계 문제와 패딩]]
		3. [[#합성곱 연산#합성곱 스트라이드|합성곱 스트라이드]]
	2. [[#합성곱 신경망 소개#최대 풀링 연산|최대 풀링 연산]]
		1. [[#최대 풀링 연산#왜 다운샘플링을 하는가?|왜 다운샘플링을 하는가?]]
2. [[#소규모 데이터셋에서 밑바닥부터 컨브넷 훈련하기|소규모 데이터셋에서 밑바닥부터 컨브넷 훈련하기]]
	1. [[#소규모 데이터셋에서 밑바닥부터 컨브넷 훈련하기#사전 준비|사전 준비]]
	2. [[#소규모 데이터셋에서 밑바닥부터 컨브넷 훈련하기#모델 만들기|모델 만들기]]
	3. [[#소규모 데이터셋에서 밑바닥부터 컨브넷 훈련하기#데이터 전처리|데이터 전처리]]
	4. [[#소규모 데이터셋에서 밑바닥부터 컨브넷 훈련하기#데이터 증식 이용하기|데이터 증식 이용하기]]
3. [[#사전 훈련 모델 적용하기|사전 훈련 모델 적용하기]]
	1. [[#사전 훈련 모델 적용하기#특성 추출|특성 추출]]
		1. [[#특성 추출#VGG16 모델 만들기|VGG16 모델 만들기]]
		2. [[#특성 추출#데이터 증식 사용하지 않는 빠른 특성 추출|데이터 증식 사용하지 않는 빠른 특성 추출]]
		3. [[#특성 추출#데이터 증식을 사용한 특성 추출|데이터 증식을 사용한 특성 추출]]
	2. [[#사전 훈련 모델 적용하기#사전 훈련 모델 미세 조정하기|사전 훈련 모델 미세 조정하기]]



- 딥러닝 초창기에 가장 큰 성공을 거둠
- `합성곱 신경망CNN`이 이미지 분류 대회에서 좋은 결과를 얻었음
	- `CNN`은 대부분의 컴퓨터 비전 앱에 쓰인다.

## 합성곱 신경망 소개
```python
from tensorflow import keras
from tensorflow.keras import layers

inputs = keras.Input(shape = (28, 28, 1))
x = layers.Conv2D(filters = 32, kernel_size = 3 , activation = 'relu')(inputs)
x = layers.MaxPooling2D(pool_size = 2)(x)
x = layers.Conv2D(filters = 64, kernel_size = 3 , activation = 'relu')(x)
x = layers.MaxPooling2D(pool_size = 2)(x)
x = layers.Conv2D(filters = 128, kernel_size = 3 , activation = 'relu')(x)
x = layers.Flatten()(x)
outputs = layers.Dense(10, activation = 'softmax')(x)

model = keras.Model(inputs = inputs, outputs = outputs)
```
- 입력을 `height, width, channel`의 텐서로 받는 것에 주목! 
- `MaxPooling2D` 출력은 채널은 유지하되 너비와 높이는` pool_size` 만큼 나눈다.

```python
model.summary()
input_2 (InputLayer) [(None, 28, 28, 1)] 0 
conv2d_3 (Conv2D) (None, 26, 26, 32) 320 
max_pooling2d_2 (MaxPooling (None, 13, 13, 32) 0 2D) 
conv2d_4 (Conv2D) (None, 11, 11, 64) 18496 
max_pooling2d_3 (MaxPooling (None, 5, 5, 64) 0 2D) 
conv2d_5 (Conv2D) (None, 3, 3, 128) 73856 
flatten (Flatten) (None, 1152) 0 
dense (Dense) (None, 10) 11530 
```
- `Conv2D`층은 너비와 높이를 줄이되(stride 때문) 채널은 늘린다.
- `Maxpooling2D`층은 너비와 높이를 1/2로 줄인다.
- `Flatten`은 이미지를 분류하기 위해 `Dense`로 연결해야 하기 때문에 1D 텐서로 펼치는 역할을 한다.

- 훈련
```python
from tensorflow.keras.datasets import mnist

(train_images, train_labels), (test_images, test_labels) = mnist.load_data()
train_images = train_images.reshape((60000, 28, 28, 1))
train_images = train_images.astype('float32') / 255

test_images = test_images.reshape((10000, 28, 28, 1))
test_images = test_images.astype('float32') / 255

  

model.compile(optimizer = 'rmsprop',
              loss = 'sparse_categorical_crossentropy',
              metrics = ['accuracy'])
model.fit(train_images, train_labels, epochs = 5, batch_size = 64)
```

- 평가
```python
test_loss, test_acc = model.evaluate(test_images, test_labels)
print(f"테스트 정확도 : {test_acc:.3f}")
```

- Dense 대비 Conv 층이 훨씬 잘 작동한다.

### 합성곱 연산
- `Dense`는 입력 특성 공간의 전역 패턴을 학습한다.
	- MNIST로 치면, 모든 픽셀에 거친 패턴을 학습한다.
- **`Conv2D`층은 지역 패턴을 학습한다.** 
	- 위 예제에서 3x3 크기의 패턴을 찾는다

- 이 특징은 아래의 성질을 제공한다.

1. **학습된 패턴은 `평행 이동 불변성Translation Invariant`을 가진다.** 
	- 어떤 위치에서 학습된 패턴이 다른 위치에서 나타나면 인식할 수 있다.
	- `Dense`는 새로운 위치는 새로운 패턴으로 학습한다.

2. **패턴의 공간적 계층 구조를 학습할 수 있다.**
	- 1번째 Conv2D 층이 작은 지역 패턴을 학습한다.
	- 2번째 Conv2D 층은 1번째 층의 특성으로 구성된, 더 큰 패턴을 학습한다.
	- 위 과정을 반복하면 Conv2D 층은 매우 복잡하고 추상적인 시각적 개념을 효과적으로 학습할 수 있다. 우리가 보는 세상이 공간적 계층 구조를 가졌기 때문이다.
		- `직선, 질감`이 모여 `눈이나 귀`를 만들고, 이것들이 모여 다시 `얼굴`을 형성한다.

- 합성곱 연산은 `특성 맵feature map`인 랭크-3 텐서에 적용된다.
	- 공간(높이, 너비) 및 깊이로 구성된다.

- 컬러는 3개의 채널을 가지므로 깊이 차원이 3, 흑백은 깊이 차원 1을 가진다.
- 출력 특성 맵의 경우, 채널은 컬러를 의미하지 않고 `필터filter`를 의미하게 된다.
	- **필터는 입력 데이터의 어떤 특성을 인코딩**한다.
	- ex) 고수준으로 보면 한 필터가 "입력에 얼굴이 있는가?"를 인코딩할 수 있음.

- 위 예제에서 `(28, 28, 1) -> (26, 26, 32)`가 됐는데, 32개의 (26 x 26) 배열을 `필터의 응답 맵response map`이라고 한다. 입력의 각 위치에서 필터의 응답을 나타냄.

**특성 맵**
- 깊이 축의 차원 = 하나의 특성(필터)
- 랭크 2 텐서 : 입력의 응답을 나타내는 2D 공간의 맵

#### 합성곱의 파라미터
- 입력으로부터 뽑아낼 **패치 크기** : 전형적으로 **3x3, 5x5**를 쓴다.
- **특성 맵의 출력 깊이** : 필터 갯수를 의미한다.

- 3D 입력 특성 맵 위를 윈도우(3x3, 5x5)가 슬라이딩하며 모든 위치에서 3D 특성 패치를 추출한다.
- 3D 패치는 합성곱 커널이라는 하나의 학습된 가중치 행렬과의 텐서 곱셈을 통해, `(output_depth, )` 크기의 1D 텐서로 변환된다.

#### 경계 문제와 패딩
- 윈도우의 크기와 이미지의 크기 때문에 출력 데이터의 너비, 높이 값은 입력 데이터보다 작아질 수 있다.

- **입력 데이터와 같은 너비, 높이를 출력 데이터에서도 유지하려면 `패딩`을 이용한다.**
	- 이미지의 가장 바깥 부분을 0으로 채운다.
- `Conv2D(padding = 'valid' or 'same')`
	- `valid` : 패딩을 사용하지 않는다.
	- `same` : 입력 데이터와 동일한 높이, 너비를 갖게끔 출력을 만든다

#### 합성곱 스트라이드
- `스트라이드` : 2번의 연속적인 윈도우 사이의 거리, 디폴트값은 1이다.
- 2를 쓴 경우, 특성 맵의 너비와 높이가 2의 배수로 다운샘플링된다.
	- 일부 유형의 모델에서 유용하다.
- **다운샘플링의 경우, 스트라이드보다는 맥스풀링 연산**을 쓰는 경우가 많다.

### 최대 풀링 연산
- 강제적으로 특성 맵을 다운샘플링하는 것이 최대 풀링의 역할이다.
- `윈도우 = 2x2` 크기를 쓰고, `스트라이드 = 2`를 쓴다. 
- 합성곱 : 들어온 값들을 모두 더해서 가운데에 넣는 연산
- 맥스 풀링 : 들어온 값 중 최댓값을 가운데에 넣는 연산

#### 왜 다운샘플링을 하는가?
- **처리할 특성 맵의 가중치 개수를 줄일 수 있다.**
	- 위 예제에서 Flatten에 들어가는 파라미터 수는 1152개지만, MaxPooling2D 층을 제거하고 모델을 만들면 61952개이다.

- 합성곱을 계속 거치면, 같은 3x3 윈도우를 통해 보더라도 뒤에서 보는 윈도우가 훨씬 많은 정보를 보게 된다. 
	- 다운샘플링이 없다면, 3번째로 보는 3x3 윈도우는 1번째 인풋의 7x7 윈도우만을 보게 되는데, 28x28 이미지에서 7x7만 보고 이미지 분류를 할 수 있는가?

- `평균 풀링average pooling`도 있지만, 맥스 풀링이 더 잘 작동하는 편이다.
	- 왜냐하면 특성이 특성 맵의 각 타일에서 패턴, 개념의 존재를 인코딩하는 경향이 있기 때문이다.

- 가장 좋은 서브샘플링 전략은 **합성곱으로 조밀한 특성 맵 만들기(Stride가 없는 Conv2D) -> 작은 패치에 최대로 활성화된 특성을 고르는 것(MaxPooling2D이다.**
	- 스트라이드, 평균 풀링보다 이게 좋대

---

## 소규모 데이터셋에서 밑바닥부터 컨브넷 훈련하기

- 매우 적은 데이터를 사용해서 이미지 분류 모델을 사용하는 일은 흔하다.
	- 수백 ~ 수만 개의 이미지를 작은 샘플이라고 한다.

> 기본적인 문제 해결 전략 : 소규모 데이터셋으로 모델 훈련하기
> 1. 규제 없이 훈련, 기준이 되는 기본 성능 만들기
> - 이 때 이슈는 과대적합
> 2. 과대적합을 줄이기 위한 방법 : `데이터 증식Data Augmentation`
> 3-1) 사전 훈련된 네트워크로 특성 추출하기
> 3-2) 사전 훈련된 네트워크를 세밀하게 튜닝하기

- 이 3가지 전략(**처음부터 작은 모델 훈련, 사전 훈련 모델로 특성 추출, 사전 훈련 모델 튜닝**)은 작은 데이터셋의 이미지 분류 문제에서 도구 상자에 포함되어 있어야 한다.


### 사전 준비
- `강아지 vs 고양이 데이터셋`은 캐글에서 받자. 코랩의 경우 아래를 따라간다.
```python
# kaggle.json 업로드
from google.colab import files
files.upload() 
```
```sh
# 경로 잘 확인하고 입력
!cp kaggle.json ~/.kaggle/
!chmod 600 kaggle.json

# 데이터 다운로드
! kaggle competitions download -c dogs-vs-cats
# 403 forbidden이 뜬 경우 www.kaggle.com/c/dogs-vs-cats/rules 에서 약관 동의 눌러주자

# 압축 해제
!unzip --qq dogs-vs-cats.zip
!unzip -qq train.zip
```

- 데이터 설명
	- 데이터를 3개의 서브셋이 들어 있는 데이터셋을 만든다.
		- 클래스마다 1000개의 샘플로 이뤄진 훈련 세트
		- 클래스마다 500개 검증세트
		- 클래스마다 1000개 테스트 세트
	- 이렇게 연습하는 이유는 실전에서는 수만개의 이미지보단 수천개의 이미지를 만날 것이며, 데이터가 많을수록 문제가 쉬워지기 때문이다.
- 전체 구조는 train, validation, test 폴더 밑에 각각 cat, dog가 있는 형태임

```python
# 이미지 -> 훈련, 검증, 테스트 디렉터리로 복사하기
import os, shutil, pathlib
original_dir = pathlib.Path("train")
new_base_dir = pathlib.Path("cats_vs_dogs_small")

def make_subset(subset_name, start_index, end_index):
  for category in ('cat', 'dog'):
    dir = new_base_dir / subset_name / category
    os.makedirs(dir)
    fnames = [f"{category}.{i}.jpg" for i in range(start_index, end_index)]
    for fname in fnames:
      shutil.copyfile(src = original_dir / fname, dst = dir / fname)

make_subset("train", start_index = 0, end_index = 1000)
make_subset("validation", start_index = 1000, end_index = 1500)
make_subset("test", start_index = 1500, end_index = 2500)
```

### 모델 만들기
- 컨브넷 구조는 동일하되, 이미지가 더 크고 복잡하다.
- 따라서 `Conv2D` 층과 `MaxPooling2D` 단계를 추가하여, `Flatten`에 들어가는 ㅍ파라미터의 수를 줄일 수 있다.
```python
# 소규모 모델 만들기

from tensorflow import keras
from tensorflow.keras import layers

inputs = keras.Input(shape = (180, 180, 3)) # 가로세로 180의 RGB 이미지를 인풋으로 기대
x = layers.Rescaling(1./255)(inputs) # 입력을 [0, 1]로 스케일을 조정함
x = layers.Conv2D(filters = 32, kernel_size = 3, activation = 'relu')(x)
x = layers.MaxPooling2D(pool_size = 2)(x)
x = layers.Conv2D(filters = 64, kernel_size = 3, activation = 'relu')(x)
x = layers.MaxPooling2D(pool_size = 2)(x)
x = layers.Conv2D(filters = 128, kernel_size = 3, activation = 'relu')(x)
x = layers.MaxPooling2D(pool_size = 2)(x)
x = layers.Conv2D(filters = 256, kernel_size = 3, activation = 'relu')(x)
x = layers.MaxPooling2D(pool_size = 2)(x)
x = layers.Conv2D(filters = 256, kernel_size = 3, activation = 'relu')(x)
x = layers.Flatten()(x)
outputs = layers.Dense(1, activation = 'sigmoid')(x)

model = keras.Model(inputs = inputs, outputs = outputs)

model.compile(loss = 'binary_crossentropy',
              optimizer = 'rmsprop',
              metrics = ['accuracy'])
```
- `Flatten()`에 전달하기 직전, `Maxpooling2D`가 아니라 `Conv2D`에서 전달한다.

### 데이터 전처리
- 데이터는 부동 소수점 타입의 텐서로 전처리되어야 한다.
- `JPEG` 파일을 네트워크에 주입하려면 아래 과정을 거친다.

> 1. 사진 파일을 읽음
> 2. JPEG -> RGB 픽셀값으로 디코딩
> 3. 부동소수점의 텐서로 변환
> 4. 동일한 크기의 이미지(180x180)로 바꿈
> 5. 배치로 묶음

- 케라스는 이를 자동으로 처리하는 유틸리티 : `image_dataset_from_directory()` 함수를 제공한다.
- `image_dataset_from_directory(directory)` 호출 시
	1. 서브 디렉터리를 찾는다. 각 서브디렉터리는 한 클래스에 해당하는 이미지가 있다고 가정한다.
	2. 각 서브 디렉터리의 파일을 인덱싱한다.
	3. 파일을 읽고, 순서를 섞고, 텐서로 디코딩하고, 동일 크기로 변경하고, 배치로 묶는 `tf.data.Dataset` 객체를 반환한다.

```python
from tensorflow.keras.utils import image_dataset_from_directory
train_dataset = image_dataset_from_directory(
    new_base_dir / "train",
    image_size = (180, 180),
    batch_size = 32
)
validation_dataset = image_dataset_from_directory(
    new_base_dir / "validation",
    image_size = (180, 180),
    batch_size = 32
)

test_dataset = image_dataset_from_directory(
    new_base_dir / "test",
    image_size = (180, 180),
    batch_size = 32
)
```

---
> Dataset 객체 이해하기  
> 머신러닝 파이프라인을 위한 `tf.data` API 중 핵심 클래스.  
> `Dataset` 객체는 반복자(iterator)이다. `for` 루프에 쓸 수 있고, 입력 데이터와 레이블의 배치를 반환한다. `Dataset` 객체를 `fit()`에 바로 전달하는 것도 가능하다.
> 직접 구현하기 어려운 여러 기능을 처리해준다. `비동기 데이터 프리페칭`이라고 하는, 이전 배치를 모델이 처리하는 동안 다음 배치 데이터를 전처리하는 작업도 가능하다.
> `Dataset` 클래스는 데이터셋을 조작하기 위한 함수형 API도 제공한다.

```python
import numpy as np
import tensorflow as tf
random_numbers = np.random.normal(size = (1000, 16))

# 넘파이 배열, 튜플, 딕셔너리 -> Dataset 생성 가능
dataset = tf.data.Dataset.from_tensor_slices(random_numbers)

# 1개의 샘플 반환
for i, element in enumerate(dataset):
	print(element.shape)
	if i>=2:
		break

# 배치 반환
batched_dataset = dataset.batch(32)
for i, element in enumerate(batched_dataset):
	print(element.shape)
	if i>=2:
		break
```

> 유용한 메서드
> - `.shuffle(buffer_size)` : 버퍼 내의 원소를 섞는다.
> - `.prefetch(buffer_size)` : 장치 활용도를 높이고자 GPU 메모리에 로드할 데이터를 미리 준비
> - `.map(callable)` : 임의의 변환을 데이터셋의 각 원소에 적용한다.(자주 쓴다)

- `.map` 예제 : 원소 크기 변환
```python
reshaped_dataset = dataset.map(lambda x : tf.reshape(x, (4, 4)))
for i, element in enumerate(reshaped_dataset):
	print(element.shape)
	if i >= 2:
		break
```
---
다시 본문으로 돌아와서, 데이터셋이 반환하는 데이터, 레이블 크기 확인
```python
for data_batch, labels_batch in train_dataset:
  print("데이터 배치 크기 : ", data_batch.shape)
  print("레이블 배치 크기 : ", labels_batch.shape)
  break
# 데이터 배치 크기 : (32, 180, 180, 3) 
# 레이블 배치 크기 : (32,)
```

```python
# 모델 훈련
callbacks = [keras.callbacks.ModelCheckpoint(
    filepath = "convnet_from_scratch.keras",
    save_best_only = True,
    monitor = "val_loss"
)]
history = model.fit(train_dataset,
                    epochs= 30,
                    validation_data = validation_dataset,
                    callbacks = callbacks)
```

```python
# 그래프 그리기

import matplotlib.pyplot as plt

accuracy = history.history['accuracy']
val_accuracy =history.history['val_accuracy']
loss = history.history['loss']
val_loss = history.history['val_loss']

epochs = range(1, len(accuracy) + 1)

plt.plot(epochs, accuracy, "bo", label = "Training Accuracy")
plt.plot(epochs, val_accuracy, "bo", label = "Validation Accuracy")
plt.title("Training and Validation Accuracy")
plt.legend()

plt.figure()
plt.plot(epochs, loss, "bo", label = "Training Loss")
plt.plot(epochs, val_loss, "b", label = "Validation Loss")
plt.legend()
plt.show()
```
![[Pasted image 20230723022402.png]]
![[Pasted image 20230723022428.png]]

```python
# 테스트 세트에서 모델 평가하기 : 과대적합 전 최대 성능 상태를 로드한다.
test_model = keras.models.load_model("convnet_from_scratch.keras")
test_loss, test_acc = test_model.evaluate(test_dataset)
print(f"테스트 정확도 : {test_acc:.3f}")
# 테스트 정확도 : 0.724
```

- 샘플 수가 적기 때문에 과대적합이 중요한 문제이다.
- `드롭 아웃`이나 `L2 규제` 등이 있지만, **여기서는 이미지에서 일반적으로 사용되는 `데이터 증식Data Augmentation`을 이용한다.**

### 데이터 증식 이용하기
과대적합이 발생하는 이유는 학습할 샘플이 너무 적어서 새로운 데이터에 일반화할 수 있는 모델을 훈련시킬 수 없기 때문이다. 따라서 데이터의 양이 중요하다.
- `데이터 증식`은 **기존 훈련 샘플로부터 더 많은 데이터를 생성**하는 방법이다. 
	- **그럴 듯한 이미지를 생성**하도록 **기존 이미지에 여러 랜덤 변환을 적용**한다.

케라스에서는 모델의 시작 부분에 여러 `데이터 증식 층data augmentation layer`을 추가할 수 있다.

```python
data_augmentation = keras.Sequential(
    [
        layers.RandomFlip("horizontal"),
        layers.RandomRotation(0.1),
        layers.RandomZoom(0.2)
    ]
)
```
- 실제 종류는 더 많음
- `RandomFlip` : 50%의 이미지를 `수평`으로 뒤집는다(디폴트 : `horizontal_and_vertical`)
- `RandomRotation` : `[-10%, +10%]` 범위 내에서 입력 이미지 회전. 
	- 각도로 치면 -36도 ~ + 36도
- `RandomZoom(0.2)` : `[-20%, +20%]`. 
	- 범위를 `(-0.2, 0.3)`처럼 다르게 줄 수 있고, 높이와 너비가 같은 비율로 변환된다. 
	- 양수는 축소, 음수는 확대를 나타낸다.

```python
# 이미지 확인

plt.figure(figsize = (10, 10))
for images, _ in train_dataset.take(1): # N개의 배치만 샘플링
  for i in range(9):
    augmented_images = data_augmentation(images) # 이미지 증식 적용
    ax = plt.subplot(3, 3, i+1)
    plt.imshow(augmented_images[0].numpy().astype('uint8'))
    plt.axis('off')
```
- 내부 반복문에서 쓰이는 `i`는 이미지 위치만 지정한다. 
- 표시되는 이미지는 `augmented_images[0]`이며, 여기서 다르게 증식된 이미지들 각각을 출력하는 방식임.
	- 근데 다른 이미지를 지정하지 않았는데 저절로 다른 이미지로 넘어간다. `dataset`의 특성인 듯.

![[Pasted image 20230723124616.png]]
- 데이터 증식을 사용했지만, 같은 원본 이미지에서 나온 데이터들이기 때문에 추가로 `Dropout()`을 추가한다.
- 이미지 증식 층은 추론 시에는 동작하지 않는다 : `predict()`와 `evaluate()`을 받아도 동작하지 않는다.

```python
inputs = keras.Input(shape = (180, 180, 3))
x = data_augmentation(inputs)
x = layers.Rescaling(1./255)(x)
x = layers.Conv2D(filters = 32, kernel_size = 3, activation = 'relu')(x)
x = layers.MaxPooling2D(pool_size = 2)(x)
x = layers.Conv2D(filters = 64, kernel_size = 3, activation = 'relu')(x)
x = layers.MaxPooling2D(pool_size = 2)(x)
x = layers.Conv2D(filters = 128, kernel_size = 3, activation = 'relu')(x)
x = layers.MaxPooling2D(pool_size = 2)(x)
x = layers.Conv2D(filters = 256, kernel_size = 3, activation = 'relu')(x)
x = layers.MaxPooling2D(pool_size = 2)(x)
x = layers.Conv2D(filters = 256, kernel_size = 3, activation = 'relu')(x)
x = layers.Flatten()(x)
x = layers.Dropout(0.5)(x)
outputs = layers.Dense(1, activation = 'sigmoid')(x)

model = keras.Model(inputs = inputs, outputs = outputs)
model.compile(loss = 'binary_crossentropy',
              optimizer = 'rmsprop',
              metrics = ['accuracy'])

callbacks = [
    keras.callbacks.ModelCheckpoint(
        filepath = "convnet_from_scratch_with_augmentation.keras",
        save_best_only = True,
        monitor = 'val_loss'
    )
]

history = model.fit(
    train_dataset,
    epochs = 100,
    validation_data = validation_dataset,
    callbacks= callbacks
)
```

- 실제 성능은 `데이터 증식` 전보다 훨씬 올라갔음을 볼 수 있음.(72% -> 81%)
```python
# 시각화

def visualize(history):
	accuracy = history.history['accuracy']
	val_accuracy =history.history['val_accuracy']
	loss = history.history['loss']
	val_loss = history.history['val_loss']
	
	epochs = range(1, len(accuracy) + 1)
	plt.plot(epochs, accuracy, "b", label = "Training Accuracy")
	plt.plot(epochs, val_accuracy, "r", label = "Validation Accuracy")
	plt.title("Training and Validation Accuracy")
	plt.legend()
	plt.figure()
	plt.plot(epochs, loss, "b", label = "Training Loss")
	plt.plot(epochs, val_loss, "r", label = "Validation Loss")
	plt.title("Training and Validation Loss")
	plt.legend()
	plt.show()
```

```python
# 최고 성능 모델로 테스트
test_model = keras.models.load_model(
    "convnet_from_scratch_with_augmentation.keras",  
)
test_loss, test_acc = test_model.evaluate(test_dataset)
print(f"테스트 정확도 :  {test_acc:.3f}") # 0.819
```

- 그러나 데이터가 적기 때문에 `ConvNet`을 처음부터 훈련해서 더 높은 정확도를 달성하기는 어렵다. 이 때 쓸 수 있는 방법이 이미 훈련된 모델을 적용하는 일이다.

---

## 사전 훈련 모델 적용하기
- `사전 훈련 모델Pretrained Model`은 대규모 이미지 분류를 위한 대량의 데이터셋에서 미리 훈련된 모델이다.
	- 새로운 문제가 원래 작업과 완전히 다른 클래스에 대한 것이더라도, `특성의 계층 구조` 때문에 **일반적으로 유용한 경우가 많다.**
	- 예를 들면 `ImageNet` 데이터셋으로 훈련된 모델은 동물, 생활용품으로 훈련되었다. 이를 **이미지에서 가구 아이템만 식별하는 방식 등, 다른 용도로 사용할 수 있다.**

- `ImageNet`은 1400만개의 레이블된 이미지, 1000개의 클래스로 이뤄졌으며, 강아지와 고양이를 비롯한 많은 동물을 포함하고 있다.
- 이 예제에서는 `VGG16`(2014년) 모델을 쓴다. 최고 수준의 성능은 아니지만 모델 구조가 비슷함. 
	- 모델들 이름으로는 `VGG, ResNet, Inception, Xception` 등이 있으며, 컴퓨터 비전 딥러닝을 공부하다보면 알게 된다.

### 특성 추출
- 사전에 학습된 모델 표현을 사용해 새로운 샘플에서 특성을 뽑아낸다.
- 컨브넷은 크게 2가지로 구분되는데,
	1. `Conv2D & MaxPooling2D`하는 부분 (합성곱 기반 층)
	2. `Dense`로 전달하여 분류하는 부분 (밀집 분류기)

- **합성곱 기반 층의 경우, 학습된 표현이 더 일반적이라 재사용이 가능**하다.
	- 특성 맵은 이미지에 대한 일반적인 개념의 존재 여부를 기록한 맵이다.
	- 특정 층에서 추출한 표현의 일반성, 재사용성 수준은 모델 층의 깊이에 달려 있다. 합성곱 기반 층 중에서도 **일부만 떼내어 쓰는 게 가능하다.**
		- 모델의 하위층은 지역적이고 일반적인 특성 맵을 추출한다.(선, 색, 질감)
		- 모델의 상위층은 좀 더 추상적인 개념을 추출한다(강아지 눈, 고양이 귀)

- 반면 분류기는 어떤 문제 상황(10개의 클래스 분류 등)에 특화되었고, 입력 이미지의 위치 정보를 담고 있지도 않아서 일반적으로 쓰기 어렵다.

- `ImageNet`의 경우 강아지와 고양이의 종류까지 포함하고 있어 완전 연결층을 써도 무방하나, 더 일반적인 경우를 다루기 위해 여기서는 쓰지 않는다.
- `ImageNet`으로 훈련된 모델은 `keras.applications`에 있음
	- Xception
	- ResNet
	- MobileNet
	- EfficientNet
	- DenseNet 등등

#### VGG16 모델 만들기
- 뒤에 붙는 숫자는 층의 깊이에 따른 버전이다.
- 16 = 합성곱 층 13개 + 밀집 연결 층 3개
- [케라스의 공식 문서](https://keras.io/api/applications) 참고.
```python
conv_base = keras.applications.vgg16.VGG16(
    weights = "imagenet",
    include_top = False,
    input_shape = (180, 180, 3)
)
```
- 매개변수
	- `weights` : 모델 초기화하는 가중치 체크포인트
	- `include_top` : `맨 위에 놓인` 밀집 연결 분류기를 사용할 것인가를 지정한다. 기본값은 1000개에 대응되는 밀집 연결 분류기 포함.
	- `input_sahpe` : 인풋 이미지 텐서 크기. 
		- 선택사항으로, 지정하지 않으면 어떤 크기의 입력도 처리할 수 있다.

- 이 상태에서 모델을 불러오면, `5, 5, 512`의 MaxPooling2D로 끝난다. 이 위에 밀집 연결 분류기를 놓는다.
- 이후 2가지 방식이 가능하다.
	1. 새로운 데이터셋에서 실행, 출력을 넘파이 배열로 디스크에 저장, 이 데이터를 독립된 밀집 연결 분류기에 입력.
		- 모든 이미지에 합성곱을 1번만 하면 되므로 **빠르고 비용이 적게 들지만, 데이터 증식을 사용할 수 없다.**
	2. `conv_base` 위에 `Dense`를 놓아 확장한 뒤, 엔드-투-엔드로 전체 모델을 실행한다.
		- 데이터 증식을 사용할 수 있지만, 비싸다.
- 두 방식을 모두 다룸.

#### 데이터 증식 사용하지 않는 빠른 특성 추출
```python
import numpy as np

def get_features_and_labels(dataset):
  all_features = []
  all_labels = []
  for images, labels in dataset:
  
    preprocessed_images = keras.applications.vgg16.preprocess_input(images)
    features = conv_base.predict(preprocessed_images)
    all_features.append(features)
    all_labels.append(labels)
  return np.concatenate(all_features), np.concatenate(all_labels)

train_features, train_labels = get_features_and_labels(train_dataset)
validation_features, validation_labels = get_features_and_labels(validation_dataset)
test_features, test_labels = get_features_and_labels(test_dataset)
```
- 이 상태의 데이터셋은 이미지와 레이블을 함께 담고 있다. 
- `vgg16.preprocess_input`으로 적절한 범위로 픽셀을 조정한 전처리된 입력을 기대한다.
	- `vgg16`은 `caffe`에서 훈련되어서, `BGR`로 채널을 바꾸고 `ImageNet` 데이터셋에서 구한 채널별 평균값 `[103.939, 116.779, 123.68]`을 뺀다.

- 추출된 특성의 크기는 `(samples, 5, 5, 512)`가 된다.

- 밀집 연결 분류기 정의 및 훈련
```python
# 밀집 연결 분류기 정의 & 훈련
inputs = keras.Input(shape = (5, 5, 512))
x = layers.Flatten()(inputs)
x = layers.Dense(256)(x)
x = layers.Dropout(0.5)(x)
outputs = layers.Dense(1, activation = 'sigmoid')(x)

model = keras.Model(inputs, outputs)
model.compile(loss = 'binary_crossentropy',
              optimizer = 'rmsprop',
              metrics = ['accuracy'])

callbacks = [keras.callbacks.ModelCheckpoint(
    filepath = 'feature_extraction.keras',
    save_best_only = True,
    monitor = 'val_loss'
)]

history = model.fit(train_features, train_labels, epochs = 20, validation_data = (validation_features, validation_labels), callbacks = callbacks)

visualize(history)
```

![[Pasted image 20230723133147.png]]
- 정확도가 바로 97~8%에 도달한다.
- 사전 훈련된 모델이 현재 주어진 작업에 딱 맞는 지식을 가졌기 때문에 이런 결과가 나오며, 모든 상황이 이렇지는 않다. 
	- 지금 예제는 데이터셋이 개, 고양이인데 `ImageNet`에도 그게 있었기 때문에 이런 결과가 나온다는 의미임. 실제로는 애매한 경우도 많을 것.

- 드롭아웃을 50% 썼음에도 훈련 시작과 동시에 과대적합이 나타나는데, `데이터 증식`을 사용하지 않았기 때문이다.

#### 데이터 증식을 사용한 특성 추출

> gpu를 쓸 수 있는 경우에만 사용한다. 불가능한 경우 방법 1을 쓴다.

- 느리고, 비용이 많이 들지만 데이터 증식을 쓸 수 있다.
- 우선, 합성곱 기반 층을 `동결freezing`해야 한다. 
	- 즉, 훈련 중 가중치가 업데이트되지 않게 막는다.
	- 동결하지 않으면 합성곱 기반 층에 의해 사전에 학습된 표현이 수정된다. `Dense`층은 랜덤 초기화이므로 매우 큰 가중치 업데이트 값이 네트워크에 전파되며, 이는 사전 학습된 모델 표현을 크게 훼손시킨다.
```python
# 합성곱 기반 층 동결하기
conv_base = keras.applications.vgg16.VGG16(
    weights = "imagenet",
    include_top = False
)

conv_base.trainable = False

# 조회
len(conv_base.trainable_weights)
```

- 이후 `데이터 증식`, `동결된 합성곱 기반 층`, `밀집 분류기`를 연결한다.
```python
data_augmentation = keras.Sequential(
    [
        layers.RandomFlip("horizontal"),
        layers.RandomRotation(0.1),
        layers.RandomZoom(0.2)
    ]

)

inputs = keras.Input(shape = (180, 180, 3))
x = data_augmentation(inputs)
x = keras.applications.vgg16.preprocess_input(x) # 입력 값의 스케일 조정
x = conv_base(x)
x = layers.Flatten()(x)
x = layers.Dense(256)(x)
x = layers.Dropout(0.5)(x)
outputs = layers.Dense(1, activation = 'sigmoid')(x)

model = keras.Model(inputs, outputs)
model.compile(loss = 'binary_crossentropy',
              optimizer = 'rmsprop',
              metrics = ['accuracy'])

callbacks = [keras.callbacks.ModelCheckpoint(
    filepath = 'feature_extraction_with_data_augmentation.keras',
    save_best_only = True,
    monitor = 'val_loss'
)]

history = model.fit(train_dataset,
                    epochs = 20,
                    validation_data = validation_dataset,
                    callbacks = callbacks)
```

```python
test_model = keras.models.load_model("feature_extraction_with_data_augmentation.keras")
test_loss, test_acc = test_model.evaluate(test_dataset)
print(f"테스트 정확도 : {test_acc :.3f}") # 0.976
```
- 데이터 증식을 사용하지 않는 경우와 비교해도 큰 차이가 없는건 쵸큼 실망스럽다.

### 사전 훈련 모델 미세 조정하기
- 특성 추출을 보완할 수 있다.
- 특성 추출에 사용했던 **동결 모델의 상위 층 몇 개를 동결에서 해제한 후, 모델에 새로 추가한 층과 함께 훈련시키는 것이다.**

- 네트워크 미세조정 단계
> 1. 사전 훈련 네트워크 위에 새로운 네트워크 추가  
> 2. 기반 네트워크 동결
> 3. 새로 추가한 네트워크 훈련
> 4. 기반 네트워크에서 일부 층 동결 해제 (주의 : `BatchNormalization`층은 해제하면 안됨. 이 예제에는 없다.)
> 5. 동결 해제한 층과 새로 추가한 층을 함께 훈련한다.

- 상위 3단계는 이미 위에서 했으며(동결 -> 네트워크 추가 -> 훈련), 4단계부터 ㄱㄱ

- 어떤 층까지를 동결 해제해야 하는가는 문제마다 다르다. 아래 사항을 고려하자.
	- **하위 층은 일반적이고 재사용 가능한 특성을 인코딩하고, 상위층은 문제 상황에 더 특화된 특성을 인코딩한다.** 하위 층으로 갈수록 미세조정 효과가 감소한다.
	- **훈련해야 할 파라미터가 많을수록 과대적합 위험이 커진다.** 이 예제의 경우 파라미터만 1500만개임.
- 그래서 최상위에서 2~3개의 층만 미세조정해준다.

- 이 예제는 마지막에서 4번째 층`block4_pool`까지를 동결한다.
```python
conv_base.trainable = True
for layer in conv_base.layers[:-4]:
  layer.trainable = False
```

- 모델 미세 조정
```python
# 모델 미세 조정
model.compile(loss = 'binary_crossentropy',
              optimizer = keras.optimizers.RMSprop(learning_rate = 1e-5),
              metrics =['accuracy'])
              
callbacks = [keras.callbacks.ModelCheckpoint(
    filepath = 'fine_tuning.keras',
    save_best_only = True,
    monitor = 'val_loss'
)]

history = model.fit(train_dataset,
                    epochs = 20,
                    validation_data = validation_dataset,
                    callbacks = callbacks)

model =  keras.models.load_model("fine_tuning.keras")
test_loss, test_acc = model.evaluate(test_dataset)
print(f"테스트 정확도 : {test_acc:.3f}")
```
- 결과는 98.5% 대로 +- 1% 정도 차이날 수 있다. 
	- 캐글 경연 대회의 최상위 결과에 해당하지만, 사전에 훈련된 다른 모델을 쓴 거니까 공정한 비교는 아니다.

