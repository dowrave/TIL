
- `예측` : 환경과의 상호작용을 통해 **주어진 정책에 대한 가치함수를 학습**함
	- `몬테카를로 예측`
	- `시간차 예측`
- `제어` : 가치함수를 토대로 정책을 끊임없이 발전, 최적 정책을 학습함
	- `살사`
	- `큐러닝`

## 강화학습과 정책 평가 1 : 몬테카를로 예측
- 강화학습 : 환경 모델 없이 **환경이라는 시스템의 입력과 출력 사이의 관계를 학습**함
	- `입력` : 에이전트의 `상태`, `행동`
	- `출력` : `보상`

- 강화학습은 **일단 해보고, 자신을 평가한 다음, 자신을 업데이트하는 과정을 반복**한다.

### 강화학습 예측과 제어
- 벨만 기대 방정식
$$
v_{\pi}(s) = E_{\pi}[R_{t+1} + \gamma v_{\pi}(S_{t+1}) | S_t = s]$$
- 정책 이터레이션은 현재 상태에서 가능한 모든 상황을 고려해서 $E_\pi$를 계산한다. 
- 그러나 사람의 판단은 항상 정확한 정보를 근거로 하지 않는다. 실제로는 적당한 추론을 통해 학습해나가는 것이 더 효율적이다.

- 정책 이터레이션의 정책 평가는 현 정책을 따랐을 때 참 가치함수를 구하는 과정이었다.
- 강화학습에서는 이 과정을 `예측Prediction`이라고 한다.
	- 마찬가지로, 정책 발전은 강화 학습에서 `제어Control`이라고 부른다.

### 몬테카를로 근사의 예시
- 원 넓이를 구하는 공식을 모르는 상태에서 원 넓이를 어떻게 구할 수 있을까?
- `몬테카를로 근사Monte-Carlo Approximation`
	- `몬테 카를로` = **무작위로 무언가를 해본다**
	- `근사` = `샘플`을 통해 원래의 값을 `추정`하는 것
		- 추정한 값은 빈도가 늘어날수록 원래 값에 근접해진다.

- 즉, 무작위로 점을 많이 찍어본 다음 원 안에 찍힌 점이 몇 개인지를 세는 것이 몬테카를로 근사이다. 시도한 횟수만 알면 유사한 넓이를 구할 수 있게 되는 것이다.

### 샘플링과 몬테카를로 예측
- 찍힌 점 하나하나를 `샘플` 이라고 하며, 그 샘플들의 평균으로 원의 넓이를 추정한다. 

- **몬테카를로 근사로 가치함수를 추정해보자.**
	- 가치함수 추정은 에이전트가 **한 환경에서 에피소드를 진행하는 것**이 `샘플링Sampling`이다. 샘플링을 통해 얻은 평균으로 참 가치함수의 값을 추정하는데, 이 때 몬테카를로 근사를 사용하는 것을 `몬테카를로 예측Monte-Carlo Prediction`이라고 한다.
	- `가치함수`는 현 상태로부터 받을 보상들을 감가시킨 다음, 기댓값을 계산한 것이었다.
		- 정책 이터레이션에서는 벨만 기대 방정식을 이용해 전체 문제를 여러 개의 작은 문제로 쪼개서 가치함수를 구했다. 
		- 그러나 정책 이터레이션은 기댓값을 `계산`한 것인데, 샘플의 평균으로 참 가치함수를 `예측`하는 방법은 무엇일까?

- 샘플링을 통해 정책에 따른 가치함수를 구하려면, **현 정책에 따라 계속 행동**해보면 된다. 현 정책에 따라 받은 보상들을 감가시켜 더한 값이 `반환값`이었다. 에피소드가 끝이 있다고 가정할 경우, `반환값`은 아래처럼 정의된다. 끝없는 에피소드는 반환값을 알기 어렵다.
$$
G_t = R_{t+1} + \gamma{R_{t+2}} + ... + \gamma^{T-t+1}R_{T}
$$
- $T$는 마침 상태를 의미하며, 1개의 에피소드는 원의 넓이를 구할 때 점을 1개 뿌리는 것과 비슷하다. 1번의 에피소드로 얻은 반환값으로는 추정이 불가능하다. 각 상태에 대한 반환값들이 많이 모여야 `몬테카를로 예측`이 가능하다. **반환값들의 평균으로 참 가치함수의 값을 추정**하기 떄문이다.

- 정책 이터레이션에서 벨만 기대 방정식을 구하려면 환경의 모델$P^a_{ss'}$과 $R_{t+1}$을 알아야 했다. 

- 반면, 몬테카를로 예측에서는 환경 모델을 알아야하는 $E_\pi$를 계산하지 않는다. 여러 에피소드를 통해 구한 **반환값의 평균**을 통해 $v_\pi(s)$를 추정한다.
$$
v_\pi(s) \sim \frac{1}{N(s)}{\overset{N(s)}{\underset{i=1} \Sigma}}G_i(s)
$$
- $N(s)$는 여러 에피소드 동안 상태 s를 방문한 횟수이다. 
- $G_i(s)$는 그 상태를 방문한 i번째 에피소드에서 s의 반환값이다. 
- 현 정책에 따라 무수히 에피소드를 진행하면 모든 상태에 대한 충분한 반환값들을 모을 수 있는데, 이들의 평균을 내면 상당히 정확한 가치함수의 값을 얻을 수 있다. 

- 아래는 상태에 대한 **표현 $s$를 생략한 수식의 전개**이다.
$$\begin{matrix}
V_{n+1} &=& \frac{1}{N}\overset{n}{\underset{i=1}{\Sigma} G_i} = \frac{1}{n}(G_n + \overset{n-1}{\underset{i=1}{\Sigma}} G_i)  \\ 
&=& \frac{1}{n}(G_n + (n-1)\frac{1}{(n-1)}\overset{n-1}{\underset{i=1}{\Sigma}} G_i)
\end{matrix}
$$
- 아래 수식에서 오른쪽 수식은 $V_{n}$ 이므로, 정리하면 아래처럼 된다.
$$
\begin{matrix}
&=& \frac{1}{n}(G_n + (n-1)V_n) \\
&=& V_n + \frac{1}{n}(G_n - V_n)
\end{matrix}
$$

- 마지막 수식은 몬테카를로 예측 중, 새로운 반환값이 들어와서 평균을 취할 때 어떻게 평균을 취하는지를 보여준다. 어떤 상태의 가치함수는 샘플링을 통해 에이전트가 그 상태를 방문할 때마다 업데이트하게 된다. 
$$
V(s) \leftarrow V(s) + \frac{1}{n}(G(s) - V(s))
$$
- 여기서 $G(s) - V(s)$ 를 `오차`라고 하며, $1/n$은 `스텝사이즈Stepsize`로, 오차 중 업데이트하는 비율을 말한다. 스텝사이즈 = 1이라면 $G(s)$가 $V(s)$를 대체하게 된다.

- 일반적으로 `스텝사이즈`는 $\alpha$로 표현한다. 
$$
V(s) \leftarrow V(s) + \alpha(G(s) - V(s))
$$
- $G(s)$ : 업데이트의 목표
- $\alpha(G(s) - V(s))$ : 업데이트의 크기


- **몬테카를로 예측 이후 모든 강화학습 방법에서, 가치함수 업데이트는 위 수식의 변형일 뿐이다.** 
- 가치함수 입장에서 업데이트 도달 목표는 `반환값`이다. 가치함수는 한 번에 목표점으로 가지 않고 스텝사이즈를 곱한 만큼만 간다.
- 스텝 사이즈는 앞으로 `상수`를 사용함.

몬테카를로 예측에서 에이전트는 위 업데이트 식으로 `에피소드 동안 경험한 모든 상태`에 대해 **가치함수를 업데이트**한다. 이게 반복되면 가치함수는 `현 정책에 대한` 참 가치함수에 수렴한다.
- 나중에 나올 `시간차 예측Temporal-Difference Prediction` 방법에서는 1번 업데이트의 목표만 변하고 나머지는 동일하다.
이렇게 하나의 에피소드가 끝나고 지나온 모든 상태의 가치함수를 업데이트한 다음, 에이전트는 다시 시작 상태부터 새로운 예측을 진행한다.

## 강화학습과 정책 평가 2: 시간차 예측
- `시간차Temporal-Difference` : 강화학습의 가장 중요한 아이디어 중 하나.
	- `몬테카를로 예측`의 단점은 실시간 업데이트가 아니라는 점이다. 에피소드가 끝난 다음에야 업데이트가 들어간다. 그래서 **에피소드의 끝이 없거나, 길이가 긴 경우 몬테카를로 예측은 적합하지 않다.**

- 각 타임스텝마다 가치함수를 업데이트하는 방법이 `시간차`이다. 
- 몬테카를로 예측에서는 가치함수의 정의를 계산하지 않고 샘플링을 통해 예측했다면, `시간차 예측`에서는 가치함수의 아래 식을 이용한다.
$$
\begin{matrix}
v_\pi(s) &=& E[G_t|S_t = s] \\
&=& E[R_{t+1} + \gamma v_\pi (S_{t+1}) | S_t = s]
\end{matrix}
$$

- `시간차`는 한 번에 하나의 가치함수만을 업데이트한다. 업데이트는 아래 식으로 이뤄진다.
$$
V(S_t) \leftarrow V(S_t) + \alpha(R + \gamma V(S_{t+1}) - V(S_t)) 
$$
> 1. $R + \gamma V(S_{t+1})$ : 업데이트의 목표
> 2. $\alpha(R + \gamma V(S_{t+1}) - V(S_t))$ : 업데이트의 크기

- 특히 2번은 `시간차 에러Temporal-Difference Error`라고 한다. 시간차 예측에서 업데이트의 목표는 실제 값이 아니다. $V(S_{t+1})$은 현재 에이전트가 가진 값으로, 에이전트가 예측하고 있는 $S_{t+1}$의 가치함수이다. 
- 다른 상태의 가치함수 예측값으로 지금 상태의 가치함수를 예측하는 방식을 `부트스트랩Bootstrap`이라고 한다. 즉, 업데이트 목표가 정확하지 않은 상황에서 가치함수를 업데이트하는 것이다.

- **시간차 예측의 샘플이 충분히 많다면 참 가치함수에 수렴하며, 많은 경우 몬테카를로 예측보다 더 효율적이다.**

## 강화학습 알고리즘 1 : 살사

### 살사
- `강화학습 알고리즘`에서, 정책 이터레이션과 가치 이터레이션은 `살사`로 발전한다고 했다. `살사`부터 강화학습이라고 한다.

- `정책 이터레이션` : 정책 평가와 발전을 번갈아가면서 실행함. 
	- `정책 평가` : 벨만 기대 방정식으로 현 정책의 가치함수를 구함
	- `정책 발전` : 구한 가치함수로 정책을 업데이트함

- `GPI : Generalized Policy Iteration` : 정책 평가와 발전을 번갈아가면서 실행하면 최적 가치함수에 가치함수가 수렴하게 되는 정책 이터레이션.
	- `GPI`에서는 단 1번만 정책을 평가해서 가치함수를 업데이트하고, 바로 정책을 발전시킨다.
	- GPI의 정책 평가 과정을 수행하는 게 `몬테카를로 예측`이나 `시간차 예측`이다.

- `시간차 방법`은 타임스텝마다 가치함수를 현 상태에 대해서만 업데이트한다. 따라서 GPI처럼 모든 상태의 정책을 발전시킬 수 없다.
- 이 때 `가치 이터레이션`의 방식을 이용한다 (별도의 정책 없이 가치함수에 대해 탐욕적으로 움직인다. )
	- 에이전트는 별도의 정책을 두지 않고, 현 상태에서 가장 큰 가치를 지니는 행동을 선택한다. 
	- 이렇게 `시간차 예측` + `탐욕 정책` => `시간차 제어`라고 한다.

| GPI            | 시간차 제어 |
| -------------- | ----------- |
| 정책 평가      | 시간차 예측 |
| 탐욕 정책 발전 | 탐욕 정책            |


- **시간차 제어에서의 탐욕 정책**
- GPI의 정책 발전 식은 아래와 같다.
$$
\pi'(s) = argmax_{a \in A} [R^a_s + \gamma P^a_{ss'}V(s')]
$$
- 환경의 모델을 모르기 때문에, 위 식은 `시간차 제어`의 탐욕 정책으로 사용할 수 없다. 
- 탐욕 정책에서, 다음 상태의 가치함수를 보고 판단하지 않고 현상태의 큐함수를 보고 판단한다면 환경의 모델을 몰라도 된다. 
- 큐함수의 탐욕 정책의 수식은 아래와 같다.
$$
\pi(s) = argmax_{a \in A}Q(s, a)
$$

- 큐함수에 따라 행동을 선택하려면 에이전트는 큐함수의 정보를 알아야 한다. 따라서 시간차 제어에서 업데이트 대상은 `큐함수`가 되어야 하는데, 이 때 시간차 제어의 수식은 아래와 같다.

$$
Q(S_t, A_t) \leftarrow Q(S_t, A_t) + \alpha(R + \gamma Q(S_{t+1}, A_{t+1}) - Q(S_t, A_t)))
$$

- 다음 상태의 큐함수 $Q(S_{t+1}, A_{t+1})$ 를 알려면, 다음 상태 $S_{t+1}$에서 다음 행동 $A_{t+1}$까지 선택해야 한다. 
- `시간차 제어`에서 큐함수를 업데이트하려면 $[S_t, A_t, R_{t+1}, S_{t+1}, A_{t+1}]$ 이라는 샘플을 사용한다.
	- $S_t$ 상태에서 탐욕 정책에 따라$A_t$ 를 선택하여 한 타임스텝을 진행한다. 환경은 보상 $R_{t+1}$을 주고, 다음 상태 $S_{t+1}$을 알려준다. 에이전트는 다시 행동 $A_{t+1}$을 선택한다.
	- 위 과정으로 하나의 샘플이 생성되면, 이를 통해 큐함수를 업데이트 한다.

- 위 과정에서 `SARSA`가 사용되기 떄문에 **시간차 제어를 다른 말로 `SARSA`라고 부른다.** 
	- `SARSA`는 큐함수를 토대로 샘플을 탐욕 정책으로 모은 뒤, 그 샘플로 방문한 큐함수를 업데이트 하는 과정을 반복한다.

충분히 많은 경험을 한 경우 탐욕 정책이 좋은 선택이 되지만, **초기 에이전트는 잘못된 학습이 될 가능성이 높다.** 이 문제를 강화학습의 중요한 문제로 `탐험Exploration`의 문제가 된다.

따라서 탐욕 정책을 대체할 새로운 정책이 필요한데, **$\varepsilon$ - 탐욕 정책**이라는 게 있다. 간단한 아이디어로, $\varepsilon$ 확률로 탐욕적이지 않은 행동을 선택하게 하는 것이다.

$$
\pi(s) = \begin{cases} a^* = argmax_{a \in A}Q(s, a) & 1-\varepsilon \\ 
a \ne a^* & \varepsilon

\end{cases}
$$
- $1-\varepsilon$ 확률로 탐욕 정책을 따르고, $\varepsilon$ 확률로 엉뚱한 행동을 한다.

최적의 큐함수를 찾더라도 $\varepsilon$ 확률로 탐험한다는 한계가 있어서 학습 진행에 따라 낮춰가는 알고리즘도 있지만, `살사`와 `큐러닝`에서는 일정한 값으로 고정한 정책을 사용한다.  

- 따라서 살사는 아래와 같다.
1. GPI의 정책 평가 -> **큐함수를 이용한 시간차 예측**
2. 탐욕 정책 발전 -> **$\varepsilon$ - 탐욕 정책**

- 살사를 간단히 두 단계로 생각하면,
1. $\varepsilon$ - 탐욕 정책으로 샘플 $[S_t, A_t, R_{t+1}, S_{t+1}, A_{t+1}]$ 을 획득
2. 획득한 샘플로 다음 식을 통해 큐함수 $Q(S_t, A_t)$를 업데이트
$$
Q(S_t, A_t) \leftarrow Q(S_t, A_t) + \alpha(R + \gamma Q(S_{t+1}, A_{t+1}) - Q(S_t, A_t)))
$$

- `살사`와 `큐러닝`에서의 그리드월드 예제의 경우, 환경의 보상이 100과 -100이다. 

### 코드 보기

#### SARSAgent - init
```python
class SARSAgent:
    def __init__(self, actions):
        self.actions = actions # [0, 1, 2, 3] (상하좌우)
        self.learning_rate = 0.01 # 스텝사이즈
        self.discount_factor = 0.9
        self.epsilon = 0.1 # 입실론-탐욕 정책
        self.q_table = defaultdict(lambda: [0.0, 0.0, 0.0, 0.0])
```
- `self.q_table`
	- 모든 상태의 가치함수를 벨만 방정식으로 한꺼번에 업데이트하는 DP와 달리, `살사`는 에이전트가 어떤 상태를 방문하면 그 상태의 큐함수만 업데이트한다. 
	- 따라서 어떤 상태의 큐함수를 담을 자료구조가 필요하며, `defaultdict`로 초기값을 같이 설정해준다. **`lambda`를 이용하면 `dict` 자료형의 기본값을 설정해줄 수 있다.**
	- 따라서 `self.q_table`에는 에이전트가 방문한 상태들의 큐함수가 저장되며, 저장된 큐함수는 `self.q_table[상태][행동]`으로 접근할 수 있다.

- 필요한 함수를 알려면 에이전트와 환경 간의 상호작용을 알아야 한다. 에이전트는 아래 순서대로 환경과 상호작용한다.
> 1. 현재 상태에서 $\varepsilon$ - 탐욕 정책에 따라 행동 선택
> 2. 선택한 행동으로 한 타임스텝 진행
> 3. 환경으로부터 보상 & 다음 상태를 받음
> 4. 다음 상태에서  $\varepsilon$ - 탐욕 정책으로 다음 행동 선택
> 5. (s, a, r, s', a')을 통해 큐함수를 업데이트

#### get_action
```python
    # 입실론 탐욕 정책에 따라서 행동을 반환
    def get_action(self, state):
        if np.random.rand() < self.epsilon:
            action = np.random.choice(self.actions)
        else:
            # 큐함수에 따른 행동 반환
            state_action = self.q_table[state]
            action = self.arg_max(state_action)
        return action
```
- 입실론 탐욕 정책에 따라, 난수가 입실론보다 작으면 아무런 행동이나 하고, 크다면 가장  가치가 큰 행동을 한다. 

#### learn
```python
    # <s, a, r, s', a'>의 샘플로부터 큐함수를 업데이트
    def learn(self, state, action, reward, next_state, next_action):
        current_q = self.q_table[state][action]
        next_state_q = self.q_table[next_state][next_action]
        new_q = (current_q + self.learning_rate *
                (reward + self.discount_factor * next_state_q - current_q))
        self.q_table[state][action] = new_q

```
- 현재와 다음 상태의 행동을 선택, 샘플을 얻으면 에이전트는 학습을 진행한다. 그 코드.
$$
Q(S_t, A_t) \leftarrow Q(S_t, A_t) + \alpha(R + \gamma Q(S_{t+1}, A_{t+1}) - Q(S_t, A_t)))
$$
- 위 수식을 구현한다. 

- `get_action`과 `learn` 메서드는 메인 함수에서 아래처럼 작동한다.
```python
            # 행동을 취한 후 다음상태 보상 에피소드의 종료 여부를 받아옴
            next_state, reward, done = env.step(action)
            # 다음 상태에서의 다음 행동 선택
            next_action = agent.get_action(str(next_state))

            # <s,a,r,s',a'>로 큐함수를 업데이트
            agent.learn(str(state), action, reward, str(next_state), next_action)

            state = next_state
            action = next_action
```

## 강화학습 알고리즘 2 : 큐러닝

### SARSA의 한계
- `그리드월드` 예제에서, (2, 3)에 초록색 삼각형이 있는 상황에서 (1,3)에 에이전트가 간 뒤, `탐험`에 의해 에이전트가 아래로 이동했다고 가정해보자.
$$
Q(S_t, A_t) \leftarrow Q(S_t, A_t) + \alpha(R + \gamma Q(S_{t+1}, A_{t+1}) - Q(S_t, A_t)))
$$
- `(1, 2)`를 s, `(1, 2) -> (1, 3)`이동을 a, `(1, 3)`을 s', `(1, 3) -> (2, 3)`이동을 a'이라고 하자.
- $Q(s', a')$는 초록색 삼각형인 보상 `-1`로 가는 행동의 큐함수이므로, 값이 낮다. **따라서 $Q(s', a')$와 평균을 취하는 업데이트로 $Q(s, a)$의 값도 낮아지게 된다.**  

- 그 다음 에피소드에서 에이전트는 (1,2)에서 (1,3)으로 이동하지 않으려고 한다.  $Q(s, a)$의 값이 낮아졌기 때문으로, 에이전트가 갇혀버리는 현상이 발생한다. (`sarsa` 코드를 돌려보면 바로 알 수 있음)

`살사`는 `온 폴리시 시간차 제어 On-Policy Temporal Difference Control`이다. 즉, 자신이 행동하는 대로 학습하는 시간차 제어이다. 탐험 때문에 입실론을 넣었지만, 에이전트는 오히려 잘못된 정책을 학습하게 된다. 

- 그러나 강화학습에서 `탐험`은 절대적으로 필요하기 떄문에, 이러한 딜레마를 해결하기 위해 사용하는 것이 `오프 폴리시 시간차 제어`로, 다른 말로는 `큐러닝Q-learning`이라고 한다.

### 큐러닝 이론
**현재 행동하는 정책과, 학습하는 정책을 구분한다.** 즉, 현재 에이전트는 현재 정책으로 지속적인 탐험을 하지만, 학습은 목표 정책에 따라서 한다. `살사`는 따로 정책이 없었고 현재 큐함수에 따라 행동을 선택했는데, `큐러닝`은 오프폴리시를 어떻게 구현했을까?
위 SARSA의 한계인 상황을 똑같이 놓고 비교해보자. 입실론 탐욕 정책으로 현 상태 s에서 행동 a를 취한 뒤, 환경으로부터 보상 r을 받고 다음 상태 s를 받는다. 여기까진 똑같다.
- `SARSA`는 다음 상태 `s'`에서 다시 입실론 탐욕 정책으로 다음 행동을 선택하고, 이를 다시 학습 샘플로 사용한다.
- **`큐러닝`은 에이전트가 다음 상태 `s'`를 알게 되면, 그 상태에서 가장 큰 큐함수를 현재 큐함수의 업데이트에 사용한다. 이 업데이트는 그 다음 상태에서 에이전트가 어떤 행동을 했는가와는 별개이다!**

따라서 `큐러닝`에서 현 상태의 큐함수를 업데이트하기 위해 필요한 샘플은 `s, a, r, s'`이다. 이를 수식으로 나타내면 아래와 같다.
$$
Q(S_t, A_t) = Q(S_t, A_t) + \alpha(R_{t+1} + \gamma \, \underset{a'}{max}Q(S_{t+1}, a') - Q(S_t, A_t))
$$

위 수식은 `벨만 최적 방정식`과 비슷하다.
$$
q_*(s,a) = E[R_{t+1} + \gamma\, \underset{a'}{max}q_*(S_{t+1}, a') | S_t = s, A_t = a]
$$
큐러닝에서 보상 $R_{t+1}$은 실제 에이전트가 환경에서 받는 값이므로, 기댓값을 뺴면 위 수식과 동일하다.

따라서, **`살사`에서는 큐함수 업데이트에 `벨만 기대 방정식`을 사용하고
`큐러닝`에서는 큐함수 업데이트에 `벨만 최적 방정식`을 사용한다.**

큐러닝 학습은 다음 상태 s'에서 선택한 행동이 안 좋은 행동이더라도, 현재 상태 s를 업데이트할 때 포함되지 않는다. 학습에 포함된 행동은 가장 큰 큐함수였지만, 실제 행동은 학습에 포함된 행동과는 달랐기 때문이다. 

큐러닝은 아래의 특징을 지닌다.
1. 살사의 딜레마인 `탐험 vs 최적` 정책 학습의 문제를 `정책 분리`로 해결했다.
2. 행동 선택은 입실론 - 탐욕 정책
3. 업데이트는 `벨만 최적 방정식`을 이용했다.
4. 간단하기 떄문에 많은 강화학습 알고리즘의 토대가 됐다.

### 코드 설명
```python
    # <s, a, r, s'> 샘플로부터 큐함수 업데이트
    def learn(self, state, action, reward, next_state):
        q_1 = self.q_table[state][action]
        # 벨만 최적 방정식을 사용한 큐함수의 업데이트
        q_2 = reward + self.discount_factor * max(self.q_table[next_state])
        self.q_table[state][action] += self.learning_rate * (q_2 - q_1)
```
- 크게 다른 부분은 에이전트의 학습 부분이다.
$$
Q(S_t, A_t) = Q(S_t, A_t) + \alpha(R_{t+1} + \gamma \, \underset{a'}{max}Q(S_{t+1}, a') - Q(S_t, A_t))
$$
- `self.q_table[next_state]`의 `max` 값만을 업데이트에 사용하므로 오프 폴리시가 되며, 다음 상태의 행동도 알 필요가 없다.


### 큐러닝 코드 실행 결과
- `살사`와 `큐러닝`의 차이는 온폴리시 vs 오프폴리시의 차이이다.
- `살사`는 지속적인 탐험으로 인해, 왼쪽 상단의 구석에 갇히게 된다. `s'`의 행동 `a'`으로 인한 영향이 `s, a`에도 미치게 되고, 그 값이 아주 작지만(-0.01) 그 값으로 인해 `s`에서의 행동 `a`를 선택하는 비중이 확 낮아진다. 그리디한 알고리즘이었기 때문.
- `큐러닝`은 항상 다음 `상태`의 `행동 a'`에 관계 없이 항상 `큐함수 중 맥스값`으로 업데이트 하므로 갇히지 않게 된다.