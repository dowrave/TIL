
1. [[#다이나믹 프로그래밍과 그리드월드|다이나믹 프로그래밍과 그리드월드]]
	1. [[#다이나믹 프로그래밍과 그리드월드#순차적 행동 결정 문제|순차적 행동 결정 문제]]
		1. [[#순차적 행동 결정 문제#다이나믹 프로그래밍|다이나믹 프로그래밍]]
		2. [[#순차적 행동 결정 문제#그리드월드 예제|그리드월드 예제]]
2. [[#다이나믹 프로그래밍 1: 정책 이터레이션|다이나믹 프로그래밍 1: 정책 이터레이션]]
	1. [[#다이나믹 프로그래밍 1: 정책 이터레이션#정책 이터레이션|정책 이터레이션]]
		1. [[#정책 이터레이션#정책 평가|정책 평가]]
		2. [[#정책 이터레이션#정책 발전|정책 발전]]
	2. [[#다이나믹 프로그래밍 1: 정책 이터레이션#전체 코드|전체 코드]]
	3. [[#다이나믹 프로그래밍 1: 정책 이터레이션#개별 코드 설명|개별 코드 설명]]
		1. [[#개별 코드 설명#PolicyIteration 클래스|PolicyIteration 클래스]]
			1. [[#PolicyIteration 클래스#__init__|__init__]]
			2. [[#PolicyIteration 클래스#policy_evaluation|policy_evaluation]]
			3. [[#PolicyIteration 클래스#policy_improvement|policy_improvement]]
			4. [[#PolicyIteration 클래스#get_action|get_action]]
			5. [[#PolicyIteration 클래스#get_policy, get_value|get_policy, get_value]]
		2. [[#개별 코드 설명#정책 이터레이션 코드 실행|정책 이터레이션 코드 실행]]
3. [[#다이나믹 프로그래밍 2: 가치 이터레이션|다이나믹 프로그래밍 2: 가치 이터레이션]]
	1. [[#다이나믹 프로그래밍 2: 가치 이터레이션#명시적인 정책과 내재적인 정책|명시적인 정책과 내재적인 정책]]
	2. [[#다이나믹 프로그래밍 2: 가치 이터레이션#벨만 최적 방정식과 가치 이터레이션|벨만 최적 방정식과 가치 이터레이션]]
	3. [[#다이나믹 프로그래밍 2: 가치 이터레이션#코드로 보기|코드로 보기]]
		1. [[#코드로 보기#Value Iteration 클래스 설명|Value Iteration 클래스 설명]]
			1. [[#Value Iteration 클래스 설명#value_iteration|value_iteration]]
			2. [[#Value Iteration 클래스 설명#get_action|get_action]]
4. [[#DP의 한계와 강화학습|DP의 한계와 강화학습]]
	1. [[#DP의 한계와 강화학습#DP의 한계|DP의 한계]]
	2. [[#DP의 한계와 강화학습#모델 없이 학습하는 강화학습|모델 없이 학습하는 강화학습]]


`다이나믹 프로그래밍DP` : 작은 문제가 큰 문제 안에 중첩된 경우, 작은 문제의 답을 다른 작은 문제에서 이용하는 것.
`정책 이터레이션` : 다이나믹 프로그래밍으로 벨만 기대 방정식을 풂
`가치 이터레이션` : 다이나믹 프로그래밍으로 벨만 최적 방정식을 풂

- 다이나믹 프로그래밍은 이후 강화학습의 근간이 되었으며, DP의 한계를 극복하는 게 `강화학습`이다.

## 다이나믹 프로그래밍과 그리드월드

### 순차적 행동 결정 문제
- 순차적 행동 결정 문제를 푸는 방법은 `MDP 정의 -> 벨만 방정식 계산 -> 최적 가치함수 + 최적 정책`을 얻는 과정이었음. [[2. 강화학습 기초 1 - MDP와 벨만 방정식]]에서는 `MDP 정의`를 다뤘으며 여기서는 `벨만 방정식 계산 + 최적 가치함수 & 정책 얻기`를 진행함.

- 벨만 방정식을 푼다 = 아래 `벨만 최적 방정식`에서 $v_*$을 찾는 과정이다
$$
v_*(s) = \underset{a}{max}\,E[R_{t+1} + \gamma v_*(S_{t+1}) | S_t = s, A_t = a]
$$

#### 다이나믹 프로그래밍
- `강화 학습` 전에 `벨만 최적 방정식`을 푸는 알고리즘이었다. 벨만 씨가 벨만 방정식도 만들었지만 다이나믹 프로그래밍도 처음 제시했음. 
- `다이나믹`이란, 그 말이 가리키는 대상이 시간에 따라 변함을 의미한다.
- `프로그래밍`이란, 컴퓨터 프로그래밍이 아니라 `계획`으로, 여러 프로세스가 다단계로 이뤄지는 것을 말한다.  

 다이나믹 프로그래밍의 기본 아이디어는 작은 문제가 큰 문제 내에 중첩되었다면, 작은 문제로 쪼개서 푼다는 것이다. 작은 문제가 `개별 프로세스`가 되는 것이다. 작은 문제의 답을 서로 이용할 수 있고, 이를 이용하면 전체 계산량을 줄일 수 있다. 
 $V_k(S) -> V_{k+1}(S)$ 이라는 문제는 결국 $V_{\pi}(S)$를 구하는 문제로 생각할 수 있다. 그런데 이를 바로 구할 수 없기 떄문에 **$V_0(S$) 부터 각 타임스텝의 모든 상태의 가치함수를 업데이트하는 식으로 이뤄진다.**  그리고 $V_0(S)$는 $V_1(S)$의 가치함수를 업데이트하는 데 사용되고 이를 반복하는 방식이다.
 
 >결국 이전 타임스텝에서의 가치함수들을 다음 타임스텝에서의 가치함수를 계산하는 데 사용하고, 이를 반복하면 정책에서의 참값을 구할 수 있다는 의미가 `다이나믹 프로그래밍`인 것 같다.
 
`정책 이터레이션` : 다이나믹 프로그래밍으로 벨만 기대 방정식을 풂
`가치 이터레이션` : 다이나믹 프로그래밍으로 벨만 최적 방정식을 풂

#### 그리드월드 예제
(0, 0)에 빨간 사각형, (2, 1)과 (1, 2)에 초록색 삼각형, (2, 2)에 파란 사각형, 격자 크기는 (5,5)이다. 초록색 삼각형에 닿으면 -1, 파란색 사각형에 닿으면 +1의 보상을 얻는다고 할 때, 파란색에 도착하는 최적 정책을 찾고 싶은 상황이다.


## 다이나믹 프로그래밍 1: 정책 이터레이션

- 강화학습 알고리즘의 흐름은 다음과 같다.

> 1. 순차적 행동 결정 문제 제시  
> 2. MDP 정의
> 3-1) 벨만 기대 방정식 -> 정책 이터레이션
> 3-2) 벨만 최적 방정식 -> 가치 이터레이션
> 4. (3-1 + 3-2) `살사`
> 5. `큐러닝`

- 순차적 행동 결정 문제는 MDP를 통해 수학적으로 정의될 수 있으며, MDP로 정의 시 목표는 에이전트가 받을 보상의 합을 최대로 하는 것이다. 에이전트는 가치함수를 통해 이 목표에 얼마나 다가갔는지를 알 수 있다. **가치함수 = 보상의 합에 대한 기댓값**이기 때문이다. 이 가치함수에 대한 방정식이 `벨만 방정식`이고, 이는 `벨만 기대 방정식`과 `벨만 최적 방정식`으로 나뉜다.

- `벨만 방정식`은 `DP`로 풀 수 있다. `정책 이터레이션 + 가치 이터레이션`은 `살사SARSA`로 발전하며, `살사`는 `오프폴리시Off-Policy` 방법으로 변형되어 `큐러닝Q-Learning`으로 이어진다.

### 정책 이터레이션
- `정책`은 에이전트가 모든 상태에서 어떻게 행동할지에 대한 정보이다. 가장 높은 보상을 얻는 정책을 찾는 것이 목표이지만, 처음에는 정책을 알 수 없다. 따라서 어떤 특정 정책을 시작하는데, 시작은 `무작위`로 행동을 정한다.
- 정책을 발전시키기 위해서, 이를 `평가`하는 방법이 필요하다. 정책 이터레이션에서는 `정책 평가Policy Evaluation`과 `정책 발전Policy Improvement`이 있다. 

#### 정책 평가
- `가치함수` = 정책이 얼마나 좋은지 판단하는 근거가 된다. 
- 2장에서 가치함수를 다뤘던 수식을 가져오면 아래와 같다.
$$
v(s) =E[R_{t+1} + \gamma R_{t+2} + \gamma^2 R_{t+3} + \gamma^3 R_{t+4} + ... | S_t = s]
$$
이 함수는 계산은 가능하지만 먼 미래를 고려할수록 고려해야 할 경우의 수가 기하급수적으로 늘어난다. `DP`는 이러한 계산량의 문제를 해결해주는데, 이게 바로 `벨만 기대 방정식`이다. 
즉, **먼 미래까지 고려하는 대신에 현재 주변 상태의 가치함수와 한 타임스텝의 보상만을 고려해서 현 상태의 다음가치함수를 계산**할 수 있다. 이 과정은 한 타임스텝의 보상만 고려하고, 주변 상태의 가치함수도 참 가치함수가 아니지만, 이를 반복하면 참 값으로 수렴하게 된다.
$$
v_{\pi}(s) = E_{\pi}[R_{t+1} + \gamma v_{\pi}(S_{t+1}) | S_t = s]
$$
기댓값이므로, 위 수식을 계산 가능한 형태로 나타내면 아래와 같다.
$$
v_\pi(s) = \Sigma_{a \in A}\pi(a|s)(R_{t+1} + \gamma v_\pi(s'))
$$
- 정책 평가는 $\pi$라는 정책에 대해 반복적으로 수행한다. 따라서 계산 단계를 표현할 새로운 변수 $k = 1, 2, 3, 4, ...$를 설정한다. $k$ 번째 가치 함수로 $k+1$번째 가치 함수를 계산하는 식은 아래와 같다.
$$
v_{k+1}(s) = \Sigma_{a \in A}\pi(a|s)(R_s^{a} + \gamma v_k(s'))
$$

- 2장에서 계산한 현 상태의 가치함수 예제가 있었다. 그 계산은 이러한 요소들이 고려되었음
	- `어떤 행동을 선택할 확률` (동서남북 무작위면 각각 1/4)
	- `감가율` (0.9로 고정)
	- `보상` (오른쪽 이동만 1, 나머지는 0)
	- `주변 상태의 가치함수`(현재의 왼쪽 1, 아래쪽 0.5, 나머지 0)
	- `상태 변환 확률`(모든 상태 변환에 대해 1 : 즉 어떤 행동을 하면 반드시 상태가 변한다)

- 위 계산은 한 타임스텝에서 모든 상태(그리드월드라면 5x5 칸 전체)에 대해 동시에 진행한다. 
- 정책 계산은 위 수식에 모두 포함되어있으며, 타임스텝을 무한히 돌리면 참값인 $v_\pi$ 에 접근할 수 있다.

#### 정책 발전
- 방법이 정해져 있는 건 아니지만, `탐욕 정책 발전 Greedy Policy Improvement`이 있다. 정책이 모든 상태에 대해 정의되어 있기 떄문에, 탐욕 정책 발전도 모든 상태에 적용한다.
- 그리드월드 예제에서, 초기 행동은 무작위이기 때문에 각각을 이렇게 표현할 수 있다.
$$
\pi(action|s) = 0.25
$$
	- `action`은 상하좌우가 되겠쥬?

- **정책 평가로 구한 것은 에이전트가 정책을 따랐을 때, 모든 상태의 가치함수에 대한 것**이다. 이 가치함수로 각 상태에 대해, `어떤 행동을 하는 것이 좋은가?`를 어떻게 알 수 있을까?

- `큐함수`를 이용하면 어떤 행동이 좋은가를 알 수 있다.
$$
\begin{matrix}
q_{\pi}(s, a) &=& E_{\pi}[R_{t+1} + \gamma v_{\pi}(S_{t+1}) | S_t = s, A_t = a] \\
&=& R_s^a + \gamma v_\pi(s')
\end{matrix}
$$

- 에이전트가 해야할 일은, **상태 s에서 선택 가능한 행동의 $q_\pi(s, a)$를 비교한 다음, 가장 큰 큐함수를 가지는 행동을 선택하는 것이다.**  이를 `탐욕 정책 발전`이라고 한다. 보이는 것중 가장 큰 것을 선택한다는 의미에서 이런 이름이 붙는다. 
- `탐욕 정책 발전`으로 업데이트된 정책은 아래 수식을 갖는다. `argmax`는 가장 큰 큐함수를 갖는 **행동을 반환**한다.
$$
\pi'(s) = argmax_{a \in A}(q_\pi(s, a))
$$
- 탐욕 정책 발전으로 정책을 업데이트하면, 업데이트된 정책으로 움직였을 때 받는 가치함수가 무조건 크거나 같다. DP는 탐욕 정책 발전을 사용 시, 단기적으로 무조건 이익을 본다. 장기적으로는 가장 큰 값을 가지는 최적 정책에 수렴할 수 있다.

### 전체 코드
- [소스 코드 위치](https://github.com/rlcode/reinforcement-learning-kr/)

> 코드 보는거는 위에 나열된 수식을 어떻게 코드로 구현하는가 위주로 보자! (보통 그게 애매한 경우가 많으니까)

- 이 예제의 경우 환경을 직접 만들기 떄문에 `environment.py`가 있지만, 일반적으로 강화학습은 이미 구축된 환경에 에이전트만 생성하므로 `에이전트 파일`만 있어도 된다. 
- 환경이 어떻게 구성되어 있는지 살펴보는 게 좋다. 

- `policy_iteration.py` 코드 설명
```python
# -*- coding: utf-8 -*-
import random
from environment import GraphicDisplay, Env

class PolicyIteration:
    def __init__(self, env):
        # 환경에 대한 객체 선언
        self.env = env
        # 가치함수를 2차원 리스트로 초기화
        self.value_table = [[0.0] * env.width for _ in range(env.height)]
        # 상 하 좌 우 동일한 확률로 정책 초기화
        self.policy_table = [[[0.25, 0.25, 0.25, 0.25]] * env.width
                                    for _ in range(env.height)]
        # 마침 상태의 설정
        self.policy_table[2][2] = []
        # 감가율
        self.discount_factor = 0.9

    def policy_evaluation(self):

        # 다음 가치함수 초기화
        next_value_table = [[0.00] * self.env.width
                                    for _ in range(self.env.height)]

        # 모든 상태에 대해서 벨만 기대방정식을 계산
        for state in self.env.get_all_states():
            value = 0.0
            # 마침 상태의 가치 함수 = 0
            if state == [2, 2]:
                next_value_table[state[0]][state[1]] = value
                continue

            # 벨만 기대 방정식
            for action in self.env.possible_actions:
                next_state = self.env.state_after_action(state, action)
                reward = self.env.get_reward(state, action)
                next_value = self.get_value(next_state)
                value += (self.get_policy(state)[action] *
                          (reward + self.discount_factor * next_value))

            next_value_table[state[0]][state[1]] = round(value, 2)

        self.value_table = next_value_table

    # 현재 가치 함수에 대해서 탐욕 정책 발전
    def policy_improvement(self):
        next_policy = self.policy_table
        for state in self.env.get_all_states():
            if state == [2, 2]:
                continue
            value = -99999
            max_index = []
            # 반환할 정책 초기화
            result = [0.0, 0.0, 0.0, 0.0]

            # 모든 행동에 대해서 [보상 + (감가율 * 다음 상태 가치함수)] 계산
            for index, action in enumerate(self.env.possible_actions):
                next_state = self.env.state_after_action(state, action)
                reward = self.env.get_reward(state, action)
                next_value = self.get_value(next_state)
                temp = reward + self.discount_factor * next_value

                # 받을 보상이 최대인 행동의 index(최대가 복수라면 모두)를 추출
                if temp == value:
                    max_index.append(index)
                elif temp > value:
                    value = temp
                    max_index.clear()
                    max_index.append(index)

            # 행동의 확률 계산
            prob = 1 / len(max_index)

            for index in max_index:
                result[index] = prob

            next_policy[state[0]][state[1]] = result

        self.policy_table = next_policy

    # 특정 상태에서 정책에 따른 행동을 반환
    def get_action(self, state):
        # 0 ~ 1 사이의 값을 무작위로 추출
        random_pick = random.randrange(100) / 100

        policy = self.get_policy(state)
        policy_sum = 0.0
        # 정책에 담긴 행동 중에 무작위로 한 행동을 추출
        for index, value in enumerate(policy):
            policy_sum += value
            if random_pick < policy_sum:
                return index

    # 상태에 따른 정책 반환
    def get_policy(self, state):
        if state == [2, 2]:
            return 0.0
        return self.policy_table[state[0]][state[1]]

    # 가치 함수의 값을 반환
    def get_value(self, state):
        # 소숫점 둘째 자리까지만 계산
        return round(self.value_table[state[0]][state[1]], 2)

if __name__ == "__main__":
    env = Env()
    policy_iteration = PolicyIteration(env)
    grid_world = GraphicDisplay(policy_iteration)
    grid_world.mainloop()
```

### 개별 코드 설명

- **다이나믹 프로그래밍에서 에이전트는 환경의 모든 정보를 알고 있다.** 따라서 환경을 가져와서 에이전트에 전달해준다. 여기서 에이전트는 `PolicyIteration`이라고 해도 무방함.

```python
import random
from environment import GraphicDisplay, Env # environment.py의 클래스를 가져옴

class PolicyIteration:
    def __init__(self, env):
        # 환경에 대한 객체 선언
        self.env = env
        # 가치함수를 2차원 리스트로 초기화
        self.value_table = [[0.0] * env.width for _ in range(env.height)]
        # 상 하 좌 우 동일한 확률로 정책 초기화
        self.policy_table = [[[0.25, 0.25, 0.25, 0.25]] * env.width
                                    for _ in range(env.height)]
        # 마침 상태의 설정
        self.policy_table[2][2] = []
        # 감가율
        self.discount_factor = 0.9

# ...

# (마지막 부분)
if __name__ == "__main__":
    env = Env() # 환경 정의
    policy_iteration = PolicyIteration(env) # 에이전트에 환경 전달
```
1. `environment.GraphicDisplay` : GUI로 그리드월드 환경을 보여준다. 
2. `environment.Env`에 정의된 변수들은 아래와 같다.
	- `env.width, env.height` : 높이와 너비  -> 정수
	- `env.state_after_action(state, action)` : 특정 상태에서 특정 행동 시, 에이전트가 움직인 후인 다음 상태  -> 상태(좌표)를 리스트로 반환
	- `env.get_all_states()` : 존재하는 모든 상태 -> 2차원 리스트
	- `env.get_reward(state)` : 특정 상태의 보상 -> 정수
	- `env.possible_actions` : 상하좌우 -> `[0,1,2,3]`은 각각 상하좌우

- 보상과 상태 변환 확률은 에이전트가 아니라 환경에 속해 있어서 `Env` 객체 내에 정의되어 있다. 
- 위 정보들을 토대로 에이전트는 정책 이터레이션을 진행한다. 

![[Pasted image 20230805171203.png]]
- 각 버튼은 `policy_evaluation()`, `policy_improvement()`, `get_action(state)`, 모든 변수 초기화 기능을 수행한다.

- 앞의 2개는 정책 평가 및 발전을 수행한다.
- `move`는 에이전트가 현재 가진 정책대로 어떻게 움직이는지를 볼 수 있다.
- 전체적인 흐름에서 코드는
```python
if __name__ == "__main__":
    env = Env() # 환경 정의
    policy_iteration = PolicyIteration(env) # 환경 -> 에이전트 전달
    grid_world = GraphicDisplay(policy_iteration) # 에이전트는 다시 environment.GraphicDisplay에 전달되어서 거기서 실행된다.
    grid_world.mainloop()
```
- 환경을 받은 에이전트가 다시 `GraphicDisplay`에 전달되어 실행되므로, `policy_iteration`을 상속받는다고 할 수 있다. 

#### PolicyIteration 클래스

##### __init__
```python
def __init__(self, env):
	# 환경에 대한 객체 선언
	self.env = env
	# 가치함수를 2차원 리스트로 초기화, env_width부터는 5x5 크기 구현하는 부분임
	self.value_table = [[0.0] * env.width for _ in range(env.height)]
	# 상 하 좌 우 동일한 확률로 정책 초기화, 
	self.policy_table = [[[0.25, 0.25, 0.25, 0.25]] * env.width
								for _ in range(env.height)]
	# 마침 상태의 설정
	self.policy_table[2][2] = []
	# 감가율
	self.discount_factor = 0.9
```
- 정책 이터레이션에 필요한 정보를 변수로 선언한다.
- `정책 이터레이션`은 모든 상태에 대해 가치함수를 계산한다. 2차원 테이블 형태로 `가치함수`를 선언한다. 
	- 가치함수는 각 상태에 대해 정의되므로 5x5 크기가 됨. 
	- 초기화 값은 `0`이다.
- `policy_table`은 모든 상태에 대해 상하좌우로 해당하는 각 행동의 확률 값이다. 
	- 모든 상태가  `5x5`, 행동이 4개이므로 `5x5x4`의 3차원 리스트가 된다. 
	- 무작위 정책으로 초기화하며, 모두 `0.25`를 갖는다.
- `discount_factor`(감가율)은 0.9로 정의한다.

##### policy_evaluation
- 정책 평가에서 에이전트는 모든 상태의 가치함수를 업데이트한다. 
- `next_value_table`을 선언한 다음, 계산 결과를 해당 테이블에 저장하며, 모든 계산이 끝나면 현재 테이블에 `next_value_table`을 덮어씌운다.
- `벨만 기대 방정식`의 수식은 아래와 같다.
$$
v_{k+1}(s) = \Sigma_{a \in A}\pi(a|s)(R_s^{a} + \gamma v_k(s'))
$$
```python
    def policy_evaluation(self):

        # 다음 가치함수 초기화
        next_value_table = [[0.00] * self.env.width
                                    for _ in range(self.env.height)]

        # 모든 상태에 대해서 벨만 기대방정식을 계산
        for state in self.env.get_all_states():
            value = 0.0
            
            # 마지막(파란 동그라미) 자리의 가치함수
            if state == [2, 2]:
                next_value_table[state[0]][state[1]] = value
                continue

            # 벨만 기대 방정식
            for action in self.env.possible_actions:
            
                next_state = self.env.state_after_action(state, action)
                reward = self.env.get_reward(state, action) # 보상
                next_value = self.get_value(next_state) # 다음 state의 가치함수
                
                # 벨만 기대 방정식 계산 부분(위 수식 참고)
                value += (self.get_policy(state)[action] *
                          (reward + self.discount_factor * next_value))

            next_value_table[state[0]][state[1]] = round(value, 2)

        self.value_table = next_value_table
```
- `environment.py`의 각 함수를 보는 것도 좋다. 직관적인 편임
##### policy_improvement
- 탐욕 정책 발전을 이용한다. `policy_table`을 복사한 `next_policy`에 업데이트된 정책을 저장한다.
- 발전은 큐함수를 이용해 구한다고 했다. 
$$
\begin{matrix}
q_{\pi}(s, a) &=& E_{\pi}[R_{t+1} + \gamma v_{\pi}(S_{t+1}) | S_t = s, A_t = a] \\
&=& R_s^a + \gamma v_\pi(s')
\end{matrix}
$$
```python
# 현재 가치 함수에 대해서 탐욕 정책 발전
def policy_improvement(self):

	next_policy = self.policy_table
	
	for state in self.env.get_all_states():
		if state == [2, 2]:
			continue
		value = -99999
		
		max_index = []
		# 반환할 정책 초기화
		result = [0.0, 0.0, 0.0, 0.0]

		# 모든 행동에 대해서 [보상 + (감가율 * 다음 상태 가치함수)] 계산
		for index, action in enumerate(self.env.possible_actions):
			next_state = self.env.state_after_action(state, action)
			reward = self.env.get_reward(state, action)
			next_value = self.get_value(next_state)
			
			# 큐함수 계산 부분
			temp = reward + self.discount_factor * next_value

			# 받을 보상이 최대인 행동의 index(최대가 복수라면 모두)를 추출
			if temp == value:
				max_index.append(index) # 보상이 같다면 그냥 추가
			# 현재 최대 보상보다 크다면 갱신
			elif temp > value:
				value = temp
				max_index.clear()
				max_index.append(index)

		# 행동의 확률 계산(같은 값이 여러개라면 그 중 하나를 선택해야 하니까)
		prob = 1 / len(max_index)

		# 계산된 확률은 다음 행동 리스트에 넣음
		# 가장 좋은 큐함수(들)만 선택되겠죠?
		for index in max_index:
			result[index] = prob

		next_policy[state[0]][state[1]] = result

	self.policy_table = next_policy
```

##### get_action
```python
# 특정 상태에서 정책에 따른 행동을 반환
	# 대부분의 처리는 중복된 확률의 행동이 들어왔을 때 어떻게 처리하느냐를 다룸
def get_action(self, state):
	# 0 ~ 1 사이의 값을 무작위로 추출
	random_pick = random.randrange(100) / 100

	policy = self.get_policy(state)
	policy_sum = 0.0
	
	# 정책에 담긴 행동 중에 무작위로 한 행동을 추출 
	for index, value in enumerate(policy):
		policy_sum += value
		if random_pick < policy_sum:
			return index
```
- 정책은 대부분 하나의 행동에 대해서만 1의 확률을 가지지만, 어떤 상태에서는 여러 행동이 동일한 확률을 가질 수 있다. 
>  **랜덤 선택 구현하기** : `policy` 리스트의 각 원소는 상하좌우 각 행동을 할 확률이다. 그런데 반복문은 항상 리스트에서 정해진 순서로 진행하므로 그냥 반복문을 돌리면 동일한 확률을 가졌을 때 우선순위가 발생한다(각 인덱스가 방향을 결정하기 때문에) 
>  따라서 동일한 확률을 갖는 행동을 무작위로 수행시키기 위해  `어떤 랜덤값(=임계값)`을 정해놓고, **임의의 값에 누적한 다음 행동 확률과 임계값을 비교한다.** 임계값을 넘지 못하면, `policy`의 다음 인덱스로 넘어가는 방식으로 구현한다.  
>  예를 들면 `[0.5, 0. 0, 0.5]`가 있다고 했을 때 `random_pick = 0.25`가 나오면 `0`번 행동이 선택되며, `random_pick = 0.75`가 나오면 `3`번 행동이 선택된다. (0번 인덱스에서 값을 넘지 못하므로 누적한 다음 값을 비교하고, 3번 인덱스에서 1 > 0.75가 되므로 3을 선택한다.)


##### get_policy, get_value
```python
# 상태에 따른 정책 반환
def get_policy(self, state):
	if state == [2, 2]:
		return 0.0
	return self.policy_table[state[0]][state[1]]

# 가치 함수의 값을 반환
def get_value(self, state):
	# 소숫점 둘째 자리까지만 계산
	return round(self.value_table[state[0]][state[1]], 2)
```
- 그냥 값 얻어 오는 거니까 지나감

#### 정책 이터레이션 코드 실행
- 실제 실행은 `정책 평가`와 `정책 발전`이 자동으로 반복적으로 이뤄지지만, 이 예제는 각각을 번갈아 클릭한다.

- 1회 Evaluation 클릭
![[Pasted image 20230805175515.png]]

- 이후 1회 Improve 클릭
![[Pasted image 20230805175547.png]]

> 개인적으로 재밌는 건 `Evaluate` 없이 `Improve`만 클릭했을 때의 그림이다
![[Pasted image 20230805175737.png]]
- 사실 당연하다. 가치 발전은 `보상 + 감가율 * 다음 상태 가치 함수`인데, 가치 평가를 하지 않았다면 `다음 상태 가치 함수`는 초기화된 값인 `0`이기 때문이다. 
- 따라서 각 상태에서의 행동을 결정하는 가치 발전은 `보상`에만 의존하게 되며, `보상 = -1`인 방향으로는 행동하지 않게 되는 것이다.

- 어쨌든, `Evaluate`와 `Improvement`를 반복해서 클릭하면 더 이상 갱신되지 않는 값으로 수렴한다.
![[Pasted image 20230805180148.png]]

- 그리고 이 상태에서 `move`를 클릭하면 시작점에서 오른쪽으로 가거나 아래쪽으로 이동하여 파란 동그라미에 도달하게 된다.

## 다이나믹 프로그래밍 2: 가치 이터레이션

### 명시적인 정책과 내재적인 정책
- `정책 이터레이션`은 `명시적인Explicit 정책`이 있으며, 정책을 평가하는 도구로 가치함수를 사용한다. **정책과 가치함수가 명확히 분리**되어 있는 것에 주목하자. 분리되어 있기 때문에, `벨만 기대 방정식`을 정책 이터레이션에 사용할 수 있는 것이다. 
- 정책이 독립적인 경우, 어떤 상태에서 1개가 아니라 여러 개의 행동을 선택하는 것도 가능하다. 대다수 정책은 확률적인 정책이기 때문에 가치함수를 계산하려면 기댓값이 들어가야 하고, 정책 이터레이션은 벨만 기대 방정식을 사용한다.

- 하지만 **정책이 결정적인 형태로만 정의**되어 있다면? - 현재 가치함수가 최적은 아니지만 최적이라고 가정하고, 그 가치함수에 대해 결정적인 형태의 정책을 적용한다면 어떨까?
- DP는 반복적인 계산을 수행한다. 이 반복이 계속되면, 가치함수를 발전시켜서 최적에 도달할 수 있다. 이를 `가치 이터레이션Value Iteration`이라고 한다. 
- **가치 이터레이션은** 정책의 발전을 설명하지 않고, **가치함수의 업데이트만을 이야기한다.** 왜냐하면 정책을 따로 명시적으로 표현하는 게 아니라, **가치함수 내에 정책이 내재되어 있기 때문에 가치함수를 업데이트하면 정책 또한 자동으로 발전**된다.

### 벨만 최적 방정식과 가치 이터레이션
- 위에서 `벨만 기대 방정식`을 통해 구한 것은, **현재 정책에 대한 참 가치함수**이다.
- 반면`벨만 최적 방정식`을 통해 구할 수 있는 것은 **최적 가치 함수**이다.
	1. 가치함수를 최적 정책에 대한 가치함수라고 가정함
	2. 반복적으로 계산
	3. 최적 정책에 대한 가치함수 = 최적 가치함수를 찾을 수 있음

$$ \begin{matrix}
v_*(s) &=& \underset{a}{max}[q_*(s,a) | S_t = s, A_t = a] \\
&=& \underset{a}{max}\,E[R_{t+1} + \gamma v_*(S_{t+1}) | S_t = s, A_t = a]
\end{matrix}
$$
- 현재 가치함수를 `최적 정책`에 대한 것으로 가정하기 때문에, 정책 발전이 필요 없다. 
- `벨만 기대 방정식`과 달리 `max`를 취한다. 따라서 가치함수 업데이트 시 정책 값을 고려할 필요가 없다. 
	- 벨만 기대 방정식은 기댓값 $E_\pi$에 정책이 포함되므로 정책을 고려한다.
- `벨만 최적 방정식`은 현재 상태에서 가능한 $R_{t+1} + \gamma v_*(S_{t+1})$ 의 값들 중 최고 값만 업데이트하면 된다. 이를 `가치 이터레이션`이라고 한다.
	- 정책 이터레이션에서는 다음 상태들을 모두 고려했음. 
	- 가치 이터레이션에서는 제일 높은 값을 가지는 값으로만 업데이트함.

- 위 수식을 계산 가능한 형태로 변환하면 아래와 같다.
$$
v_{k+1}(s) = \underset{a \in A}{max}(R_s^a + \gamma v_k(s'))
$$

### 코드로 보기
- `정책 이터레이션`은 정책 평가 + 발전이 있었다면, `가치 이터레이션`은 더 간단하게 구현할 수 있다. 현재 가치함수가 최적 정책에 대한 가치함수라고 가정하므로, `정책 발전`이 필요 없다. 따라서 `get_action` 함수를 정책 출력에 사용한다.

- 독립적인 정책이 없기 떄문에 `get_policy` 함수가 없으며, 정책 평가와 발전이 `value_iteration` 함수 하나로 대체되었다. 

#### Value Iteration 클래스 설명

##### value_iteration

```python
# 가치 이터레이션
# 벨만 최적 방정식을 통해 다음 가치 함수 계산
def value_iteration(self):
	next_value_table = [[0.0] * self.env.width for _ in
						range(self.env.height)]
	for state in self.env.get_all_states():
		if state == [2, 2]:
			next_value_table[state[0]][state[1]] = 0.0
			continue
		# 가치 함수를 위한 빈 리스트
		value_list = []

		# 가능한 모든 행동에 대해 계산
		for action in self.env.possible_actions:
			next_state = self.env.state_after_action(state, action)
			reward = self.env.get_reward(state, action)
			next_value = self.get_value(next_state)
			value_list.append((reward + self.discount_factor * next_value))
		# 최댓값을 다음 가치 함수로 대입
		next_value_table[state[0]][state[1]] = round(max(value_list), 2)
	self.value_table = next_value_table
```

$$
v_{k+1}(s) = \underset{a \in A}{max}(R_s^a + \gamma v_k(s'))
$$
- 참고 : 벨만 기대 방정식은 아래와 같다
$$
v_{k+1}(s) = \Sigma_{a \in A}\pi(a|s)(R_s^{a} + \gamma v_k(s'))
$$

- 결국 **`어떤 상태에서 가능한 행동들을 이용해 (R + 감가율 * 상태함수)`을 계산**하는 것까지는 동일하지만, 이 값들을 취합하는 방식이 다르다.
	- **벨만 기대 방정식은 저 값에 각각의 확률을 곱해 모두 합함**
	- **벨만 최적 방정식은 저 값들을 모은 다음, 최댓값 하나만 선택함**
- 공통점으로 둘 모두 `가치 함수`를 계산하는 것!
##### get_action
```python
# 현재 가치 함수로부터 행동을 반환
def get_action(self, state):
	action_list = []
	max_value = -99999

	if state == [2, 2]:
		return []

	# 모든 행동에 대해 큐함수 (보상 + (감가율 * 다음 상태 가치함수))를 계산
	# 최대 큐 함수를 가진 행동(복수일 경우 여러 개)을 반환
	for action in self.env.possible_actions:

		next_state = self.env.state_after_action(state, action)
		reward = self.env.get_reward(state, action)
		next_value = self.get_value(next_state)
		value = (reward + self.discount_factor * next_value)

		if value > max_value:
			action_list.clear()
			action_list.append(action)
			max_value = value
		elif value == max_value:
			action_list.append(action)

	return action_list
```
- 크게 차이는 없어 보임. 현 상태에서 주변 상태함수를 살피고, 같은 가치인 경우 모두 포함시키는 것까지 동일함

![[Pasted image 20230805183739.png]]
- `Calculate`만 여러 번 클릭한 다음,`Move`를 눌러주면 된다.

## DP의 한계와 강화학습

### DP의 한계
- DP는 계산을 빠르게 하는 것이지, 학습을 하는 게 아니다.
> DP의 한계점
> 1. **계산 복잡도** : 문제의 규모가 늘어나면 계산 만으로 풀어내는 데에 한계가 있다. DP의 계산 복잡도는 **상태 크기의 3제곱**이다. 따라서 바둑 같은 문제는 절대 풀 수 없음.
> 2. **차원의 저주** : 그리드월드는 2차원 (X, Y)지만, 차원이 늘어난다면 상태 수도 지수적으로 증가한다. 이를 `차원의 저주Curse Of Dimensionality`라고 한다.
> 3. **환경에 대한 완벽한 정보 필요** : 계산 복잡도 외에도, `보상 & 상태 변환 확률`을 안다는 전제 하에 문제를 풀었다. 그러나 저 둘은 "환경 모델"에 해당하기 때문에 에이전트가 보통은 저 정보를 알 수 없다.

- 위 한계를 환경과의 상호작용으로 극복한게 `강화학습`이다.

### 모델 없이 학습하는 강화학습
- `환경 모델` : MDP에서 환경 모델은 `상태 변환 확률 + 보상`이다.
$$
환경의 \, 모델 = P^{a}_{ss'}, \, R^a_s
$$
- `모델` : 입력과 출력의 관계를 식으로 나타내는 과정
	- 입출력 사이의 방정식은 정확할 수 없다. 방정식에선 정확해도 실제 세상에 적용할 때는 정확하지 않을 수도 있다. 많은 공학분야에서 이러한 `모델링 오차`를 사람이 테스트한다.
	- 모델은 정확할수록 복잡해지며, 자연현상을 100% 정확하게 모델링하는 건 불가능에 가깝다.
	- 반면 `게임` 같이 사람이 환경을 만들고 정해준 대로만 작동하는 경우 **모델링 오차는 없다고 할 수 있다.**

- 모델을 정확히 알기 어려운 경우, 시스템의 입출력 간 관계를 알기 위해 2가지 방법으로 접근할 수 있다.

1. 할 수 있는 선에서 모델링 후, 오차를 실험을 통해 조절한다. (고전)
	- 시스템의 안정성을 보장하나 문제가 복잡해질수록 한계가 있다.
2. **모델 없이 환경과의 상호작용을 통해 입출력 간 관계를 학습한다. (강화학습)**


