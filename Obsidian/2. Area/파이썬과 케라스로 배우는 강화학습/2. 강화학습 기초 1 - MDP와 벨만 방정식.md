
## MDP
- 순차적으로 행동을 결정하는 문제를 수학적으로 정의
- MDP는 `상태`, `행동`, `보상 함수`, `상태 변환 확률`, `감가율`로 구성되어 있다.
- 문제를 정의하는 것은 에이전트의 학습에서 가장 중요한 단계 중 하나이다. 정보가 너무 많거나 너무 적으면 안됨.

- 예시는 `그리드월드`로 들며, 일정 크기의 격자에서 일어나는 것을 의미함.

### 상태
$S$는 에이전트가 관찰 가능한 상태의 집합이다. `상태`라 함은, `자신의 상황에 대한 관찰`을 의미한다. 로봇이라면 센서 값이겠지만, 게임이라면 사용자가 정의해줘야 한다. 또한, 사용자는 "내가 정의하는 상태가 에이전트의 학습에 충분한 정보를 주는 것인가?"라는 의문을 가져야 한다. 
5x5 크기의 그리드월드를 예로 들면, 각 상태의 정의로 위치를 들 수 있다. 이 때, 상태의 개수는 25개이며 (x, y)로 나타낼 수 있고, 가로축이 x이고 세로축이 y이다.
$$
S = \{ (1,1), (1, 2), ..., (5,5) \}
$$
- 이 때 에이전트는 시간 $t$에 따라 집합 내의 상태를 탐험한다. 시간 t일 때의 상태를 $S_t$로 표현할 수 있다. $S_t$는 정해져 있지 않기 때문에, 뽑을 때마다 값이 달라질 수 있다. 이러한 것을 `확률 변수Random Variable`라고 한다. 
- "시간 t에서의 상태 $S_t$ 는 어떤 상태 $s$다"를 표현할 때 이런 식으로 나타냄. 확률 변수는 대문자로 나타낼 것이다.
$$
S_t = s
$$

### 행동
에이전트가 상태 $S_t$에서 할 수 있는 가능한 행동의 집합은 $A$다. 모든 상태에서 에이전트가 취할 수 있는 행동은 같기 때문에 $A$로 나타낸다. 따라서 특정한 행동은 소문자 $a$로 나타낸다. 시간 $t$에 에이전트의 특정 행동 $a$는 
$$
A_t = a
$$
로 나타낼 수 있다. 여기서도 마찬가지로 $t$에 취하는 행동이 정해진 게 아니기 떄문에 $A_t$로 나타낸 것.
그리드월드는 상하좌우로 이동할 수 있다. 따라서 아래처럼 나타낼 수 있다.
$$
A = \{up, down, left, right\}
$$

### 보상함수
에이전트가 학습할 수 있는 유일한 정보로, 환경 -> 에이전트에게 주는 정보이다.  
시간 $t$에서 상태가 $S_t = s$이고 행동이 $A_t = a$일 때, 에이전트가 받을 보상을 아래처럼 나타낼 수 있다.
$$
R_s^{a} = E[R_{t+1} | S_t = s, A_t = a]
$$
- 위는 `보상 함수Reward Function`의 정의이다. 시간 t에서의 상태 s, 행동 a일 때의(조건부확률 $|$ ) 보상$R_{t+1}$의 기댓값($E$)을 의미한다.
- 기댓값으로 표현하는 이유는, **환경에 따라 같은 상태와 같은 행동이더라도 보상이 달라질 수 있기 때문이다.**
- 특이한 점은 **다음 시간$t+1$에서 보상을 받는다는 것인데, 왜냐하면 보상은 에이전트가 알고 있는 게 아니라 환경이 알고 있기 때문이다.** 어떤 상태에서 행동을 하는 시점이 $t$이고, 이로 인해 환경이 에이전트에게 보상과 다음 상태를 알려주는 시점은 $t+1$이 된다는 의미이다.

`그리드월드`의 예제에서 에이전트는 빨간색 사각형으로, 초록색 삼각형에 닿으면 (-1), 파란색 동그라미에 닿으면 (+1)을 얻는다. 파란색 동그라미의 주위에서 파란색 동그라미로 간다면 +1을 얻는다고 하면, 가는 시점을 `타임 스텝 t`라고 하면 보상을 받는 시점은 `t + 1`이 된다. 
중요한 건 파란색 동그라미에 도달했기 때문에 보상을 얻는 게 아니라, **파란색 동그라미로 가는 행동을 했기 때문에 보상을 얻는다**는 것이다.

### 상태 변환 확률
- 에이전트가 상태 $s$에서 행동 $a$를 해서 $s'$ 으로 도달할 확률이다.
- 여기서도 **상태 s와 행동 a가 정해졌다고 해서 100% s'에 도달한다는 보장이 없다.** 다른 요인이 관여할 가능성을 남겨둔 것이다. 수식으로는 아래처럼 표현한다.
$$
P^a_{ss} = P[S_{t+1} = s' | S_t = s, A_t = a]
$$
- 여기서 $P$는 확률을 의미한다. 
- 이 값도 보상과 마찬가지로 에이전트가 알지 못하는 환경의 일부이다.
- 상태 변환 확률은 `모델Model`이라고도 부른다.

### 감가율
에이전트는 항상 현재에 판단을 내리므로, 현재에 가까운 보상일수록 더 큰 가치를 지닌다. 보상을 즉시 받는다면 더 높은 보상으로 치지만, 조금 지나서 보상을 받는다면 그 가치를 감가시켜 현재의 가치로 따지게 된다. 즉, **같은 보상을 받는다고 정해져 있다면 나중에 받을수록 그 가치가 내려간다는 의미로, 수학적인 표현으로는 `감가율Discount Rate`이라고 표현한다.**  
감가율은 $\gamma$로 표현하며, $[0, 1]$의 범위에 있다. 
현재 시간 $t$로부터 시간 $k$가 지난 뒤 보상을 $R_{t+k}$를 받는다면, 그 보상의 가치는 아래처럼 표현한다.
$$
\gamma^{k-1}R_{t+k}
$$

### 정책
모든 상태에서 에이전트가 할 행동이다. **상태를 입력으로 받아 행동을 출력으로 내보내는 함수**로 생각해도 된다. 
에이전트가 강화학습으로 학습할 목표는 `최적 정책`이다. 최적 정책은 각 상태에서 하나의 행동만을 선택하지만, 학습 중에는 확률적으로 여러 행동을 선택할 수 있어야 한다. 그래야 다양한 상황을 학습하고 최적 정책을 찾을 수 있게 된다. 수식으로 나타내면 아래와 같다.

$$
\pi(a|s) = P[A_t = a | S_t = s]
$$
- 즉, 상태 s일 때 행동 a를 선택할 확률이 정책이다.

`그리드월드`에서는 각 격자에 화살표를 표시한다고 생각할 수 있다. 즉, `어떤 격자에서는 어떤 행동을 해야 한다`라는 의미가 되며, 이는 상태가 주어졌을 때 행동을 선택할 확률인 정책의 정의와 부합한다.

## 가치함수

### 가치함수
에이전트가 최적의 정책을 찾는 방법은, 앞으로 **받을 보상들을 고려해서 선택**해야 좋은 선택을 할 수 있다. 미래의 보상을 고려하는 방법이 **`가치함수`** 이다.
모든 타임스텝에서의 보상을 생각하면 일련의 보상의 합을 아래처럼 나타낼 수 있다.
$$
R_{t+1} + R_{t+2} + R_{t+3} + R_{t+4} + ... 
$$
- $R$ : 대문자로 썼기 때문에 `확률변수`로, 정해져 있는 값이 아니다. 0일 수도, 실수일 수도 있다.
- $t$부터 시작하면 보상은 $t+1$부터 받는 거 기억하시죠?


그러나 위 수식은 감가를 고려하지 않았는데, 이로 인해 아래의 문제가 생길 수 있다.
> 1. 에이전트 입장에서 미래의 보상과 현재의 보상을 똑같이 취급한다. 더 정확히는 구분할 수 없다.
> 2. 100 x 1과 20 x 5를 구분할 수 없다.
> 3. 무한의 타임스텝이 주어진다면, 0.1을 무한으로 더하나 1을 무한으로 더하나 똑같다.  

따라서 단순합으로는 판단을 내리기가 어렵다. 그래서 `감가율`을 고려한다. 이를 적용하면 아래처럼 수식을 나타낼 수 있다.
$$
G_t = R_{t+1} + \gamma R_{t+2} + \gamma^2 R_{t+3} + \gamma^3 R_{t+4}  + ...
$$
- **$G_t$는 반환값**으로, 감가율을 적용한 보상들의 전체합이다. $t$가 들어간 것에서 보이듯 **모든 타임스텝에서 반환값을 고려할 수 있다.**


- 반환값은 에피소드가 끝난 후에 알 수 있지만, 정확한 정보를 알기 위해 끝까지 기다리는 것보다 정확하지 않더라도 현재의 정보로 행동하는 게 나을 때도 있다. 
- 즉, **어떤 상태에 있다면 앞으로 얼마의 보상을 받을 것인지에 대한 기댓값**을 고려할 수 있는데, 이것이 `가치함수`이다. 
$$
v(s) = E[G_t|S_t = s]
$$
- 가치함수는 소문자로 표현하는데, 확률변수가 아니라 에이전트가 갖고 있는 값을 나타내기 때문에 그렇다. 
- 상태의 가치를 고려하는 이유는 에이전트가 갈 수 있는 상태들의 가치를 안다면 그 중 가장 가치가 높은 상태를 선택할 수 있기 때문이다.

> 그러면 이런 의문이 듦 : 보상은 에이전트가 가지고 있는 정보가 아니니까 미리 알 수 없는 거 아닌가요? 
- **정확하지 않아도 상관이 없다. 행동과 피드백을 계속 받으면 기대가 정확해지기 때문.** 위 수식을 반환값의 정의로 풀어써보자.

$$
\begin{matrix}
v(s) &=&E[R_{t+1} + \gamma R_{t+2} + \gamma^2 R_{t+3} + \gamma^3 R_{t+4} + ... | S_t = s] \\ &=& E[R_{t+1} + \gamma(R_{t+2} + \gamma R_{t+3} + \gamma^2 R_{t+4} + ...) |S_t = s] \\
&=& E[R_{t+1} + \gamma G_{t+1} | S_t = s]
\end{matrix}
$$
`반환값`인 $G_{t+1}$로 표현했으나, 실제로는 앞으로 받을 것이라 예상하는 보상이므로, 기댓값으로 나타낼 수 있다. 따라서 $v$로 표현하면 아래처럼 표현할 수 있다.
$$
v(s) = E[R_{t+1} + \gamma v(S_{t+1}) | S_t = s]
$$

정책도 고려되어야 한다. 다음 상태로 넘어가는 과정이 있어야 하고, 상태에 따른 행동을 결정하는 게 정책이기 때문. 위 수식에서 $v$와 $E$ 밑에 $\pi$ 만 써주면 그게 `벨만 기대 방정식Bellman Expectation Equation`이다.
$$
v_{\pi}(s) = E_{\pi}[R_{t+1} + \gamma v_{\pi}(S_{t+1}) | S_t = s]
$$
- **벨만 기대 방정식은 현재 상태의 가치함수 $v_{\pi}(s)$와 다음 상태의 가치함수 $v_{\pi}(S_{t+1})$ 사이의 관계를 말해주는 방정식이다.** 강화학습은 벨만 방정식을 풀어나가는 스토리다.

### 큐함수
어떤 상태에서 어떤 행동이 좋은지 알려주는 함수를 `행동 가치함수`라고 하며, 간단하게 `큐 함수`라고 한다. 

> 상태 s가 있고, 행동1과 행동2가 있다고 하면, 이는 1개의 상태에서 2개의 `행동 상태`를 갖는 것이다. 이 2개의 행동 상태에서 가치함수를 따로 계산할 수 있는데 이를 `큐함수`라고 한다.  
> 즉, s -> (s + a1), s -> (s + a2)로, (s + a1)과 (s + a2)가 행동 상태가 되는 것이고, 이 행동 상태에서 큐함수를 얻을 수 있으며, 모든 행동상태에서 큐함수와 정책을 곱한 값의 합을 가치함수라고 함
- 따라서 큐함수는 `상태, 행동`이라는 2개의 변수를 가지며 아래처럼 나타낸다.
$$
큐함수 : q_{\pi}(s, a)
$$

수식으로는,
1. **각 행동을 했을 때 앞으로 받을 보상인 큐함수를 정책 $\pi(a|s)$에 곱한다.**
2. 모든 행동에 대해 1.을 반복한 값을 더하면 가치함수가 된다.

$$
v_\pi(s) = \Sigma_{a \in A}\pi(a | s)q_\pi(s, a)
$$
- 강화학습에서 **행동을 선택하는 기준으로는 가치함수보다는 큐함수를 사용한다.**
- 큐함수 또한 벨만 기대 방정식으로 나타낼 수 있으며, 조건문에 행동이 더 들어간다는 정도에 차이가 있다.

$$
q_{\pi}(s, a) = E_{\pi}[R_{t+1} + \gamma q_{\pi}(S_{t+1}, A_{t+1}) | S_t = s, A_t = a]
$$

## 벨만 방정식

### 벨만 기대 방정식

$$
v_{\pi}(s) = E_{\pi}[R_{t+1} + \gamma v_{\pi}(S_{t+1}) | S_t = s]$$
- 가치함수는 어떤 상태의 가치에 대한 기대를 나타낸다. 어떤 상태의 가치함수는 에이전트가 그 상태로 갈 경우, 앞으로 받을 보상에 대한 기댓값이다. 에이전트의 현재 정책에 영향을 받는 것까지 반영한 게 위 수식이다.  

- 벨만 방정식이 왜 중요한가? : 위에서 든 의문에서 그게 있었다 - **앞으로 받을 모든 보상을 고려할 수 있는가?** -> 물리적으로 불가능하기 때문에, 컴퓨터가 이를 계산하기 위해 다른 조치가 필요하다. 일반적으로 식 하나로 푸는 방법을 쓰지 않고 **식 자체로는 풀리지 않으나 계속 계산하는 방법을 사용한다.**

> 예를 들어, 1을 100번 더하는 상황을 생각해보자. 식 하나로 풀면 $1 + 1 + 1 + ... = 100$으로 구할 수 있다.
> 그런데 **$x$라는 변수를 하나 지정한 다음, 1을 100번 더해나가는 과정으로 구현할 수도 있다.**

- 벨만 방정식은 2번째 방식과 동일하게, **값을 변수에 저장하고 루프를 도는 계산을 통해 참값을 알아나간다.** 이는 DP하고도 관련이 있음.

- 즉 계속 $v_\pi(s)$라는 변수를 갱신해나간다는 의미인데, 그렇다면 기댓값 계산이 필요하다. 이는 어떻게 이뤄져야 할까?
기댓값에는 어떤 **행동 확률 $\pi(a|s)$** 과 행동을 했을 때 어떤 상태로 가는 확률인 **상태 변환 확률$P^a_{ss'}$** 이 포함되어 있다. 따라서 정책과 상태 변환 확률을 포함해서 계산하는데, 이를 계산 가능한 형태로 나타낸다면 아래 수식으로 나타낼 수 있다.
$$
v_{\pi}(s) = \Sigma_{a \in A}\pi(a|s)(R_{t+1} + \gamma\Sigma_{s'\in S}P^a_{ss'}v_\pi(s')
$$

Q ) 그리드월드에서 모든 상태변화확률을 s와 a에 대해 1이라고 가정해보자. 왼쪽으로 행동을 선택하면, 왼쪽에 있는 상태로 무조건 가는 환경인 설정이다. 오른쪽으로 갔을 때 보상 1을 얻으며, 상하좌우의 가치함수는 각각 (0, 0.5, 1, 0)이다. 현재 상태의 가치함수 = 0이라면, 벨만 기대 방정식으로 업데이트되는 가치함수는 얼마일까? 행동은 상하좌우 4개가 있지만, 초기 정책은 무작위로, 각 행동이 선택되는 확률은 25%이다. 그리고 가치함수는 왼쪽 상태의 가치함수만 1이고 나머지는 0이다. 감가율은 0.9이다.

A ) 상태변환확률이 무조건 1이므로 위에 있는 수식은
$$
v_{\pi}(s) = \Sigma_{a \in A}\pi(a|s)(R_{t+1} + \gamma v_\pi(s'))
$$
으로 바뀐다. 이 식은 말로 표현하면
1. 각 행동에 대해 행동할 확률$\pi({a|s})$
2. 행동을 했을 때 받을 보상$R_{t+1}$
3. 다음 상태의 가치함수$v_\pi(s')$
이 3가지를 고려한다는 의미가 된다.

4개의 행동에 대해, 아래처럼 계산할 수 있다.
1. 상 : 0.25 x (0 + 0.9 x 0) = 0 -> 보상도 없고, 다음 상태의 가치함수도 0
2. 하 : 0.25 x (0 + 0.9 x 0.5) = 0.1125 -> 보상은 없지만 다음 상태의 가치함수 0.5
3. 좌 : 0.25 x (0 + 0.9 x 1) = 0.225 -> 보상은 없지만 다음 상태의 가치함수 1
4. 우 : 0.25 x (1 + 0.9 x 0) = 0.25 -> 보상은 있지만 다음 상태의 가치함수 0

- 기댓값 = 0.5875(모두 합)

이 4개의 값을 모두 더하면 현재 상태의 가치함수가 업데이트되게 된다. 그리고 위 과정이 계속 반복되면 `참값`을 구할 수 있다. 여기서 `참값`은 에이전트가 **현재 정책을 계속 따라갔을 때 실제 보상에 대한 참기댓값을 의미한다.**

> 개인 감상) 검색 엔진에서 웹사이트 중요도 계산할 때 나에 대한 중요도는 나에게 들어오는 링크들의 중요도로 계산되는 것이 생각난다. 이를 계속 반복하면 어떤 값에 수렴한다고 했는데, 그거랑 비슷한 거 같음.


### 벨만 최적 방정식
- 아래는 벨만 기대 방정식.
$$
v_{\pi}(s) = E_{\pi} [R_{t+1} + \gamma v_\pi(S_{t+1}) | S_t = s]
$$

- 벨만 기대 방정식을 계속 반복하면 언젠가 $v_\pi(s)$ 값이 수렴한다. 이를 현재 정책에 대한 `참 가치함수`라고 한다. 이는 `최적 가치 함수Optimal Value Function`와는 다른 개념으로, 최적 가치 함수는 `전체 정책` 중 가장 높은 보상을 주는 가치함수다. 

- 위 수식을 계산 가능한 형태로 바꿔보자. 
$$
v_{k+1}(s) = \Sigma_{a\in A}\pi(a|s)(R_s^{a} + \gamma v_k(s'))
$$
- $k$는 현재 정책에 따라 $k$번째 계산한 것을 의미한다. 
- $k+1$번째 상태 함수는 현재 상태 $s$의 주변 상태 $s'$들을 통해 계산한다. 이 계산은 모든 상태에 대해 동시에 계산된다. 그리드월드라면 25개의 상태에서 동시에 일어난다. 
- 이를 통해 상태집합에 속한 모든 상태에 대해 가능한 행동들을 고려, 주변 상태에 저장된 가치함수로 현재의 가치함수를 업데이트한다.

---
- 위는 `참 가치함수`를 구할 수 있다. 그렇다면 `최적 정책`을 찾는 방법은 무엇일까?
- 이는 정책을 따라갔을 때의 보상의 합인 `가치함수`를 통해 판단할 수 있다. 가치함수는 결국 정책이 얼마나 좋은지를 말해준다. `최적 가치함수`를 수식으로는 아래처럼 나타낼 수 있다.
$$
v_{*}(s) = \underset{\pi}{max}[v_\pi(s)]
$$

- `최적 큐함수`도 마찬가지로 아래처럼 표현할 수 있다.
$$
q_{*}(s, a) = \underset{\pi}{max}[q_{\pi}(s,a)]
$$

가장 높은 가치함수나 큐함수를 에이전트가 찾았다고 가정하면, 최적 정책은 각 상태 $s$에서의 최적의 큐함수 중 가장 큰 큐함수를 가진 행동을 취하는 것이다. 선택 상황에서의 판단 기준은 언제나 큐함수이며, 최적 정책은 이 큐함수 중 가장 높은 행동 하나를 하는 것이다.  
따라서 최적 정책은 최적 큐함수 $q_*$만 있다면 아래 수식처럼 구할 수 있다.
$$
\pi_*(s, a) = 
\begin{cases}
1 & if & a = argmax_{a \in A}(q_*(s, a)) \\
0 & otherwise
\end{cases}
$$
- `argmax`는 $q_*$를 최대로 해주는 행동 a를 반환하는 함수이다. 

- `최적 가치함수, 큐함수`를 구하는 방법은 순차적 행동 결정 문제를 푸는 것이다. 여기서는 최적의 가치함수 간 관계를 살펴본다. 

벨만 방정식은 현재 가치함수 - 다음 타임스텝의 가치함수 사이의 관계식이다. 현재 상태의 가치함수가 최적이라면, 에이전트가 가장 좋은 행동을 선택한다는 의미이다. 에이전트의 행동 선택 기준은 큐함수이다. 즉, **선택의 기준이 되는 큐함수가  최적의 큐함수가 아니라면, 에이전트가 아무리 큐함수 중 최대를 선택해도 가치함수는 최적의 가치함수가 되지 않는다.**   

따라서 최적의 가치함수와 큐함수 간의 관계는 아래처럼 나타낼 수 있다.
$$ \begin{matrix}
v_*(s) &=& \underset{a}{max}[q_*(s,a) | S_t = s, A_t = a] \\
&=& \underset{a}{max}\,E[R_{t+1} + \gamma v_*(S_{t+1}) | S_t = s, A_t = a]
\end{matrix}
$$
- 위는 최적 큐함수로 최적 가치함수 표현, 아래는 최적 가치함수 간의 표현이다.
- 이를 **벨만 최적 방정식(Bellman Optimality Equation)** 이라고 부른다. 

- 큐함수에 대해서도 벨만 최적 방정식을 표현할수 있다.
$$
q_*(s,a) = E[R_{t+1} + \gamma\, \underset{a'}{max}q_*(S_{t+1}, a') | S_t = s, A_t = a]
$$
- 최적 정책을 따라갈 때, 현재 상태의 큐함수는 다음 상태에 선택 가능한 행동 중, 가장 높은 값의 큐함수를 1번 감가하고 보상을 더한 것과 같다. 

- $E$가 있는 이유는, 큐함수가 행동까지 선택한 상황이므로, 이에 따른 보상은 환경이 주는 값이기 때문이다.(정해지지 않은 기댓값 개념)

- `벨만 기대 방정식`과 `벨만 최적 방정식`을 통해, MDP로 정의되는 문제를 계산으로 푸는 방법이 `Dynamic Programming`이다.