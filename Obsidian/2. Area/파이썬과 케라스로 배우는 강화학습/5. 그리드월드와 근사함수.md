
지금까지는 상태 공간이 작고 환경이 변하지 않는 문제였다.  
실제로는 에이전트에게 주어지는 상태가 다양하고, 환경도 시간에 따라 변한다.   

- 기존 강화학습 알고리즘은 각 상태에 대한 정보를 `테이블`로 저장했지만, 각 상태 정보를 `근사`한다면 상태 공간의 크기가 크고 환경이 변해도 학습할 수 있다. 여기서 `인공신경망`을 `강화학습`과 함께 사용한다.

- 케라스는 많이 배웠으니까 넘어간다. 인공신경망을 이용해 큐함수를 근사한 `딥살사` 알고리즘과, 정책을 근사하는 `REINFORCE` 알고리즘을 다룬다.

## 근사함수

### 몬테카를로, 살사, 큐러닝의 한계
- 기존 DP의 한계
	1. 계산 복잡도
	2. 차원의 저주
	3. 환경에 대한 완벽한 정보 필요

- 몬테카를로, 살사, 큐러닝은 `3.`을 극복했다. 하지만 `1, 2`는 그렇지 못하다.
- 큐러닝까지의 알고리즘은 기본적으로 **상태가 적은 문제에만 적용 가능**하다.
- 이러한 문제는 `큐함수를 매개변수로 근사`함으로써 해결할 수 있다.
### 근사함수를 통한 가치함수의 매개변수화
- 어떤 산점도가 있을 때, 이 산점도를 나타내는 함수를 `근사함수Function Approximation`라고 한다. 
- `인공신경망`을 통해 `근사함수`로 가치함수를 표현하는 이유는
	1. 오래 전부터 강화학습에서 인공신경망을 근사함수로 사용해왔음
	2. 딥러닝 발전 이후로 거의 대부분 인공신경망을 사용함

- 시그모이드 함수, 출력은 $[0, 1]$
$$
f(x) = \frac{1}{1+e^{-x}}
$$

- 현재 대부분의 인공신경망에서는 `ReLU:Rectified Linear Unit` 함수를 활성함수로 사용하고 있다.
- `활성함수`는 `비선형 함수`를 쓴다 : 선형 변환을 하면 넓거나 깊은 층을 쌓아봤자 1개의 층으로 바꿔 나타낼 수 있기 때문임.
- 딥러닝은 또한 **다양하고 복잡한 데이터에서 `특징Feature`을 추출**해 높은 추상화를 가능하게 한다.
	- `추상화`의 의미 중 하나가 `특징 추출`이다. 
	- 딥러닝에서 **각 노드는 각자 다른 특징을** 추출한다. 잘 설계되고 학습된 신경망은 여러 복잡한 특징을 추출한다. 심층신경망은 특징 추출을 **스스로 한다**.

## 딥살사

### 딥살사 이론
- 그리드월드에서 장애물이 1개 추가되었고, 각 장애물은 같은 속도로 한 스텝마다 1칸씩 움직인다. 벽에 부딪힌 경우 튕겨나와서 다시 반대로 움직인다.
- 이를 `살사 알고리즘`을 사용하되, 큐함수는 `인공신경망`으로 근사한다.
- 2개의 은닉층을 사용하므로 심층 신경망이 된다. 그래서 `딥살사DeepSARSA`라고 이 책에서는 부른다.

우선, MDP를 정의한다 : 이 때 기존 문제와 달리 `상태의 정의를 다르게 하는데,` 왜냐하면 물체가 나에게 다가오는지를 알아야 하기 떄문이다. 따라서 이 문제에서는 상태를 아래처럼 정의한다.

> 1. 에이전트에 대한 도착 지점의 상대 위치 x, y
> 2. 도착지점의 라벨
> 3. 에이전트에 대한 장애물의 상대 위치 x, y
> 4. 장애물의 라벨
> 5. 장애물의 속도

- 장애물이 3개이므로, 3, 4, 5는 12개이고 그 외에 1과 2를 더하면 총 15개의 원소를 가진다. 
- 강화학습에서도 특징 추출을 위해 문제에 대한 전문 지식이 필요하다. 이 문제는 간단해서 바로 특징 추출이 가능하다. 

그리고 살사의 큐함수를 업데이트할 때, `경사하강법`을 사용해야 한다. 이를 위해 `오차함수`를 정의하는데, 오차함수로는 `MSE`를 사용한다.

- 살사의 큐함수 업데이트 식
$$
Q(S_t, A_t) \leftarrow Q(S_t, A_t) + \alpha(R + \gamma Q(S_{t+1}, A_{t+1}) - Q(S_t, A_t)))
$$

- 정답 라벨 역할은 $R + \gamma Q(S_{t+1}, A_{t+1})$ 부분, 예측은 $Q(S_t, A_t)$ 부분이 한다. 
- 이 두 부분을 떼내어 $MSE$로 구현하면 아래 수식이 된다.
$$
MSE = (정답 - 예측)^2 = (R_{t+1} + \gamma Q(S_{t+1},\, A_{t+1}) - Q(S_t, A_t))^2
$$

### 코드로 구현

- 에이전트가 환경과 상호작용하면서 하는 일은 아래와 같다.
> 1. 상태에 따른 행동 선택
> 2. 선택한 행동으로 환경에서 한 타임스텝 진행
> 3. 환경으로부터 다음 상태와 보상을 받음
> 4. 다음 상태에 대한 행동 선택
> 5. 환경으로부터 받은 정보를 토대로 학습 진행

- 위 과정을 코드로 나타내면 아래와 같다.
```python
env = Env()
agent = DeepSARSAgent()

# 현재 상태에 대한 행동 선택
action = agent.get_action(state)

# 선택한 행동으로 환경에서 한 타임스텝 진행 후 샘플 수집
next_state, reward, done = env.step(action)
next_state = np.reshape(next_state, [1, 15])
next_action = agent.get_action(next_state)

# 샘플로 모델 학습
agent.train_model(state, action, reward, next_state, next_action,
				  done)
state = next_state
```
- 기존 알고리즘에서는 에이전트의 큐함수 테이블을 이용했지만, `딥살사`에서는 큐함수 테이블 대신 인공신경망을 이용한다. 현재 상태 특징들이 인공신경망의 입력으로 들어가면 인공신경망은 각 행동에 대한 큐함수를 출력으로 내놓는다. 이 큐함수는 근사된 함수이다.
- 따라서 **에이전트 클래스는 상태가 입력이고 출력이 각 행동에 대한 큐함수인 인공신경망 모델을 갖고 있어야 한다.** 

- 인공신경망 모델은 `DeepSARSAgent` 클래스에 아래처럼 구현되어 있다.
```python
    def build_model(self):
        model = Sequential()
        model.add(Dense(30, input_dim=self.state_size, activation='relu'))
        model.add(Dense(30, activation='relu'))
        model.add(Dense(self.action_size, activation='linear'))
        model.summary()
        model.compile(loss='mse', optimizer=Adam(lr=self.learning_rate))
        return model
```
- `action_space = [0, 1, 2, 3, 4]`로 가능한 행동은 5가지이다.
- 입력층의 유닛 차원은 `state_size`를 따라간다.
- 출력층의 활성화 함수를 `linear`로 구현했는데, 출력값이 큐함수이고 큐함수는 $[0, 1]$의 값이 아니기 떄문이다. 

-`get_action` : 큐함수를 통해 에이전트는 선택을 한다
```python
# 입실론 탐욕 방법으로 행동 선택
def get_action(self, state):
	if np.random.rand() <= self.epsilon:
		# 무작위 행동 반환
		return random.randrange(self.action_size)
	else:
		# 모델로부터 행동 산출
		state = np.float32(state)
		q_values = self.model.predict(state) # 2차원이라서
		return np.argmax(q_values[0]) # 1차원으로 내보내준다
```
- 정책은 탐험을 제외하면 큐함수의 값 중 가장 큰 값을 갖는 행동을 반환한다.

- `train_model` : 모델 훈련
$$
MSE = (정답 - 예측)^2 = (R_{t+1} + \gamma Q(S_{t+1},\, A_{t+1}) - Q(S_t, A_t))^2
$$
```python
def train_model(self, state, action, reward, next_state, next_action, done):
	if self.epsilon > self.epsilon_min:
		self.epsilon *= self.epsilon_decay

	state = np.float32(state)
	next_state = np.float32(next_state)
	target = self.model.predict(state)[0]
	
	# 살사의 큐함수 업데이트 식
	if done:
		target[action] = reward
	else:
		target[action] = (reward + self.discount_factor *
						  self.model.predict(next_state)[0][next_action])

	# 출력 값 reshape
	target = np.reshape(target, [1, 5])
	# 인공신경망 업데이트
	self.model.fit(state, target, epochs=1, verbose=0)
```
- 살사 및 큐러닝과 달리, `딥살사`에서의 입실론은 시간에 따라 감소시킨다 : 초반에 에이전트가 다양한 상황을 학습하고, 학습 후에는 예측하는대로 에이전트가 움직이기 위함이다.
- 케라스의 모델 인풋은 `float`를 쓰기 떄문에 `np.float32`로 전달한다.
- (중요) : 딥살사의 출력은 5개의 큐함수 값을 가지지만, 모델 업데이트를 위해 **오류함수를 계산하는 출력은 이 중 실제로 행동된 큐함수 1개이다.** 따라서 타깃에서 실제 행동에 해당하는 큐함수 외에는, 예측 값에서 해당하는 큐함수와 동일해야 한다. 
	- (솔직히 이해 안됨) `target = self.model.predict(state)[0]` -> 이 코드로 예측을 타깃으로 놓고, 실제 행동에 해당하는 부분을 뒤에서 계산해서 변경하면 실제 행동에 대한 큐함수를 제외하고는 타깃과 예측의 차이가 0이 된다.
- 큐함수 업데이트 식은 $\gamma Q(S_{t+1},\, A_{t+1})$ 이며, `done`은 에피소드가 끝났음을 의미하므로 즉각적인 보상만을 생각한다. 

### 실제 실행
- 에피소드 = 1000개이며, 1개의 에피소드는 도착 지점에 도달하면 종료되며, 에피소드의 각 상태(위치)에서 딥러닝 모델이 실행된다. 
- 이미 저장된 모델도 깃에 포함되어 있으며, 이는 `__init__`의 `self.load_model = True`로 실행하면 된다. 

## 폴리시 그래디언트 : REINFORCE
### 정책 기반 강화 학습
- 지금까지 한 걸 `가치 기반 강화학습 Value-Based Reinforcement Learning`이라고 한다. 에이전트가 가치함수를 기반으로 행동을 선택하고, 가치함수를 업데이트해서 학습하기 때문이다.

- 반면 `정책 기반 강화학습 Policy-based Reinforcement Learning`이라는 것도 있다. 
	- 가치함수를 토대로 행동을 선택하지 않고, 바로 행동을 선택한다.
	- `딥살사`에서는 인공신경망이 큐함수를 근사했지만, 정책 기반 강화학습에서는 인공신경망이 정책을 근사한다. 
	- 따라서 입력은 `상태`, 출력은 `행동 확률`이 된다.

- 이를 케라스 모델로 만들면 아래와 같다. 
```python
# 상태가 입력, 각 행동의 확률이 출력인 인공신경망 생성
def build_model(self):
	model = Sequential()
	model.add(Dense(24, input_dim=self.state_size, activation='relu'))
	model.add(Dense(24, activation='relu'))
	model.add(Dense(self.action_size, activation='softmax'))
	model.summary()
	return model
```
1. 출력층의 활성함수가 `softmax`이다.
- Softmax : 인공신경망의 모든 출력의 합이 1일 때 사용한다
$$
s(y_i) = \frac{e^{y_i}}{\Sigma_je^{y_j}}
$$
- $s(y_i)$는 i번째 행동을 할 확률을 의미한다.
2. `model.compile()`이 들어가지 않는다.
	- `딥살사`에서는 MSE 오차를 줄였다.
	- 여기서는 따로 오류함수를 정의해야 한다.

### 폴리시 그래디언트
- 정책을 근사하는 법 중 하나로 `정책신경망`이 있다. 정책신경망의 가중치값에 따라 에이전트의 누적 보상이 달라진다. 즉, **정책신경망의 계수**는 목표로 하는 누적 보상에 대한 **함수의 변수**이다.
- 정책이 정책 신경망으로 근사되기 떄문에 아래 수식으로 표현할 수 있다.
$$
정책 = \pi_{\theta}({a|s})
$$

- 누적 보상은 최적화하고자 하는 목표 함수가 되며, 최적화를 하게 되는 변수는 `인공신경망의 계수`이다. 정책 기반 강화학습의 목표는 아래와 같다.
$$
maximize(J(\theta))
$$
- 목표함수를 $J(\theta)$라고 하자. 이를 최적화하는 방법은
	1. **목표함수를 미분**해서
	2. 그 **미분값에 따라 정책을 업데이트**한다

- 딥살사와는 다르게 **목표는 목표함수의 최대화**이다. 따라서 경사하강법이 아니라 `경사 상승법Gradient Ascent`을 적용해야 한다. 이를 수식으로 나타내면 아래와 같다. 이를 `폴리시 그래디언트Policy Gradient`라고 한다.
$$
\theta_{t+1} = \theta_{t} + \alpha \nabla_\theta J(\theta)
$$
- $\alpha$는 학습률임

- 이제 목표함수의 정의와, 목표함수의 미분 표현 식을 알아보자. 만약 에피소드의 끝이 있고, 에이전트가 어떤 특정 상태 $s_0$에서 에피소드를 시작하는 경우, 목표함수는 상태 $s_0$에 대한 가치함수로 나타낼 수 있다. 이를 수식으로 나타내면 아래와 같다.
$$
J(\theta) = v_{\pi_\theta}(s_0)
$$

- 위 수식의 미분값을 구해야 하는데, 그 과정은 쉽지 않다. 여기선 그 과정의 흐름만을 본다.
$$
\nabla_\theta J(\theta) = \nabla_\theta v_{\pi_\theta}(s_0)
$$

- 이 떄 등장하는 게 `폴리시 그래디언트 정리`로, 위 수식은 아래처럼 표현할 수 있다.
$$
\nabla_\theta J(\theta) = \underset{s}\Sigma \, d_{\pi_\theta}(s) \underset{a}\Sigma \, \pi_\theta(a | s) \times \frac{\nabla_\theta \pi_\theta(a | s)}{\pi_\theta(a|s)}q_\pi(s, a)
$$

- 로그로 정리되는 식을 정리하면
$$
\nabla_\theta J(\theta) = \underset{s}\Sigma \, d_{\pi_\theta}(s) \underset{a}\Sigma \, \pi_\theta(a | s) \times \nabla_\theta log \pi_\theta(a|s)\,q_\pi(s, a)
$$
기댓값의 정의가 `확률 x 받은 값`이었다. 위 식 또한 기댓값의 일종으로 볼 수 있는데, $\underset{s}\Sigma \, d_{\pi_\theta}(s) \underset{a}\Sigma \, \pi_\theta(a | s)$ 는 에이전트가 어떤 상태 $s$에서 행동$a$를 선택할 확률을 의미한다. 따라서 이를 기댓값으로 표현하면

$$
\nabla_\theta J(\theta) = E_{\pi_\theta}[\nabla_\theta log \pi_\theta(a|s)\,q_\pi(s, a)]
$$

- 폴리시 그래디언트에서도 기댓값은 샘플링으로 대체할 수 있다. 에이전트가 정책 신경망을 업데이트하기 위해 구해야 하는 식은 $\nabla_\theta log \pi_\theta(a|s)\,q_\pi(s, a)$이다. 

- 따라서 폴리시 그래디언트에서 정책을 업데이트하는 식은 아래처럼 구성된다.
$$
\theta_{t+1} = \theta_t + \alpha\nabla_\theta J(\theta) \sim \theta_t + \alpha[\nabla_\theta log \pi_\theta(a|s)\,q_\pi(s, a)]
$$

케라스를 사용한다면 미분값을 직접 구할 필요는 없다. 그러나 문제가 있는데, 폴리시 그래디언트에서 행동을 선택하는 데 가치함수가 꼭 필요하지 않다. 현재 에이전트는 정책만 갖고 있고 가치함수나 큐함수가 없기 떄문에 $q_\pi(s,a)$를 구할 수 없다.

따라서 **목표함수의 미분값을 어떻게 근사할 것인가는 폴리시 그래디언트에서 중요한 문제**인데, 가장 고전적인 방법 중 하나는 큐함수를 반환값 $G_t$로 대체하는 것이다. 이를 `REINFORCE` 알고리즘이라고 하며, 이 알고리즘의 업데이트 식은 아래 수식이 된다. 

$$
\theta_{t+1} \sim \theta_t + \alpha[\nabla_\theta log \pi_\theta(a|s)\,G_t]
$$
- 에피소드가 끝날 때까지 기다리면 에피소드 동안 지나온 상태에 대해 각 반환값을 구할 수 있다. REINFORCE 알고리즘은 에피소드마다 실제 얻은 보상으로 학습하는 폴리시 그래디언트라고 할 수 있다. 그래서 `몬테카를로 폴리시 그래디언트`라고도 부른다. 

### 코드 설명
- 에이전트와 환경의 상호작용을 생각해보면
> 1. 상태에 따른 행동 선택
> 2. 선택한 행동으로 한 타임스텝 진행
> 3. 환경으로부터 다음 상태와 보상을 받음
> 4. 다음 상태에 대한 행동을 선택, 에피소드 종료까지 반복
> 5. 환경으로부터 받은 정보를 토대로 에피소드마다 학습 진행

- `딥살사`와 달리, `REINFORCE` 에이전트는 정책 신경망을 가졌다. 따라서 `1번`에서 행동 선택시 정책신경망의 출력을 이용하면 된다.
```python
    # 정책신경망으로 행동 선택
    def get_action(self, state):
        policy = self.model.predict(state)[0]
        return np.random.choice(self.action_size, 1, p=policy)[0]
```
- 현재 `state`를 입력으로 넣으면 출력은 정책 policy로 나온다. **정책이 확률적이므로 그 확률에 따라 행동을 선택하면 에이전트는 저절로 탐험을 하게 된다**. `입실론 탐욕 정책` 같이, 임의로 다른 행동을 선택하게 하지 않아도 된다.

- 중요하게 봐야 할 부분으로, 목표함수의 미분값을 계산하여 정책 신경망을 업데이트하는 부분이 있다. 
$$
\theta_{t+1} \sim \theta_t + \alpha[\nabla_\theta log \pi_\theta(a|s)\,G_t]
$$
위 수식이 REINFORCE 알고리즘의 업데이트 식으로, 큐함수를 에피소드마다 얻는 반환값으로 대체했기 때문에 반환값 계산 함수가 필요하다.
`4.`번에서 처럼 에이전트는 에피소드가 끝날 때까지 행동을 선택한다. `상태와 행동, 보상`을 리스트로 저장한다.
```python
    # 한 에피소드 동안의 상태, 행동, 보상을 저장
    def append_sample(self, state, action, reward):
        self.states.append(state[0])
        self.rewards.append(reward)
        act = np.zeros(self.action_size)
        act[action] = 1
        self.actions.append(act)
```

에피소드가 끝나면 에이전트는 그래디언트를 계산한다. 그 전에 아래 코드로 `rewards` 리스트로부터 반환값을 계산한다.
```python
    # 반환값 계산
    def discount_rewards(self, rewards):
        discounted_rewards = np.zeros_like(rewards)
        running_add = 0
        for t in reversed(range(0, len(rewards))):
            running_add = running_add * self.discount_factor + rewards[t]
            discounted_rewards[t] = running_add
        return discounted_rewards
```

- 반환값은 $G_i = R_{i+1} + \gamma R_{i+2} + \gamma^2R_{i+3} + ...$으로 구성되는데, 가장 마지막 타임스텝의 반환값을 계산하면 조금 더 편하게 계산할 수 있다.
$$\begin{matrix}
G_5 = R_6 \\
G_4 = R_5 + \gamma R_6 = R_5 + \gamma G_5 \\
...
\end{matrix}
$$
- 위 과정이 반복문으로 들어간 것이다.

계산된 반환값으로 오류함수를 계산할 수 있다. 그 후 오류함수에 대해 그래디언트를 구해 정책신경망을 업데이트한다. 케라스는 반환값을 이용해 그래디언트를 구하는 걸 지원하지 않기 때문에, 사용자가 텐서플로우를 갖고 변형해서 사용해야 한다.

---
```python
from keras import backend as K

# ...

    # 정책신경망을 업데이트 하기 위한 오류함수와 훈련함수의 생성
    def optimizer(self):
        action = K.placeholder(shape=[None, 5])
        discounted_rewards = K.placeholder(shape=[None, ])
        
        # 크로스 엔트로피 오류함수 계산
        action_prob = K.sum(action * self.model.output, axis=1)
        cross_entropy = K.log(action_prob) * discounted_rewards
        loss = -K.sum(cross_entropy)
        
        # 정책신경망을 업데이트하는 훈련함수 생성
        optimizer = Adam(lr=self.learning_rate)
        updates = optimizer.get_updates(self.model.trainable_weights,[],
                                        loss)
        train = K.function([self.model.input, action, discounted_rewards], [],
                           updates=updates)

        return train

```
- 반환값으로 그래디언트를 계산해서 네트워크를 업데이트하는 함수가 `build_optimizer`이다. **이는 케라스의 `model.fit` 대신 사용된다.** 위 함수는 입력이 들어가서 네트워크를 업데이트하지 않고, 형태만 구축하는 역할을 한다.

- 그래디언트 계산을 위해 우선 반환값을 이용해 네트워크를 업데이트하기 위한 오류함수의 값을 구현해야 한다.
```python
        cross_entropy = K.log(action_prob) * discounted_rewards
        loss = -K.sum(cross_entropy)
```
$$
\nabla_\theta log \pi_\theta(a|s)\,G_t
$$

위 코드에서 중요한 점은 3가지가 있다.
> 1. 반환값 $G_t$가 $\theta$의 함수가 아니므로, 그래디언트의 괄호 안으로 넣을 수 있다. 이 때 반환값 $G_t$는 정규화된 반환값이다.
$$
\nabla_\theta [log \pi_\theta(a|s)\,G_t]
$$
- 괄호 내의 값들이 `오류함수`가 된다.

> 2. 오류함수의 의미 : `크로스 엔트로피Cross Entropy`

- 엔트로피의 의미는 아래와 같다.
$$
엔트로피 = -\underset{i}\Sigma p_i log p_i
$$
$p_i$는 $i$번째 사건이 일어날 확률을 의미한다.

- 크로스엔트로피는 **$y_i$와 $p_i$의 값이 얼마나 비슷한가**를 의미한다. **두 값이 같을 때 식은 최소**가 되며, 지도학습에서 $y_i$는 정답을 사용하기 때문에 현재 예측 값이 정답과 얼마나 가까운지를 나타내게 된다.

$$
크로스엔트로피 = -\underset{i} \Sigma(y_i log p_i)
$$
오류함수의 값이 줄어들수록 $p_i$가 $y_i$로 다가가서 정답을 선택할 확률을 100%로 만드는 것이 목표가 된다.  `action`은 선택한 행동이 1, 나머지가 0인 리스트이다. 여기에 정책 신경망의 출력을 곱하면, 실제로 선택한 행동을 할 확률이 된다.

```python
        action_prob = K.sum(action * self.model.output, axis=1)
```
- 위 코드의 의미는 지도학습의 정답 대신 실제 선택한 행동을 정답으로 두겠다는 뜻이 된다. 이 때 크로스 엔트로피는 아래 수식처럼 된다.
$$
크로스 엔트로피 = -\underset{i} \Sigma(y_i log p_i) = - log p_{action}
$$- 그래서 코드 마지막에 로그를 취해준다.
```python
 K.log(action_prob)
```

- 위 오류함수를 통해 **정책신경망을 업데이트하면 무조건 실제로 선택한 행동을 더 선택하는 방향으로 업데이트한다. 그러나 부정적인 보상을 받았다면 그 확률을 낮춰야 한다.**
- 그러한 정보를 제공하는게 `반환값`이다. 역전파를 통해 크로스 엔트로피를 줄이는 방향의 가중치 업데이트 값을 구했다면, 그 업데이트 값은 행동의 좋고 나쁨의 정보를 가진 반환값과 곱해진다. 그렇게 구한 새로운 가중치의 업데이트 값으로 정책신경망은 업데이트된다.

- 크로스 엔트로피에 반환값을 곱하는 코드는 아래와 같다. `loss`가 최종 오류 함수이다.
```python
        cross_entropy = K.log(action_prob) * discounted_rewards
        loss = -K.sum(cross_entropy)
```

> 3. REINFORCE 알고리즘은 경사상승법을 쓴다.

$$
\theta_{t+1} = \theta_t + \alpha\nabla_\theta J(\theta) \sim \theta_t + \alpha[\nabla_\theta log \pi_\theta(a|s)\,q_\pi(s, a)]
$$
- 그런데 경사 상승법을 적용하는 다른 방법으로, **$-log \pi_\theta(a|s)\,q_\pi(s, a)$ 에 경사하강법을 적용**해도 된다. 

- 경사하강법은 `Adam`을 사용하며, `loss`로 가중치를 업데이트하는 함수는 `get_updates`이다.
```python
        # 정책신경망을 업데이트하는 훈련함수 생성
        optimizer = Adam(lr=self.learning_rate)
        updates = optimizer.get_updates(self.model.trainable_weights,[],
                                        loss)
        train = K.function([self.model.input, action, discounted_rewards], [],
                           updates=updates)
```

---
- `train_model` : 실질적으로 학습을 진행하는 함수
```python
    # 정책신경망 업데이트
    def train_model(self):
	    # rewards 구함
        discounted_rewards = np.float32(self.discount_rewards(self.rewards))
	    
	    # rewards 정규화
        discounted_rewards -= np.mean(discounted_rewards)
        discounted_rewards /= np.std(discounted_rewards)

        self.optimizer([self.states, self.actions, discounted_rewards])
        self.states, self.actions, self.rewards = [], [], []
```
- 에피소드가 끝날 떄 호출되면 `rewards`를 이용해 `discounted_rewards`를 구하고, 정규화한다. 
- `정규화`는 정책신경망 업데이트 성능에 도움을 준다. 
- 이렇게 정규화된 반환값과 상태와 행동을 `self.optimizer`에 입력으로 주면 정책 신경망이 업데이트된다.

---
- `main` 루프에서 함수의 학습을 더 자세히 볼 수 있다.
```python
if __name__ == "__main__":
    # 환경과 에이전트의 생성
    env = Env()
    agent = ReinforceAgent()

    global_step = 0
    scores, episodes = [], []

    for e in range(EPISODES):
        done = False
        score = 0
        # env 초기화
        state = env.reset()
        state = np.reshape(state, [1, 15])

        while not done:
            global_step += 1
            # 현재 상태에 대한 행동 선택
            action = agent.get_action(state)
            # 선택한 행동으로 환경에서 한 타임스탭 진행 후 샘플 수집
            next_state, reward, done = env.step(action)
            next_state = np.reshape(next_state, [1, 15])

            agent.append_sample(state, action, reward)
            score += reward
            state = copy.deepcopy(next_state)
			
			# 에피소드 종료
            if done:
                # 에피소드마다 정책신경망 업데이트
                agent.train_model()
                scores.append(score)
                episodes.append(e)
                score = round(score,2)
                print("episode:", e, "  score:", score, "  time_step:",
                      global_step)

        # 100 에피소드마다 학습 결과 출력 및 모델 저장
        if e % 100 == 0:
            pylab.plot(episodes, scores, 'b')
            pylab.savefig("./save_graph/reinforce.png")
            agent.model.save_weights("./save_model/reinforce.h5")

```
- 에피소드 동안 에이전트는 정책신경망을 통해 행동을 선택한다. 에피소드가 끝날 때마다 학습한다.

- 근데 놀랍게도 코드가 실행되지 않죠? `Adam`에는 `get_updates`가 없대!