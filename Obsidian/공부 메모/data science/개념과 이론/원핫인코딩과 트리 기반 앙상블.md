[원문](https://towardsdatascience.com/one-hot-encoding-is-making-your-tree-based-ensembles-worse-heres-why-d64b282b5769)
```
일반적으로 원 핫 인코딩은 모델에 대한 더 나은 데이터 해상도를 제공하며 대부분의 모델은 결국 더 나은 성능을 보입니다. 이것이 모든 모델에 해당되는 것은 아니며 놀랍게도 랜덤 포레스트는 **높은 카디널리티 범주 변수가 있는 데이터 세트에서 일관되게 더 나쁜 성능**을 보였습니다.
```
- `카디널리티Cardinality` : 
	- 고유한 값이 많을수록 카디널리티가 높고
	- 중복된 값이 많을수록 카디널리티가 낮다.

```
왜 이런 일이 일어나는가?

1. 원핫인코딩의 결과는 희소성Sparsity이 생기는데, 이는 데이터셋에 바람직하지 않음
2. 분할 알고리즘에서 봤을 때, 원핫인코딩의 결과(Dummy)의 변수는 모두 독립적이다. 따라서 더미 변수 중 하나가 분할 기준이 됐을 때, 이로 인해 얻는 정보 이득Information Gain은 매우 미미하다.
```
- 즉, `카디널리티가 높은` 열에 원핫인코딩을 가하면, 더 차원이 높은 `Sparse`한 행렬이 생길 것이다. 너무 높은 차원은 해석할 수 없다.