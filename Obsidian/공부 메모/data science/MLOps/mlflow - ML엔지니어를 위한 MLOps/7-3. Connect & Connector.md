- 이전 : [[7-2. Producer & Consumer]]
![[Pasted image 20230104140608.png]]
- 실제 시스템의 파이프라인 : 전달할 DB가 많아질수록, `Producer`와 `Consumer`의 갯수도 늘려야 할 것이다.
- 간편하고 효율적으로 파이프라인을 구축하기 위해 카프카에서는 `Connect` 및 `Connector`를 지원한다.

Connect ##  

- `Connect` : 데이터 시스템 - 카프카 간의 데이터를 확장 가능 & 안전한 방법으로 Streaming하기 위한 도구.
- 이를 사용하기 위해 데이터를 가져오는 곳과 전달하는 곳을 알려주는 `Connector`를 정의해야 한다. 
- `Connector` : 메시지 파이프라인에 대한 추상 객체, Task를 관리한다.

- **Connect는 프레임워크**
- **Connector는 Connect 안에서 돌아가는 플러그인**이다.

- Connect의 종류
>  `Source Connector` 
>- `Source System` -> `브로커의 토픽`으로 Publish하는 커넥터
> - Producer의 역할을 한다

> `Sink Connector`
	- `브로커의 토픽` -> `Target System`으로 전달하는 커넥터
	- Consumer의 역할을 한다.

각 Connector에 관한 설정 명세를 Connect에 전달하면, Connector는 주기적으로 메시지를 확인하고 새로운 메시지를 파이프라인으로 흘려보낸다.

![[Pasted image 20230104141247.png]]
- DB의 수가 늘어나면 만들어야 하는 커넥터의 수도 늘어날 것이다.

##### Producer, Consumer 대비 Connector의 장점
- `Connector`에 대한 설정 파일만으로 간단하게 띄울 수 있음
- Connector의 유형, 연결 URL, user, password, 테이블 이름, 토픽 파티션 수, Replication Factor 수 등을 설정하여 Connect에서 인스턴스로 생성할 수 있다. 
- 위 과정이 Producer 개발보다 훨씬 비용이 적고 간편하다.

## Schema Registry
- Kafka에는 Decoupling이라는 특징이 있다. 즉 **Producer, Consumer가 서로 의존적이지 않고 완벽히 분리**되어 있다.
- 또한 Broker는 메시지를 1번 저장하면 수정할 수 없다.

- 위 특징 떄문에 이런 상황이 벌어질 수 있다.
![[Pasted image 20230104141647.png]]
> 1. Producer 1과 2는 각각 브로커의 토픽 A에 메시지를 보냄
> 2. Consumer는 토픽 A의 메시지를 읽음
> 3. Producer 2가 Schema를 변경해서 메시지를 발행(`4`)함
> 4. Consumer는 위 상황을 모르기 때문에 `4번 메시지`를 구독 & 처리하는 과정에서 메시지를 못 읽고 에러가 발생함

- 즉 내부적인 결합도에 대한 문제가 잔존한다.
- 또한, 동일한 스키마의 메시지가 계속 들어오는 경우 같은 스키마를 계속 저장해야 하기 때문에 메시지가 커지고, 스키마의 중복으로 인해 불필요한 데이터 용량을 차지하게 된다.

- 이를 위해 Kafka에서는 `Schema Registry`를 사용한다.
- `Schema Registry` 
	- 메시지의 스키마를 저장해주는 저장소
	- 메시지 구조의 결합도를 낮추기 위해 고안되었다. 

### 메시지 구조
![[Pasted image 20230104142153.png]]
- `Connector`에 의해 생성됨
- Message는 key, value로 구성되어 있고, 각각은 Schema와 Payload로 구성되어 있다.
	- `Schema` : 데이터 타입
	- `Payload` : 데이터 값

### 관계도
![[Pasted image 20230104142352.png]]
- 작동 순서
> 1. Producer에서 Kafka의 `Serializer`(or `Converter`)에 메시지를 보냄
> 2. `Serializer`는 메시지를 받아 `Schema`를 `Schema Registry`에 보냄
> 3. `Serializer`는 `Schema Registry`로부터 `Schema ID`를 받고, `Schema ID` + 데이터를 카프카에 보냄

- 데이터 중복 문제는 key, value에 명시된 Schema를 따로 저장하므로 Connector가 스키마 대신 스키마 ID를 저장하여 용량을 줄일 수 있음
- 결합도 문제도 `Schema 호환성 강제` 기능으로 해결할 수 있다.
	- Schema 버전 간의 호환성을 강제해 규칙을 세우는 것.

![[Pasted image 20230104142706.png]]
- Forward 호환성의 예시
> 1. Consumer는 Version 1로 메시지를 처리함
> 2. Gender라는 Column이 Version 2에서 추가, Consumer는 Version 2의 Schema를 메시지를 구독하여 처리한다.
> 3. Consumer는 이 때 새로 추가된 필드를 제외하고, **기존 버전 1에 맞춰서 메시지를 처리**한다.

#### 스키마 호환성의 종류
- 버전 1,2 스키마 기준 설명
- `Backward`
	- 2번 스키마로 메시지를 처리하지만 1번도 가능 
	- 필드 삭제 or 기본 값이 있는 필드 추가
- `Forward`
	- 1번 스키마로 메시지를 처리하지만 2번도 가능
	- 필드 추가 or 기본 값이 있는 필드 삭제
- `Full`
	- `Backward` + `Forward`]
- `None` 
	- 체크 X


- 다음 : [[7-4. Kafka System]]