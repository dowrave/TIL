- 이전 : [[7-4. Kafka System]]
![[Pasted image 20230104152249.png]]
- 소스 커넥터에 DB 연결해야 함 : 1장에서 만든 컨테이너 띄우고 오자


## Source Connector 생성
- `source_connector.json` 생성
```sh
# 아래 둘 중 하나 사용 : 위 명령어는 아래 내용 그대로 붙이면 됨
cat <<EOF > source_connector.json 
vi source_connector.json
```
```json

{  
	"name": "postgres-source-connector",  
	"config": {  
		"connector.class": "io.confluent.connect.jdbc.JdbcSourceConnector",  
		"connection.url": "jdbc:postgresql://postgres-server:5432/mydatabase",  
		"connection.user": "myuser",  
		"connection.password": "mypassword",  
		"table.whitelist": "iris_data",  
		"topic.prefix": "postgres-source-",  
		"topic.creation.default.partitions": 1,  
		"topic.creation.default.replication.factor": 1,  
		"mode": "incrementing",  
		"incrementing.column.name": "id",  
		"tasks.max": 2,  
		"transforms": "TimestampConverter",  
		"transforms.TimestampConverter.type": "org.apache.kafka.connect.transforms.TimestampConverter$Value",  
		"transforms.TimestampConverter.field": "timestamp",  
		"transforms.TimestampConverter.format": "yyyy-MM-dd HH:mm:ss.S",  
		"transforms.TimestampConverter.target.type": "string"  
	}  
}
// EOF // cat으로 만들면 추가
```
- `name` : `connector` 이름
- `config
> `connector.class` 
>> - 커넥터 생성을 위한 클래스
>> - `JDBC Source Connector`이므로 이에 대한 걸 입력
>
>`connection.url`
>> - Source DB에 접근하기 위한 주소
>> - Postgre Server의 URL을 입력함 - 앞에 `jdbc:postgresql`을 붙임
>
>`connection.user, connection.password`
>> - Source DB에 접속하기 위한 유저 이름 & 비밀번호
>
>`table.whitelist`
>> - 데이터를 가져올 테이블의 목록
>> - 여러 테이블이라면 `,`로 구분할 수 있음
>
>`topic.prefix`
>> - 토픽 생성 시 이름 앞에 붙일 `prefix`를 결정함
>> - `prefix_tablename`이 최종 토픽의 이름
>
>`topic.creation.default.partitions`
>> - 토픽 자동 생성을 위해 반드시 설정돼야 하는 값 1
>> - Source Connector 실행 시 토픽이 없다면 자동으로 토픽을 생성할 수 있는데, 설정 파일에서 `topic.creation.enable = true`로 설정해야 한다(디폴트 값임) -> 그 결과 `default`라는 이름의 토픽 생성을 담당하는 그룹이 생긴다.
>> - `default` 그룹에서 토픽 자동 생성할 때 `Partition`의 수를 결정한다.
>
>`topic.creaetion.default.replication.factor`
>> - 토픽 자동 생성을 위해 반드시 결정되어야 하는 값 2
>> - `replication factor` 수
>
>`mode`
>> 테이블에 변경이 발생했을 때 어떤 방식으로 데이터를 가져올 지 결정한다.
>> 이 예제에서는 `incrementing`을 사용함.
>> 
>`mode`의 종류
>> 1. `bulk`
>>> - 이벤트가 발생한 테이블 내용을 모두 가져온다
>>2. `timestamp`
>>> - `timestamp column`을 통해 들어온 row를 새로운 데이터로 판단해 해당 데이터만 가져온다.
>>3. `incrementing`
>>> - `incrementing column`을 통해 들어온 row를 새로운 데이터로 판단한다. 
>>> - 이벤트 `삭제`와 `수정`에 대해서는 작동하지 않으며,  이들을 가져오고 싶다면 `Shadow` 테이블을 만들어야 한다.
>>4. `timestamp+incrementing` : `timestamp` + `incrementing` 
>>> - 2개의 열을 사용해 들어온 row를 신규로 판단하고 해당 데이터만 가져온다.
> `incrementing.column.name`
>> - `incrementing column`의 이름을 지정
>>> 해당 `column`의 데이터 타입이 `varchar`이면 에러 발생.
>> - 여기선 `id`를 지정한다. 
>
>`tasks.max`
>> Connector에서의 task의 수 결정
>
>`transforms`
>> 1번 파트에서 만든 `iris_data` 테이블의 `timestamp column`을 Source Connector를 이용해 데이터를 가져올 경우 타입이 `Unix Epoch Time`으로 변경된다. 
>> 따라서 Source Connector 생성 시 `Tmiestamp Converter`를 이용해 Unix Epoch Time을 `Timestamp Type`으로 변경한 뒤 토픽에는 `String`으로 넣어야 한다. 
>
> `transforms.TimestampConverter.type`
>> - `TimestampConverter`의 타입을 설정한다. 
>> - 추적하는 열이 `value`에 있기 때문에 `value`에 대한 `timestamp converter`을 설정한다.
>
>`transforms.TimestampConverter.field`
>> - 추적할 `field`를 설정한다. (`column`과 동일)
>
>`transforms.TimestampConverter.format`
>> - 추적 field의 포맷을 설정한다.
>> - `yyyy-MM-dd HH:mm:ss.S`
>
> `transforms.TimestampConverter.target.type`
>> - 컨버터를 통해 변환한 후에 적용할 Type을 설정한다.
>> - 브로커의 토픽엔 `String`으로 넣는다.

- 위에서 생성한 파일을 `curl` 명령어를 이용해 `Connect`의 REST API에 POST method로 보낸다.
```sh
curl -X POST http://localhost:8083/connectors -H "Content-Type: application/json" -d @source_connector.json

# key-value의 뭐시기가 출력되면 성공
```

- 생성 확인
```sh
curl -X GET http://localhost:8083/connectors
```

- `postgres-source-connector` 정보 확인
```sh
curl -X GET http://localhost:8083/connectors/postgre-source-connector
```

- Topic에 쌓인 데이터 확인
	- 내 경우 `kcat` 명령어는 작동 안하고 `kafkacat`은 작동함
```sh
apt-get install kafkacat

kafkacat -L -b localhost:9092
```

- 해당 토픽의 데이터 확인
```sh
kafkacat -b localhost:9092 -t postgres-source-iris_data
```

- 다음 : [[7-6. Sink Connector]]